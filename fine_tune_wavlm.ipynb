{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "586b4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import WavLMForCTC, WavLMConfig, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "\n",
    "# Add import for local WavLM model\n",
    "import sys\n",
    "sys.path.append('/home/zohreh/ownCloud/hackathon2025/local_test/unilm/wavlm')\n",
    "from WavLM import WavLM, WavLMConfig as LocalWavLMConfig\n",
    "\n",
    "# Install required packages if not already installed\n",
    "# !pip install transformers torchaudio datasets\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to local WavLM model\n",
    "LOCAL_MODEL_PATH = '/home/zohreh/ownCloud/hackathon2025/local_test/models/WavLM-Base+.pt'\n",
    "\n",
    "# Function to load local WavLM model\n",
    "def load_local_wavlm_model():\n",
    "    \"\"\"Load WavLM model from local file\"\"\"\n",
    "    checkpoint = torch.load(LOCAL_MODEL_PATH, map_location='cpu')\n",
    "    cfg = LocalWavLMConfig(checkpoint['cfg'])\n",
    "    model = WavLM(cfg)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    return model, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ab314c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_get_processor(phoneme_list_path, output_dir=\"./phoneme_tokenizer\"):\n",
    "    \"\"\"Create a phoneme tokenizer and processor or load existing ones.\"\"\"\n",
    "    # Create tokenizer\n",
    "    if os.path.exists(output_dir):\n",
    "        # Load existing tokenizer\n",
    "        tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(output_dir)\n",
    "    else:\n",
    "        # Create new tokenizer from phoneme list\n",
    "        with open(phoneme_list_path, \"r\") as f:\n",
    "            phoneme_dict = json.load(f)\n",
    "\n",
    "        vocab_dict = {p: i for i, p in enumerate(phoneme_dict[\"phonemes\"])}\n",
    "        # Add special tokens\n",
    "        vocab_dict[\"<pad>\"] = len(vocab_dict)\n",
    "        vocab_dict[\"<unk>\"] = len(vocab_dict)\n",
    "        vocab_dict[\"<s>\"] = len(vocab_dict)\n",
    "        vocab_dict[\"</s>\"] = len(vocab_dict)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(os.path.join(output_dir, \"vocab.json\"), \"w\") as f:\n",
    "            json.dump(vocab_dict, f)\n",
    "\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            os.path.join(output_dir, \"vocab.json\"),\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            word_delimiter_token=\"|\",\n",
    "        )\n",
    "\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Get feature extractor\n",
    "    # Instead of downloading from Microsoft repo, we just create a new feature extractor\n",
    "    # with the same parameters as WavLM\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1,\n",
    "        sampling_rate=16000,\n",
    "        padding_value=0.0,\n",
    "        do_normalize=True,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # Create processor (combines feature extractor and tokenizer)\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "    return processor, len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2138953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeDataset(Dataset):\n",
    "    def __init__(self, data_path, processor, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to CSV file with columns 'file_path', 'phoneme_sequence'\n",
    "            processor: WavLM processor\n",
    "            sample_rate: Target sample rate\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        file_path = self.df.iloc[idx]['file_name']\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Convert to mono if necessary\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Get phoneme sequence\n",
    "        phoneme_sequence = self.df.iloc[idx]['phoneme_sequence']\n",
    "\n",
    "        # Process inputs - return raw audio data\n",
    "        input_values = waveform.squeeze().numpy()\n",
    "\n",
    "        # Process labels\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor(phoneme_sequence).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,  # Return raw audio data\n",
    "            \"labels\": labels,\n",
    "            \"phoneme_sequence\": phoneme_sequence\n",
    "        }\n",
    "\n",
    "# Completely rewritten data collator\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor, np.ndarray]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract raw audio waveforms and process them with the feature extractor\n",
    "        waveforms = [feature[\"input_values\"] for feature in features]\n",
    "        batch = self.processor(\n",
    "            waveforms,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "            sampling_rate=16000,\n",
    "            return_attention_mask=False,\n",
    "        )\n",
    "\n",
    "        # Process text labels\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 for loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Add labels to the batch\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        # Add original phoneme sequences for later use\n",
    "        batch[\"phoneme_sequences\"] = [feature[\"phoneme_sequence\"] for feature in features]\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3bf7bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavLMForPhonemeRecognition(nn.Module):\n",
    "    def __init__(self, num_phonemes):\n",
    "        super().__init__()\n",
    "        # Load pre-trained WavLM model from local file\n",
    "        local_model, local_cfg = load_local_wavlm_model()\n",
    "\n",
    "        # Create a standard HuggingFace config for the CTC head\n",
    "        self.config = WavLMConfig()\n",
    "        self.config.ctc_loss_reduction = \"mean\"\n",
    "        self.config.ctc_zero_infinity = True\n",
    "        self.config.vocab_size = num_phonemes\n",
    "        self.config.num_labels = num_phonemes\n",
    "        print(num_phonemes)\n",
    "\n",
    "        # Initialize WavLM with CTC head for phoneme recognition\n",
    "        # We use the default WavLMForCTC model initially, but replace its WavLM encoder\n",
    "        # with our locally loaded model\n",
    "        self.model = WavLMForCTC(self.config)\n",
    "\n",
    "        # Replace the wavlm model inside WavLMForCTC with our local model\n",
    "        # (This assumes the model structure is compatible)\n",
    "        self.model.wavlm = local_model\n",
    "\n",
    "        # Initialize the CTC head properly\n",
    "        self.model.lm_head = nn.Linear(local_cfg.encoder_embed_dim, num_phonemes)\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        return self.model(\n",
    "            input_values=input_values,\n",
    "            labels=labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e81f85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            print(input_values.shape)\n",
    "            attention_mask = batch.get(\"attention_mask\", None)\n",
    "            print(attention_mask)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            print(labels.shape)\n",
    "\n",
    "            outputs = model(\n",
    "                input_values=input_values,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_values = batch[\"input_values\"].to(device)\n",
    "                attention_mask = batch.get(\"attention_mask\", None)\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_values=input_values,\n",
    "\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_phoneme_model.pt\")\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "04494814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_time_indices(model, processor, audio_path, device, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Predict phonemes with their time indices from an audio file\n",
    "    using the custom WavLM model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess audio\n",
    "    waveform, file_sample_rate = torchaudio.load(audio_path)\n",
    "    if file_sample_rate != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(file_sample_rate, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # Process audio\n",
    "    input_values = processor(waveform.squeeze().numpy(),\n",
    "                             sampling_rate=sample_rate,\n",
    "                             return_tensors=\"pt\").input_values.to(device)\n",
    "\n",
    "    # Get logits\n",
    "    with torch.no_grad():\n",
    "        # Just use input_values without attention_mask\n",
    "        outputs = model(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get predicted tokens\n",
    "    predicted_ids = torch.argmax(logits, dim=-1).squeeze()\n",
    "\n",
    "    # Decode tokens to phonemes\n",
    "    predicted_phonemes = processor.tokenizer.convert_ids_to_tokens(predicted_ids.tolist())\n",
    "\n",
    "    # Get time indices\n",
    "    # First, remove repeated tokens (CTC decoding)\n",
    "    decoded_phonemes = []\n",
    "    time_indices = []\n",
    "\n",
    "    prev_phoneme = None\n",
    "    current_start = 0\n",
    "\n",
    "    for i, phoneme in enumerate(predicted_phonemes):\n",
    "        # Skip blank tokens (represented by special tokens) and repeated tokens\n",
    "        if phoneme in processor.tokenizer.all_special_tokens or phoneme == prev_phoneme:\n",
    "            continue\n",
    "\n",
    "        if prev_phoneme is not None:\n",
    "            # Convert frame index to time in seconds\n",
    "            # (20ms per frame at 16kHz with stride of 320 samples)\n",
    "            start_time = current_start * 0.02\n",
    "            end_time = i * 0.02\n",
    "            decoded_phonemes.append(prev_phoneme)\n",
    "            time_indices.append([start_time, end_time])\n",
    "\n",
    "        prev_phoneme = phoneme\n",
    "        current_start = i\n",
    "\n",
    "    # Don't forget the last phoneme\n",
    "    if prev_phoneme is not None and prev_phoneme not in processor.tokenizer.all_special_tokens:\n",
    "        start_time = current_start * 0.02\n",
    "        end_time = len(predicted_phonemes) * 0.02\n",
    "        decoded_phonemes.append(prev_phoneme)\n",
    "        time_indices.append([start_time, end_time])\n",
    "\n",
    "    return decoded_phonemes, time_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db97ad",
   "metadata": {},
   "source": [
    "training csv should look like this\n",
    "file_path,phoneme_sequence\n",
    "/path/to/audio1.wav,\"a b c\"\n",
    "/path/to/audio2.wav,\"d e f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4849217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25085/2277899015.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(LOCAL_MODEL_PATH, map_location='cpu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohreh/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "torch.Size([4, 479232])\n",
      "None\n",
      "torch.Size([4, 202])\n",
      "torch.Size([4, 479232])\n",
      "None\n",
      "torch.Size([4, 202])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohreh/miniforge3/envs/kidtalk/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_forward_unimplemented() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Load best model for inference\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_phoneme_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[188], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     30\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[187], line 28\u001b[0m, in \u001b[0;36mWavLMForPhonemeRecognition.forward\u001b[0;34m(self, input_values, labels)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_values, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py:1319\u001b[0m, in \u001b[0;36mWavLMForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1319\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavlm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/kidtalk/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: _forward_unimplemented() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example paths - replace with your own\n",
    "train_data_path = \"train_phonemes_clean.csv\"  # Should contain file_path and phoneme_sequence\n",
    "val_data_path = \"val_phonemes_clean.csv\"      # Should contain file_path and phoneme_sequence\n",
    "phoneme_list_path = \"phoneme_list.json\"\n",
    "\n",
    "# Create processor from the phoneme list using the function defined above\n",
    "processor, vocab_size = create_and_get_processor(phoneme_list_path)\n",
    "\n",
    "# Create datasets (without time indices)\n",
    "train_dataset = PhonemeDataset(train_data_path, processor)\n",
    "val_dataset = PhonemeDataset(val_data_path, processor)\n",
    "\n",
    "# Create data loaders\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=data_collator)\n",
    "print(vocab_size)\n",
    "\n",
    "# Initialize model\n",
    "model = WavLMForPhonemeRecognition(vocab_size).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, val_loader, optimizer, num_epochs=10, device=device)\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load(\"best_phoneme_model.pt\"))\n",
    "\n",
    "# Example inference with time indices\n",
    "audio_path = \"test_audio.wav\"\n",
    "predicted_phonemes, time_indices = predict_with_time_indices(model, processor, audio_path, device)\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted phonemes with time indices:\")\n",
    "for phoneme, (start, end) in zip(predicted_phonemes, time_indices):\n",
    "    print(f\"Phoneme: {phoneme}, Start: {start:.2f}s, End: {end:.2f}s\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"./phoneme_recognition_model.pt\")\n",
    "processor.save_pretrained(\"./phoneme_recognition_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a450478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WavLMForPhonemeRecognition(\n",
       "  (model): WavLMForCTC(\n",
       "    (wavlm): WavLM(\n",
       "      (feature_extractor): ConvFeatureExtractionModel(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "          (1-4): 4 x Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (5-6): 2 x Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "      (encoder): TransformerEncoder(\n",
       "        (pos_conv): Sequential(\n",
       "          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (1): SamePad()\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "              (relative_attention_bias): Embedding(320, 12)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1-11): 11 x TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (lm_head): Linear(in_features=768, out_features=36, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa9e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(\"Hackathon_ASR/2_Audiofiles/Decoding_IT_T1/1001_edugame2023_59aa8ecf74c44db2adf56d71d1705cf5_1de23ac3deaf4b4d8c7db6d0cc9d6bfe.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phoneme_list.json'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304b6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
