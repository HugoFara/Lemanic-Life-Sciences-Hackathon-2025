{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b6c7c4",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Let's create a model first, with some vocab.\n",
    "The output is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f115131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.12/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.venv/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa numpy soundfile torch torchaudio datasets transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d917c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  3.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities shape: (1, 292, 50)\n",
      "Recognized phoneme sequence: ɹjɹaɹuʊwawɾespaɹəũœ̪œlɑɲkɪsʌʃurv̪øv̪n(ønʊnwzmɔkbsbjrɲmɪnʃoʌmøonɪ(eanŋ̪mnʃʌɑœɲkɪkbʁzajʎømwnvj)øəɪwnləpʃɡ(ɹyɹŋɲiʊɲːknœɾ̪ʁɑuuʌɪnɪɹaʌøʌlaɲoɛ̪(øtmnɔlvʌwsajømnɔɪnaob̪ɾ̪s)nsɹ\n",
      "Transcript for reference: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# PhonemeRecognizer: WavLM + CTC for phoneme speech recognition\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# Load vocab from file\n",
    "with open(\"phoneme_tokenizer/vocab.json\") as vocab_file:\n",
    "    vocab = json.load(vocab_file)\n",
    "\n",
    "# IT + FR phonemes + blank\n",
    "VOCAB = (\n",
    "    \"[PAD]\", \"[UNK]\",\n",
    "    \"ʒ\",\"ɹ\",\"j\",\"d\",\"ɲ\",\"ʌ\",\"ɒ\",\"ɐ\",\"ʃ\",\"ɔ\",\"f\",\"ø\",\"z\",\"ŋ\",\"i\",\"u\",\"̃\",\"o\",\"œ\",\"a\",\"(\",\"ə\",\"ɜ\",\n",
    "    \"ɾ\",\"ː\",\"̪\",\"e\",\"b\",\"ʁ\",\"w\",\"n\",\"p\",\"y\",\"ɡ\",\"ɪ\",\"r\",\"v\",\"t\",\")\",\"m\",\"k\",\"ʊ\",\"ʎ\",\"ɑ\",\"s\",\"l\",\"ɛ\",\n",
    "    '<blank>'\n",
    ")\n",
    "PHONEME_DICT = {v: i for i, v in enumerate(VOCAB)}\n",
    "\n",
    "NUM_PHONEMES = len(VOCAB)\n",
    "\n",
    "class PhonemeRecognizer(nn.Module):\n",
    "    def __init__(self, wavlm_model, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "        self.wavlm = wavlm_model\n",
    "\n",
    "        # Get the hidden size from the WavLM model\n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(hidden_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get WavLM embeddings\n",
    "        outputs = self.wavlm(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    seq.append(VOCAB[p])\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "\n",
    "\n",
    "    def recognize(self, inputs):\n",
    "        \"\"\"Perform phoneme recognition.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get log probabilities\n",
    "            log_probs = self(inputs)\n",
    "\n",
    "            return self.classify_to_phonemes(log_probs)\n",
    "\n",
    "    def tokenize(self, char_list, lenient=False):\n",
    "        \"\"\"\n",
    "        Go from a list of characters to a list of indices.\n",
    "        \n",
    "        :param list[str] char_list: Characters top be mapped.\n",
    "        :param bool lenient: If True, characters not in vocab are mapped to [UNK] \n",
    "        \"\"\"\n",
    "        if not lenient:\n",
    "            return torch.tensor([PHONEME_DICT[x] for x in char_list])\n",
    "        \n",
    "        return torch.tensor([PHONEME_DICT[x] if x in PHONEME_DICT else PHONEME_DICT[\"[UNK]\"] for x in char_list])\n",
    "    \n",
    "    def get_embedding(self, char_list):\n",
    "        tokens = self.tokenize(char_list)\n",
    "        out_tensor = torch.zeros((len(tokens), len(PHONEME_DICT)))\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            out_tensor[i, token_id] = 1\n",
    "        return out_tensor\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# Method A: Using the PhonemeRecognizer for speech-to-phoneme ASR\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1. Load the feature extractor and model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "\n",
    "def preprocess_audios(batch):\n",
    "    \"\"\"Preprocess the audio files (pad/truncate + batch-dim).\"\"\"\n",
    "    for data_row in batch:\n",
    "        if data_row[\"sampling_rate\"] != 16000:\n",
    "            raise NotImplementedError(\n",
    "                f\"No sampling rate can be different from 16000, is {data_row[\"sampling_rate\"]}\"\n",
    "            )\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        [data_row[\"array\"] for data_row in batch],\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,       # pad to longest in batch\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def run_inference(batch, model):\n",
    "    \"\"\"Return log probs and most likely phonemes.\"\"\"\n",
    "    inputs = preprocess_audios(batch[\"audio\"])\n",
    "\n",
    "    # 4. Inference for phoneme recognition\n",
    "    with torch.no_grad():\n",
    "        # Get phoneme log probabilities\n",
    "        log_probs = model(inputs)\n",
    "\n",
    "        # Recognize phoneme sequence\n",
    "        phoneme_sequences = model.recognize(inputs)\n",
    "\n",
    "    return {\"log_probs\": log_probs, \"phonemes\": phoneme_sequences}\n",
    "\n",
    "\n",
    "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "# Create the phoneme recognizer with the WavLM model\n",
    "phoneme_recognizer = PhonemeRecognizer(wavlm_model)\n",
    "phoneme_recognizer.eval()  # disable dropout, etc.\n",
    "\n",
    "# 2. Load an example audio file (here using a small demo from `datasets`)\n",
    "# ds = datasets.load_dataset(\"csv\", data_files=\"Hackathon_ASR/2_Audiofiles/Phoneme_Deletion_FR_T1\", features=features, split=\"train\")\n",
    "ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "audio_sample = ds[0][\"audio\"][\"array\"]\n",
    "sr = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "\n",
    "predicted = ds.select(range(1)).map(\n",
    "    lambda data_row: run_inference(data_row, phoneme_recognizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Print output\n",
    "print(\"Log probabilities shape:\", np.shape(predicted[\"log_probs\"]))  # (batch_size, seq_len, num_phonemes)\n",
    "print(\"Recognized phoneme sequence:\", \"\".join(predicted[\"phonemes\"][0]))\n",
    "print(\"Transcript for reference:\", ds[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5207f7",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load our data in a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65d3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5507 rows before filtering\n",
      "4789 rows after filtering\n",
      "Dataset({\n",
      "    features: ['file_name', 'audio', 'target_phonemes1', 'target_phonemes2'],\n",
      "    num_rows: 4789\n",
      "})\n",
      "{'path': 'Hackathon_ASR/2_Audiofiles/Phoneme_Deletion_FR_T1/3101_edugame2023_1c148def3c254026adc7a7fdc3edc6f6_2249c5f2f75d4b089bee1f36e1a7aef2.wav', 'array': array([1.97946756e-05, 3.31596239e-05, 2.90264506e-05, ...,\n",
      "       1.05573982e-03, 2.05589482e-03, 0.00000000e+00], shape=(66317,)), 'sampling_rate': 16000}\n",
      "['ɑ', '̃', '[PAD]', 'n', 'a', 'ʒ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import soundfile\n",
    "\n",
    "# 1. Location of your CSV\n",
    "LANGUAGE = (\"fr\", \"it\")[0]\n",
    "audio_files_path = \"Hackathon_ASR/2_Audiofiles/\" + {\n",
    "    \"fr\": \"Phoneme_Deletion_FR\",\n",
    "    \"it\": \"Decoding_IT\"\n",
    "}[LANGUAGE] + \"_T1/\"\n",
    "\n",
    "dataset_path = f\"datasets/phonemized_{LANGUAGE}.csv\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"{dataset_path} does not exist and should be regenerated!\")\n",
    "\n",
    "\n",
    "# 2. Define initial features: audio paths as plain strings, phonemes as plain strings\n",
    "features = datasets.Features({\n",
    "    \"file_name\": datasets.Value(\"string\"),\n",
    "    \"phonemes_coder1\": datasets.Value(\"string\"),\n",
    "    \"phonemes_coder2\": datasets.Value(\"string\")\n",
    "})\n",
    "\n",
    "# 3. Load the CSV into a DatasetDict (default split is 'train')\n",
    "dataset = datasets.load_dataset(\"csv\", data_files=dataset_path, features=features, split=\"train\")\n",
    "\n",
    "dataset = dataset.map(lambda data_row: {\"audio\": audio_files_path + data_row[\"file_name\"]}, desc=\"Select audio files path\")\n",
    "\n",
    "# 6. Map + split phoneme strings into lists\n",
    "def split_in_bracket(string):\n",
    "    output = []\n",
    "    in_brackets = False\n",
    "    if string is None:\n",
    "        return output\n",
    "    for char in string:\n",
    "        if in_brackets:\n",
    "            output[-1] += char\n",
    "        else:\n",
    "            output.append(char)\n",
    "\n",
    "        if char == '[':\n",
    "            in_brackets = True\n",
    "        elif char == ']':\n",
    "            if output[-1] not in VOCAB:\n",
    "                print(f\"Removing {output.pop()}\")\n",
    "            in_brackets = False\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_phonemes(data_row):\n",
    "    \"\"\"Split each phoneme into a list.\"\"\"\n",
    "    data_row[\"target_phonemes1\"] = split_in_bracket(data_row[\"phonemes_coder1\"])\n",
    "    data_row[\"target_phonemes2\"] = split_in_bracket(data_row[\"phonemes_coder2\"])\n",
    "    return data_row\n",
    "\n",
    "def check_audios_valid(data_row):\n",
    "    \"\"\"Mark file invalid when it cannot be read.\"\"\"\n",
    "    if not os.path.exists(data_row[\"audio\"]):\n",
    "        return False\n",
    "    \n",
    "    with open(data_row[\"audio\"], 'rb') as file:\n",
    "        try:\n",
    "            soundfile.read(file)\n",
    "            return True\n",
    "        except soundfile.LibsndfileError:\n",
    "            return False\n",
    "\n",
    "print(dataset.num_rows, \"rows before filtering\")\n",
    "dataset = dataset.filter(check_audios_valid, desc=\"Filtering out unreadable files\")\n",
    "print(dataset.num_rows, \"rows after filtering\")\n",
    "\n",
    "# 5. Cast 'audio' to the Audio type (will load the file when you access it)\n",
    "dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "\n",
    "dataset = dataset.map(split_phonemes, remove_columns=[\"phonemes_coder1\", \"phonemes_coder2\"], desc=\"Phonemize data\")\n",
    "\n",
    "# 7. Cast the phoneme_sequence column to a Sequence of strings\n",
    "dataset = dataset.cast_column(\n",
    "    \"target_phonemes1\",\n",
    "    datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    ").cast_column(\n",
    "    \"target_phonemes2\",\n",
    "    datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    ")\n",
    "\n",
    "# Now 'dataset' has:\n",
    "#   - dataset[i][\"audio\"] → { \"array\": np.ndarray, \"sampling_rate\": 16000 }\n",
    "#   - dataset[i][\"target_phonemes1\"] → list of strings\n",
    "print(dataset)\n",
    "print(dataset[0][\"audio\"])\n",
    "print(dataset[0][\"target_phonemes1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf53be",
   "metadata": {},
   "source": [
    "## Putting stuff together\n",
    "\n",
    "Now we run the model on our in-house dataset.\n",
    "We will extract the features so that it is easier to work with latter on.\n",
    "For this version we don't train the model, only a fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb0ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting audio features: 100%|██████████| 4789/4789 [01:03<00:00, 75.52 examples/s] \n",
      "Saving the dataset (14/14 shards): 100%|██████████| 4789/4789 [00:05<00:00, 896.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to datasets/features_fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_name', 'target_phonemes1', 'target_phonemes2', 'features'],\n",
       "    num_rows: 4789\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS_DIR = \"datasets\"\n",
    "dataset_path = f\"{DATASETS_DIR}/features_{LANGUAGE}\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def audio_processor(batch):\n",
    "    preprocessed = preprocess_audios(batch)\n",
    "\n",
    "    return {\"features\": wavlm_model(\n",
    "        input_values=preprocessed[\"input_values\"].to(device), attention_mask=preprocessed[\"attention_mask\"].to(device)\n",
    "        # (torch.tensor(input_values) if isinstance(input_values, list) else input_values).to(device),\n",
    "        # (torch.tensor(attention_mask) if isinstance(attention_mask, list) else attention_mask).to(device)\n",
    "    ).last_hidden_state}\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    features_dataset = datasets.load_from_disk(dataset_path)\n",
    "    print(f\"Using existing dataset {dataset_path}\")\n",
    "else:\n",
    "    wavlm_model.eval()\n",
    "    wavlm_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_dataset = (\n",
    "            dataset\n",
    "            .map(\n",
    "                audio_processor,\n",
    "                batched=True,\n",
    "                input_columns=[\"audio\"],\n",
    "                remove_columns=[\"audio\"],\n",
    "                batch_size=30,\n",
    "                desc=\"Extracting audio features\"\n",
    "            )\n",
    "            .with_format(\"torch\")\n",
    "        )\n",
    "\n",
    "    features_dataset.save_to_disk(dataset_path)\n",
    "    print(f\"Saved to {dataset_path}\")\n",
    "features_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690dea4",
   "metadata": {},
   "source": [
    "## Defining a new linear layer\n",
    "\n",
    "As a speed-up, we simply create a linear layer to map from the extracted features to the phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfda0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PhonemeMapper(nn.Module):\n",
    "    def __init__(self, features_size, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(features_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(inputs)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    phoneme = list(PHONEME_DICT.keys())[list(PHONEME_DICT.values()).index(p)]\n",
    "                    seq.append(phoneme)\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "    \n",
    "linear_mapper = PhonemeMapper(wavlm_model.config.hidden_size, NUM_PHONEMES)\n",
    "\n",
    "wavlm_model.config.hidden_size, NUM_PHONEMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc4765",
   "metadata": {},
   "source": [
    "## Model fine-tuning\n",
    "\n",
    "Now that the linear layer is ready, we can simply train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bfb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.142487049102783\n",
      "Epoch 0, Loss: 1.7919325828552246\n",
      "Epoch 0, Loss: 2.030946969985962\n",
      "Epoch 0, Loss: 2.1164352893829346\n",
      "Epoch 0, Loss: 1.653796672821045\n",
      "Epoch 0, Loss: 1.8782861232757568\n",
      "Epoch 0, Loss: 1.701148271560669\n",
      "Epoch 0, Loss: 1.9954755306243896\n",
      "Epoch 0, Loss: 2.041874885559082\n",
      "Epoch 0, Loss: 1.891783595085144\n",
      "Epoch 0, Loss: 2.093203067779541\n",
      "Epoch 0, Loss: 1.9534753561019897\n",
      "Epoch 0, Loss: 1.826332926750183\n",
      "Epoch 0, Loss: 2.2181437015533447\n",
      "Epoch 0, Loss: 1.5160703659057617\n",
      "Epoch 0, Loss: 1.6633764505386353\n",
      "Epoch 0, Loss: 1.7538090944290161\n",
      "Epoch 0, Loss: 1.9537432193756104\n",
      "Epoch 0, Loss: 1.5408451557159424\n",
      "Epoch 0, Loss: 2.0181922912597656\n",
      "Epoch 0, Loss: 1.9438436031341553\n",
      "Epoch 0, Loss: 1.9143803119659424\n",
      "Epoch 0, Loss: 1.7948527336120605\n",
      "Epoch 0, Loss: 1.7069571018218994\n",
      "Epoch 0, Loss: 1.7748850584030151\n",
      "Epoch 0, Loss: 1.8667664527893066\n",
      "Epoch 0, Loss: 1.7573446035385132\n",
      "Epoch 0, Loss: 1.7409292459487915\n",
      "Epoch 0, Loss: 1.9165968894958496\n",
      "Epoch 0, Loss: 1.7666367292404175\n",
      "Epoch 0, Loss: 1.8116307258605957\n",
      "Epoch 0, Loss: 2.233245849609375\n",
      "Epoch 0, Loss: 1.9596588611602783\n",
      "Epoch 0, Loss: 2.0075812339782715\n",
      "Epoch 0, Loss: 1.716322422027588\n",
      "Epoch 0, Loss: 1.7236195802688599\n",
      "Epoch 0, Loss: 2.044489622116089\n",
      "Epoch 0, Loss: 1.618316888809204\n",
      "Epoch 0, Loss: 2.0748629570007324\n",
      "Epoch 0, Loss: 1.9103350639343262\n",
      "Epoch 0, Loss: 1.631504774093628\n",
      "Epoch 0, Loss: 2.0172972679138184\n",
      "Epoch 0, Loss: 1.7887898683547974\n",
      "Epoch 0, Loss: 1.7015814781188965\n",
      "Epoch 0, Loss: 1.7825332880020142\n",
      "Epoch 0, Loss: 1.7434837818145752\n",
      "Epoch 0, Loss: 1.7674955129623413\n",
      "Epoch 0, Loss: 1.929113507270813\n",
      "Epoch 0, Loss: 2.351846218109131\n",
      "Epoch 0, Loss: 1.8265745639801025\n",
      "Epoch 0, Loss: 1.8882548809051514\n",
      "Epoch 0, Loss: 1.816835641860962\n",
      "Epoch 0, Loss: 1.8234244585037231\n",
      "Epoch 0, Loss: 2.117098808288574\n",
      "Epoch 0, Loss: 1.8445062637329102\n",
      "Epoch 0, Loss: 1.9641332626342773\n",
      "Epoch 0, Loss: 2.0074281692504883\n",
      "Epoch 0, Loss: 2.0476863384246826\n",
      "Epoch 0, Loss: 1.6305108070373535\n",
      "Epoch 0, Loss: 1.851981282234192\n",
      "Epoch 0, Loss: 2.0298264026641846\n",
      "Epoch 0, Loss: 1.9832004308700562\n",
      "Epoch 0, Loss: 1.864495038986206\n",
      "Epoch 0, Loss: 1.8618786334991455\n",
      "Epoch 0, Loss: 1.7009143829345703\n",
      "Epoch 0, Loss: 1.8538835048675537\n",
      "Epoch 0, Loss: 1.8419148921966553\n",
      "Epoch 0, Loss: 1.7478500604629517\n",
      "Epoch 0, Loss: 2.0825088024139404\n",
      "Epoch 0, Loss: 1.967712640762329\n",
      "Epoch 0, Loss: 1.7725064754486084\n",
      "Epoch 0, Loss: 1.7991666793823242\n",
      "Epoch 0, Loss: 1.8145501613616943\n",
      "Epoch 0, Loss: 1.93711519241333\n",
      "Epoch 0, Loss: 2.2719688415527344\n",
      "Epoch 1, Loss: 1.8426523208618164\n",
      "Epoch 1, Loss: 1.7420103549957275\n",
      "Epoch 1, Loss: 1.969801902770996\n",
      "Epoch 1, Loss: 1.9219800233840942\n",
      "Epoch 1, Loss: 1.9572131633758545\n",
      "Epoch 1, Loss: 1.9943159818649292\n",
      "Epoch 1, Loss: 1.665245771408081\n",
      "Epoch 1, Loss: 2.010321617126465\n",
      "Epoch 1, Loss: 1.7707529067993164\n",
      "Epoch 1, Loss: 1.8466631174087524\n",
      "Epoch 1, Loss: 1.590796947479248\n",
      "Epoch 1, Loss: 1.752872109413147\n",
      "Epoch 1, Loss: 2.018211603164673\n",
      "Epoch 1, Loss: 1.8503776788711548\n",
      "Epoch 1, Loss: 1.8788578510284424\n",
      "Epoch 1, Loss: 1.7565431594848633\n",
      "Epoch 1, Loss: 1.7234892845153809\n",
      "Epoch 1, Loss: 2.0474090576171875\n",
      "Epoch 1, Loss: 2.0328166484832764\n",
      "Epoch 1, Loss: 1.8746271133422852\n",
      "Epoch 1, Loss: 1.582366943359375\n",
      "Epoch 1, Loss: 1.7615985870361328\n",
      "Epoch 1, Loss: 2.208714008331299\n",
      "Epoch 1, Loss: 1.8393460512161255\n",
      "Epoch 1, Loss: 1.9618089199066162\n",
      "Epoch 1, Loss: 1.7747809886932373\n",
      "Epoch 1, Loss: 1.9141181707382202\n",
      "Epoch 1, Loss: 2.157694101333618\n",
      "Epoch 1, Loss: 1.881962776184082\n",
      "Epoch 1, Loss: 1.9628455638885498\n",
      "Epoch 1, Loss: 1.7652124166488647\n",
      "Epoch 1, Loss: 1.6323926448822021\n",
      "Epoch 1, Loss: 1.6333341598510742\n",
      "Epoch 1, Loss: 2.2004549503326416\n",
      "Epoch 1, Loss: 1.88962984085083\n",
      "Epoch 1, Loss: 1.940741777420044\n",
      "Epoch 1, Loss: 1.6402547359466553\n",
      "Epoch 1, Loss: 1.9673569202423096\n",
      "Epoch 1, Loss: 1.9709115028381348\n",
      "Epoch 1, Loss: 1.6450753211975098\n",
      "Epoch 1, Loss: 1.9818333387374878\n",
      "Epoch 1, Loss: 1.682823657989502\n",
      "Epoch 1, Loss: 1.8611507415771484\n",
      "Epoch 1, Loss: 1.8769482374191284\n",
      "Epoch 1, Loss: 1.9488375186920166\n",
      "Epoch 1, Loss: 1.7096794843673706\n",
      "Epoch 1, Loss: 1.8003268241882324\n",
      "Epoch 1, Loss: 1.8724370002746582\n",
      "Epoch 1, Loss: 1.9927409887313843\n",
      "Epoch 1, Loss: 1.8535842895507812\n",
      "Epoch 1, Loss: 1.9677765369415283\n",
      "Epoch 1, Loss: 1.8793282508850098\n",
      "Epoch 1, Loss: 1.6920809745788574\n",
      "Epoch 1, Loss: 1.9701054096221924\n",
      "Epoch 1, Loss: 2.3966689109802246\n",
      "Epoch 1, Loss: 1.8899054527282715\n",
      "Epoch 1, Loss: 2.1007437705993652\n",
      "Epoch 1, Loss: 1.835949182510376\n",
      "Epoch 1, Loss: 1.9582828283309937\n",
      "Epoch 1, Loss: 2.1039071083068848\n",
      "Epoch 1, Loss: 1.6993019580841064\n",
      "Epoch 1, Loss: 1.9620552062988281\n",
      "Epoch 1, Loss: 1.810668706893921\n",
      "Epoch 1, Loss: 1.9520267248153687\n",
      "Epoch 1, Loss: 1.9550102949142456\n",
      "Epoch 1, Loss: 2.053271770477295\n",
      "Epoch 1, Loss: 1.6910085678100586\n",
      "Epoch 1, Loss: 1.7127580642700195\n",
      "Epoch 1, Loss: 1.7152366638183594\n",
      "Epoch 1, Loss: 1.9140173196792603\n",
      "Epoch 1, Loss: 2.034971237182617\n",
      "Epoch 1, Loss: 1.8076727390289307\n",
      "Epoch 1, Loss: 1.8449881076812744\n",
      "Epoch 1, Loss: 1.7670115232467651\n",
      "Epoch 1, Loss: 2.1939170360565186\n",
      "Epoch 2, Loss: 2.086210250854492\n",
      "Epoch 2, Loss: 1.7695335149765015\n",
      "Epoch 2, Loss: 2.152392864227295\n",
      "Epoch 2, Loss: 1.850888729095459\n",
      "Epoch 2, Loss: 1.7805862426757812\n",
      "Epoch 2, Loss: 1.788425087928772\n",
      "Epoch 2, Loss: 1.9279731512069702\n",
      "Epoch 2, Loss: 2.0601248741149902\n",
      "Epoch 2, Loss: 1.7748123407363892\n",
      "Epoch 2, Loss: 1.7801282405853271\n",
      "Epoch 2, Loss: 2.0617599487304688\n",
      "Epoch 2, Loss: 1.8378827571868896\n",
      "Epoch 2, Loss: 1.8482646942138672\n",
      "Epoch 2, Loss: 1.9110527038574219\n",
      "Epoch 2, Loss: 1.8275983333587646\n",
      "Epoch 2, Loss: 1.6643040180206299\n",
      "Epoch 2, Loss: 1.8325490951538086\n",
      "Epoch 2, Loss: 2.001786470413208\n",
      "Epoch 2, Loss: 2.093491315841675\n",
      "Epoch 2, Loss: 1.805912971496582\n",
      "Epoch 2, Loss: 1.8599882125854492\n",
      "Epoch 2, Loss: 2.010895013809204\n",
      "Epoch 2, Loss: 1.6627193689346313\n",
      "Epoch 2, Loss: 1.9323561191558838\n",
      "Epoch 2, Loss: 1.8132332563400269\n",
      "Epoch 2, Loss: 1.8171617984771729\n",
      "Epoch 2, Loss: 1.9498645067214966\n",
      "Epoch 2, Loss: 1.8327646255493164\n",
      "Epoch 2, Loss: 1.8059827089309692\n",
      "Epoch 2, Loss: 2.0122151374816895\n",
      "Epoch 2, Loss: 1.805286169052124\n",
      "Epoch 2, Loss: 1.7331576347351074\n",
      "Epoch 2, Loss: 1.636606216430664\n",
      "Epoch 2, Loss: 2.047394275665283\n",
      "Epoch 2, Loss: 1.6770422458648682\n",
      "Epoch 2, Loss: 1.9384722709655762\n",
      "Epoch 2, Loss: 1.7641181945800781\n",
      "Epoch 2, Loss: 1.6142847537994385\n",
      "Epoch 2, Loss: 2.0200226306915283\n",
      "Epoch 2, Loss: 1.73880934715271\n",
      "Epoch 2, Loss: 1.7788200378417969\n",
      "Epoch 2, Loss: 1.8984800577163696\n",
      "Epoch 2, Loss: 1.781179428100586\n",
      "Epoch 2, Loss: 1.7647278308868408\n",
      "Epoch 2, Loss: 1.6890777349472046\n",
      "Epoch 2, Loss: 1.8416552543640137\n",
      "Epoch 2, Loss: 2.1002068519592285\n",
      "Epoch 2, Loss: 1.6446268558502197\n",
      "Epoch 2, Loss: 1.7807059288024902\n",
      "Epoch 2, Loss: 1.917137622833252\n",
      "Epoch 2, Loss: 1.8103467226028442\n",
      "Epoch 2, Loss: 2.180586338043213\n",
      "Epoch 2, Loss: 1.879883885383606\n",
      "Epoch 2, Loss: 1.5829393863677979\n",
      "Epoch 2, Loss: 1.9060420989990234\n",
      "Epoch 2, Loss: 1.872437596321106\n",
      "Epoch 2, Loss: 2.0046660900115967\n",
      "Epoch 2, Loss: 1.987052083015442\n",
      "Epoch 2, Loss: 1.898578405380249\n",
      "Epoch 2, Loss: 1.7221128940582275\n",
      "Epoch 2, Loss: 1.6921675205230713\n",
      "Epoch 2, Loss: 2.1554322242736816\n",
      "Epoch 2, Loss: 1.9315977096557617\n",
      "Epoch 2, Loss: 1.8933758735656738\n",
      "Epoch 2, Loss: 1.7628027200698853\n",
      "Epoch 2, Loss: 2.0964972972869873\n",
      "Epoch 2, Loss: 1.8521339893341064\n",
      "Epoch 2, Loss: 1.8146673440933228\n",
      "Epoch 2, Loss: 2.20796537399292\n",
      "Epoch 2, Loss: 1.6005034446716309\n",
      "Epoch 2, Loss: 2.0193843841552734\n",
      "Epoch 2, Loss: 1.7399048805236816\n",
      "Epoch 2, Loss: 1.9875452518463135\n",
      "Epoch 2, Loss: 1.9662648439407349\n",
      "Epoch 2, Loss: 2.4581899642944336\n",
      "Epoch 3, Loss: 1.9627406597137451\n",
      "Epoch 3, Loss: 1.9044992923736572\n",
      "Epoch 3, Loss: 1.8864197731018066\n",
      "Epoch 3, Loss: 1.736417293548584\n",
      "Epoch 3, Loss: 2.119983196258545\n",
      "Epoch 3, Loss: 1.947826862335205\n",
      "Epoch 3, Loss: 1.7324798107147217\n",
      "Epoch 3, Loss: 1.755183458328247\n",
      "Epoch 3, Loss: 1.6845731735229492\n",
      "Epoch 3, Loss: 1.7543163299560547\n",
      "Epoch 3, Loss: 1.7517361640930176\n",
      "Epoch 3, Loss: 2.011913776397705\n",
      "Epoch 3, Loss: 1.823626160621643\n",
      "Epoch 3, Loss: 1.966400384902954\n",
      "Epoch 3, Loss: 1.7550673484802246\n",
      "Epoch 3, Loss: 2.262399196624756\n",
      "Epoch 3, Loss: 1.8698804378509521\n",
      "Epoch 3, Loss: 1.7000062465667725\n",
      "Epoch 3, Loss: 1.845388412475586\n",
      "Epoch 3, Loss: 1.8684442043304443\n",
      "Epoch 3, Loss: 2.0929574966430664\n",
      "Epoch 3, Loss: 1.967113971710205\n",
      "Epoch 3, Loss: 1.8280012607574463\n",
      "Epoch 3, Loss: 1.7554051876068115\n",
      "Epoch 3, Loss: 1.8760462999343872\n",
      "Epoch 3, Loss: 1.984304666519165\n",
      "Epoch 3, Loss: 1.8755327463150024\n",
      "Epoch 3, Loss: 1.959193229675293\n",
      "Epoch 3, Loss: 2.07136607170105\n",
      "Epoch 3, Loss: 1.8055553436279297\n",
      "Epoch 3, Loss: 1.804781436920166\n",
      "Epoch 3, Loss: 1.5592522621154785\n",
      "Epoch 3, Loss: 1.9888073205947876\n",
      "Epoch 3, Loss: 1.9640785455703735\n",
      "Epoch 3, Loss: 1.7610282897949219\n",
      "Epoch 3, Loss: 1.9272080659866333\n",
      "Epoch 3, Loss: 2.0900814533233643\n",
      "Epoch 3, Loss: 1.7189983129501343\n",
      "Epoch 3, Loss: 2.131620407104492\n",
      "Epoch 3, Loss: 1.647615909576416\n",
      "Epoch 3, Loss: 1.8920073509216309\n",
      "Epoch 3, Loss: 1.922753930091858\n",
      "Epoch 3, Loss: 2.137855291366577\n",
      "Epoch 3, Loss: 1.858583927154541\n",
      "Epoch 3, Loss: 1.9082043170928955\n",
      "Epoch 3, Loss: 1.6633551120758057\n",
      "Epoch 3, Loss: 1.8626540899276733\n",
      "Epoch 3, Loss: 2.0725274085998535\n",
      "Epoch 3, Loss: 2.0648083686828613\n",
      "Epoch 3, Loss: 2.1173582077026367\n",
      "Epoch 3, Loss: 1.8912248611450195\n",
      "Epoch 3, Loss: 1.5775771141052246\n",
      "Epoch 3, Loss: 1.855133056640625\n",
      "Epoch 3, Loss: 1.7795052528381348\n",
      "Epoch 3, Loss: 1.6870521306991577\n",
      "Epoch 3, Loss: 1.8557624816894531\n",
      "Epoch 3, Loss: 1.735836386680603\n",
      "Epoch 3, Loss: 1.9482005834579468\n",
      "Epoch 3, Loss: 1.9413458108901978\n",
      "Epoch 3, Loss: 1.9337198734283447\n",
      "Epoch 3, Loss: 1.6755213737487793\n",
      "Epoch 3, Loss: 1.8724836111068726\n",
      "Epoch 3, Loss: 2.0249180793762207\n",
      "Epoch 3, Loss: 1.748014211654663\n",
      "Epoch 3, Loss: 1.8751680850982666\n",
      "Epoch 3, Loss: 1.7299323081970215\n",
      "Epoch 3, Loss: 2.082944869995117\n",
      "Epoch 3, Loss: 2.235714912414551\n",
      "Epoch 3, Loss: 1.6917610168457031\n",
      "Epoch 3, Loss: 1.5904690027236938\n",
      "Epoch 3, Loss: 1.7743793725967407\n",
      "Epoch 3, Loss: 1.9195374250411987\n",
      "Epoch 3, Loss: 1.7636308670043945\n",
      "Epoch 3, Loss: 1.7861032485961914\n",
      "Epoch 3, Loss: 1.803633451461792\n",
      "Epoch 4, Loss: 1.8452280759811401\n",
      "Epoch 4, Loss: 1.8912461996078491\n",
      "Epoch 4, Loss: 1.719979166984558\n",
      "Epoch 4, Loss: 1.7040094137191772\n",
      "Epoch 4, Loss: 1.682180404663086\n",
      "Epoch 4, Loss: 1.9555076360702515\n",
      "Epoch 4, Loss: 1.6736271381378174\n",
      "Epoch 4, Loss: 1.7217515707015991\n",
      "Epoch 4, Loss: 1.9452162981033325\n",
      "Epoch 4, Loss: 2.046217203140259\n",
      "Epoch 4, Loss: 1.9325432777404785\n",
      "Epoch 4, Loss: 2.1366958618164062\n",
      "Epoch 4, Loss: 2.040238618850708\n",
      "Epoch 4, Loss: 2.1177303791046143\n",
      "Epoch 4, Loss: 1.8656938076019287\n",
      "Epoch 4, Loss: 2.0245556831359863\n",
      "Epoch 4, Loss: 1.9212913513183594\n",
      "Epoch 4, Loss: 1.7918894290924072\n",
      "Epoch 4, Loss: 1.8040130138397217\n",
      "Epoch 4, Loss: 1.8880250453948975\n",
      "Epoch 4, Loss: 1.757229208946228\n",
      "Epoch 4, Loss: 1.9392507076263428\n",
      "Epoch 4, Loss: 2.1008710861206055\n",
      "Epoch 4, Loss: 1.674823522567749\n",
      "Epoch 4, Loss: 1.8798540830612183\n",
      "Epoch 4, Loss: 2.0080931186676025\n",
      "Epoch 4, Loss: 1.6860888004302979\n",
      "Epoch 4, Loss: 1.7574491500854492\n",
      "Epoch 4, Loss: 1.8323819637298584\n",
      "Epoch 4, Loss: 1.8763246536254883\n",
      "Epoch 4, Loss: 1.760293960571289\n",
      "Epoch 4, Loss: 1.9354894161224365\n",
      "Epoch 4, Loss: 1.7248668670654297\n",
      "Epoch 4, Loss: 1.7505398988723755\n",
      "Epoch 4, Loss: 1.7602667808532715\n",
      "Epoch 4, Loss: 1.8886340856552124\n",
      "Epoch 4, Loss: 1.7722504138946533\n",
      "Epoch 4, Loss: 1.8697154521942139\n",
      "Epoch 4, Loss: 1.7497657537460327\n",
      "Epoch 4, Loss: 1.768790602684021\n",
      "Epoch 4, Loss: 1.7687592506408691\n",
      "Epoch 4, Loss: 1.8146361112594604\n",
      "Epoch 4, Loss: 1.718060851097107\n",
      "Epoch 4, Loss: 1.7544119358062744\n",
      "Epoch 4, Loss: 2.008977174758911\n",
      "Epoch 4, Loss: 1.9578137397766113\n",
      "Epoch 4, Loss: 1.8811320066452026\n",
      "Epoch 4, Loss: 2.200202703475952\n",
      "Epoch 4, Loss: 2.1956353187561035\n",
      "Epoch 4, Loss: 1.7584823369979858\n",
      "Epoch 4, Loss: 1.9647178649902344\n",
      "Epoch 4, Loss: 1.7155888080596924\n",
      "Epoch 4, Loss: 1.9152120351791382\n",
      "Epoch 4, Loss: 1.865646243095398\n",
      "Epoch 4, Loss: 2.0237655639648438\n",
      "Epoch 4, Loss: 1.9342327117919922\n",
      "Epoch 4, Loss: 2.0448644161224365\n",
      "Epoch 4, Loss: 2.0774927139282227\n",
      "Epoch 4, Loss: 1.7220277786254883\n",
      "Epoch 4, Loss: 1.8286733627319336\n",
      "Epoch 4, Loss: 1.854016661643982\n",
      "Epoch 4, Loss: 2.0003035068511963\n",
      "Epoch 4, Loss: 1.8950567245483398\n",
      "Epoch 4, Loss: 1.7912020683288574\n",
      "Epoch 4, Loss: 1.5998424291610718\n",
      "Epoch 4, Loss: 1.7908953428268433\n",
      "Epoch 4, Loss: 1.7052942514419556\n",
      "Epoch 4, Loss: 1.9001986980438232\n",
      "Epoch 4, Loss: 2.0358262062072754\n",
      "Epoch 4, Loss: 2.185664415359497\n",
      "Epoch 4, Loss: 1.9366588592529297\n",
      "Epoch 4, Loss: 1.7508158683776855\n",
      "Epoch 4, Loss: 1.7211451530456543\n",
      "Epoch 4, Loss: 1.8892784118652344\n",
      "Epoch 4, Loss: 1.925828456878662\n",
      "Epoch 5, Loss: 1.6587893962860107\n",
      "Epoch 5, Loss: 1.986680030822754\n",
      "Epoch 5, Loss: 1.7266972064971924\n",
      "Epoch 5, Loss: 1.9519767761230469\n",
      "Epoch 5, Loss: 1.629018783569336\n",
      "Epoch 5, Loss: 1.9648711681365967\n",
      "Epoch 5, Loss: 2.0201761722564697\n",
      "Epoch 5, Loss: 1.825897216796875\n",
      "Epoch 5, Loss: 1.9896403551101685\n",
      "Epoch 5, Loss: 1.9901738166809082\n",
      "Epoch 5, Loss: 2.135352611541748\n",
      "Epoch 5, Loss: 1.9516757726669312\n",
      "Epoch 5, Loss: 1.7857658863067627\n",
      "Epoch 5, Loss: 2.076850175857544\n",
      "Epoch 5, Loss: 1.6754074096679688\n",
      "Epoch 5, Loss: 1.8624622821807861\n",
      "Epoch 5, Loss: 1.8525097370147705\n",
      "Epoch 5, Loss: 1.8785545825958252\n",
      "Epoch 5, Loss: 1.822761058807373\n",
      "Epoch 5, Loss: 1.6909841299057007\n",
      "Epoch 5, Loss: 2.2234835624694824\n",
      "Epoch 5, Loss: 1.6804847717285156\n",
      "Epoch 5, Loss: 1.6785073280334473\n",
      "Epoch 5, Loss: 1.6611716747283936\n",
      "Epoch 5, Loss: 2.239917755126953\n",
      "Epoch 5, Loss: 1.8695621490478516\n",
      "Epoch 5, Loss: 1.6867848634719849\n",
      "Epoch 5, Loss: 1.9324463605880737\n",
      "Epoch 5, Loss: 1.7093586921691895\n",
      "Epoch 5, Loss: 2.0222389698028564\n",
      "Epoch 5, Loss: 1.924534797668457\n",
      "Epoch 5, Loss: 1.6265696287155151\n",
      "Epoch 5, Loss: 1.8745198249816895\n",
      "Epoch 5, Loss: 1.6050992012023926\n",
      "Epoch 5, Loss: 2.0084762573242188\n",
      "Epoch 5, Loss: 1.7487962245941162\n",
      "Epoch 5, Loss: 1.8982796669006348\n",
      "Epoch 5, Loss: 1.994619369506836\n",
      "Epoch 5, Loss: 1.7666016817092896\n",
      "Epoch 5, Loss: 1.8267195224761963\n",
      "Epoch 5, Loss: 1.953936219215393\n",
      "Epoch 5, Loss: 1.9377771615982056\n",
      "Epoch 5, Loss: 1.9404075145721436\n",
      "Epoch 5, Loss: 1.9071173667907715\n",
      "Epoch 5, Loss: 1.911290168762207\n",
      "Epoch 5, Loss: 1.7535735368728638\n",
      "Epoch 5, Loss: 1.934680461883545\n",
      "Epoch 5, Loss: 1.518681526184082\n",
      "Epoch 5, Loss: 1.7871379852294922\n",
      "Epoch 5, Loss: 1.8081018924713135\n",
      "Epoch 5, Loss: 1.5534576177597046\n",
      "Epoch 5, Loss: 1.721837043762207\n",
      "Epoch 5, Loss: 1.905085802078247\n",
      "Epoch 5, Loss: 1.7148737907409668\n",
      "Epoch 5, Loss: 1.8290936946868896\n",
      "Epoch 5, Loss: 1.5995477437973022\n",
      "Epoch 5, Loss: 2.06833815574646\n",
      "Epoch 5, Loss: 1.746516466140747\n",
      "Epoch 5, Loss: 1.9262592792510986\n",
      "Epoch 5, Loss: 2.0592362880706787\n",
      "Epoch 5, Loss: 2.090444326400757\n",
      "Epoch 5, Loss: 1.8180789947509766\n",
      "Epoch 5, Loss: 1.9082715511322021\n",
      "Epoch 5, Loss: 1.7548350095748901\n",
      "Epoch 5, Loss: 2.2552855014801025\n",
      "Epoch 5, Loss: 1.9827830791473389\n",
      "Epoch 5, Loss: 1.8125946521759033\n",
      "Epoch 5, Loss: 1.8557026386260986\n",
      "Epoch 5, Loss: 1.8166245222091675\n",
      "Epoch 5, Loss: 1.8957035541534424\n",
      "Epoch 5, Loss: 1.8928800821304321\n",
      "Epoch 5, Loss: 2.0486419200897217\n",
      "Epoch 5, Loss: 1.897249698638916\n",
      "Epoch 5, Loss: 1.7768889665603638\n",
      "Epoch 5, Loss: 2.1823313236236572\n",
      "Epoch 6, Loss: 1.7068994045257568\n",
      "Epoch 6, Loss: 1.9651896953582764\n",
      "Epoch 6, Loss: 1.9123404026031494\n",
      "Epoch 6, Loss: 1.8533210754394531\n",
      "Epoch 6, Loss: 1.45884370803833\n",
      "Epoch 6, Loss: 1.762265920639038\n",
      "Epoch 6, Loss: 1.808870792388916\n",
      "Epoch 6, Loss: 1.793665885925293\n",
      "Epoch 6, Loss: 1.9112048149108887\n",
      "Epoch 6, Loss: 1.960851788520813\n",
      "Epoch 6, Loss: 1.905465006828308\n",
      "Epoch 6, Loss: 1.5868926048278809\n",
      "Epoch 6, Loss: 1.6356632709503174\n",
      "Epoch 6, Loss: 1.8123269081115723\n",
      "Epoch 6, Loss: 1.8860670328140259\n",
      "Epoch 6, Loss: 2.2663421630859375\n",
      "Epoch 6, Loss: 1.9123833179473877\n",
      "Epoch 6, Loss: 1.5988976955413818\n",
      "Epoch 6, Loss: 1.8757859468460083\n",
      "Epoch 6, Loss: 2.171525001525879\n",
      "Epoch 6, Loss: 1.941413402557373\n",
      "Epoch 6, Loss: 1.9301249980926514\n",
      "Epoch 6, Loss: 1.8146600723266602\n",
      "Epoch 6, Loss: 1.7791900634765625\n",
      "Epoch 6, Loss: 1.8931914567947388\n",
      "Epoch 6, Loss: 1.945652961730957\n",
      "Epoch 6, Loss: 1.689685344696045\n",
      "Epoch 6, Loss: 1.8609659671783447\n",
      "Epoch 6, Loss: 1.493647813796997\n",
      "Epoch 6, Loss: 1.8622475862503052\n",
      "Epoch 6, Loss: 2.047698736190796\n",
      "Epoch 6, Loss: 2.156359910964966\n",
      "Epoch 6, Loss: 1.531520128250122\n",
      "Epoch 6, Loss: 1.8321775197982788\n",
      "Epoch 6, Loss: 1.8390671014785767\n",
      "Epoch 6, Loss: 1.8532204627990723\n",
      "Epoch 6, Loss: 1.6042689085006714\n",
      "Epoch 6, Loss: 1.897052526473999\n",
      "Epoch 6, Loss: 1.9368469715118408\n",
      "Epoch 6, Loss: 1.709498405456543\n",
      "Epoch 6, Loss: 1.834350347518921\n",
      "Epoch 6, Loss: 1.738688349723816\n",
      "Epoch 6, Loss: 2.2222766876220703\n",
      "Epoch 6, Loss: 2.196944236755371\n",
      "Epoch 6, Loss: 1.9731701612472534\n",
      "Epoch 6, Loss: 1.7154088020324707\n",
      "Epoch 6, Loss: 2.3143913745880127\n",
      "Epoch 6, Loss: 1.6896177530288696\n",
      "Epoch 6, Loss: 2.0176620483398438\n",
      "Epoch 6, Loss: 1.6340547800064087\n",
      "Epoch 6, Loss: 2.0401852130889893\n",
      "Epoch 6, Loss: 1.6974501609802246\n",
      "Epoch 6, Loss: 1.9015432596206665\n",
      "Epoch 6, Loss: 2.027146100997925\n",
      "Epoch 6, Loss: 1.6836578845977783\n",
      "Epoch 6, Loss: 1.8952444791793823\n",
      "Epoch 6, Loss: 1.9986841678619385\n",
      "Epoch 6, Loss: 2.118753433227539\n",
      "Epoch 6, Loss: 1.8738036155700684\n",
      "Epoch 6, Loss: 1.783098578453064\n",
      "Epoch 6, Loss: 1.9823437929153442\n",
      "Epoch 6, Loss: 2.04085111618042\n",
      "Epoch 6, Loss: 1.7888935804367065\n",
      "Epoch 6, Loss: 1.7899726629257202\n",
      "Epoch 6, Loss: 1.7581002712249756\n",
      "Epoch 6, Loss: 1.8688030242919922\n",
      "Epoch 6, Loss: 1.980224847793579\n",
      "Epoch 6, Loss: 1.6308114528656006\n",
      "Epoch 6, Loss: 1.7674299478530884\n",
      "Epoch 6, Loss: 1.798393726348877\n",
      "Epoch 6, Loss: 1.9484955072402954\n",
      "Epoch 6, Loss: 1.8034732341766357\n",
      "Epoch 6, Loss: 1.733451247215271\n",
      "Epoch 6, Loss: 1.8955466747283936\n",
      "Epoch 6, Loss: 2.1707613468170166\n",
      "Epoch 7, Loss: 1.7053184509277344\n",
      "Epoch 7, Loss: 1.7653788328170776\n",
      "Epoch 7, Loss: 1.9093639850616455\n",
      "Epoch 7, Loss: 1.9449775218963623\n",
      "Epoch 7, Loss: 1.9687135219573975\n",
      "Epoch 7, Loss: 1.773544192314148\n",
      "Epoch 7, Loss: 2.0710208415985107\n",
      "Epoch 7, Loss: 2.20180082321167\n",
      "Epoch 7, Loss: 1.9422426223754883\n",
      "Epoch 7, Loss: 1.6892733573913574\n",
      "Epoch 7, Loss: 1.8587136268615723\n",
      "Epoch 7, Loss: 2.251931667327881\n",
      "Epoch 7, Loss: 1.5372694730758667\n",
      "Epoch 7, Loss: 1.8228716850280762\n",
      "Epoch 7, Loss: 1.978088617324829\n",
      "Epoch 7, Loss: 2.1484341621398926\n",
      "Epoch 7, Loss: 2.1086833477020264\n",
      "Epoch 7, Loss: 1.7064604759216309\n",
      "Epoch 7, Loss: 1.770453691482544\n",
      "Epoch 7, Loss: 1.8370487689971924\n",
      "Epoch 7, Loss: 1.728510856628418\n",
      "Epoch 7, Loss: 1.797001838684082\n",
      "Epoch 7, Loss: 1.7724909782409668\n",
      "Epoch 7, Loss: 1.8445450067520142\n",
      "Epoch 7, Loss: 1.9410631656646729\n",
      "Epoch 7, Loss: 1.6807196140289307\n",
      "Epoch 7, Loss: 1.766237735748291\n",
      "Epoch 7, Loss: 1.6906359195709229\n",
      "Epoch 7, Loss: 1.966158151626587\n",
      "Epoch 7, Loss: 1.7726634740829468\n",
      "Epoch 7, Loss: 2.0632987022399902\n",
      "Epoch 7, Loss: 1.7268248796463013\n",
      "Epoch 7, Loss: 1.767326831817627\n",
      "Epoch 7, Loss: 1.8065636157989502\n",
      "Epoch 7, Loss: 1.705260992050171\n",
      "Epoch 7, Loss: 1.9187418222427368\n",
      "Epoch 7, Loss: 1.8781367540359497\n",
      "Epoch 7, Loss: 1.7311058044433594\n",
      "Epoch 7, Loss: 1.6908559799194336\n",
      "Epoch 7, Loss: 1.9667422771453857\n",
      "Epoch 7, Loss: 1.6235337257385254\n",
      "Epoch 7, Loss: 2.0872206687927246\n",
      "Epoch 7, Loss: 1.9892971515655518\n",
      "Epoch 7, Loss: 1.7212493419647217\n",
      "Epoch 7, Loss: 1.7459735870361328\n",
      "Epoch 7, Loss: 1.840674638748169\n",
      "Epoch 7, Loss: 1.7591227293014526\n",
      "Epoch 7, Loss: 1.787550926208496\n",
      "Epoch 7, Loss: 1.8932377099990845\n",
      "Epoch 7, Loss: 1.7962024211883545\n",
      "Epoch 7, Loss: 1.8273744583129883\n",
      "Epoch 7, Loss: 1.8373968601226807\n",
      "Epoch 7, Loss: 1.8776336908340454\n",
      "Epoch 7, Loss: 1.6733676195144653\n",
      "Epoch 7, Loss: 1.9271130561828613\n",
      "Epoch 7, Loss: 1.7725857496261597\n",
      "Epoch 7, Loss: 1.7931013107299805\n",
      "Epoch 7, Loss: 2.1182234287261963\n",
      "Epoch 7, Loss: 1.670959234237671\n",
      "Epoch 7, Loss: 2.1205358505249023\n",
      "Epoch 7, Loss: 1.6454521417617798\n",
      "Epoch 7, Loss: 1.9412832260131836\n",
      "Epoch 7, Loss: 1.894330620765686\n",
      "Epoch 7, Loss: 2.029146432876587\n",
      "Epoch 7, Loss: 1.7831389904022217\n",
      "Epoch 7, Loss: 1.8043534755706787\n",
      "Epoch 7, Loss: 1.6627631187438965\n",
      "Epoch 7, Loss: 2.303205966949463\n",
      "Epoch 7, Loss: 1.7360913753509521\n",
      "Epoch 7, Loss: 2.0470194816589355\n",
      "Epoch 7, Loss: 1.6385741233825684\n",
      "Epoch 7, Loss: 1.818678617477417\n",
      "Epoch 7, Loss: 2.092801570892334\n",
      "Epoch 7, Loss: 1.8925371170043945\n",
      "Epoch 7, Loss: 1.9462316036224365\n",
      "Epoch 8, Loss: 1.896470069885254\n",
      "Epoch 8, Loss: 1.9164209365844727\n",
      "Epoch 8, Loss: 2.163123607635498\n",
      "Epoch 8, Loss: 2.230111598968506\n",
      "Epoch 8, Loss: 1.9852149486541748\n",
      "Epoch 8, Loss: 1.8630009889602661\n",
      "Epoch 8, Loss: 2.011962890625\n",
      "Epoch 8, Loss: 1.7817152738571167\n",
      "Epoch 8, Loss: 2.2963342666625977\n",
      "Epoch 8, Loss: 2.0231592655181885\n",
      "Epoch 8, Loss: 1.508481502532959\n",
      "Epoch 8, Loss: 2.0087010860443115\n",
      "Epoch 8, Loss: 1.7595033645629883\n",
      "Epoch 8, Loss: 2.0797996520996094\n",
      "Epoch 8, Loss: 1.7106642723083496\n",
      "Epoch 8, Loss: 1.6456632614135742\n",
      "Epoch 8, Loss: 1.671644926071167\n",
      "Epoch 8, Loss: 1.9190815687179565\n",
      "Epoch 8, Loss: 1.9234782457351685\n",
      "Epoch 8, Loss: 1.733066201210022\n",
      "Epoch 8, Loss: 1.7473235130310059\n",
      "Epoch 8, Loss: 1.5661134719848633\n",
      "Epoch 8, Loss: 1.892529010772705\n",
      "Epoch 8, Loss: 1.8504996299743652\n",
      "Epoch 8, Loss: 1.832342505455017\n",
      "Epoch 8, Loss: 1.6627870798110962\n",
      "Epoch 8, Loss: 1.9458116292953491\n",
      "Epoch 8, Loss: 1.826629877090454\n",
      "Epoch 8, Loss: 1.7785390615463257\n",
      "Epoch 8, Loss: 1.8928194046020508\n",
      "Epoch 8, Loss: 1.7013745307922363\n",
      "Epoch 8, Loss: 2.069276809692383\n",
      "Epoch 8, Loss: 2.040163516998291\n",
      "Epoch 8, Loss: 1.899221420288086\n",
      "Epoch 8, Loss: 1.872270107269287\n",
      "Epoch 8, Loss: 1.6949418783187866\n",
      "Epoch 8, Loss: 1.9557346105575562\n",
      "Epoch 8, Loss: 1.8436191082000732\n",
      "Epoch 8, Loss: 1.835580825805664\n",
      "Epoch 8, Loss: 1.9812853336334229\n",
      "Epoch 8, Loss: 1.8957836627960205\n",
      "Epoch 8, Loss: 1.9808619022369385\n",
      "Epoch 8, Loss: 1.7269127368927002\n",
      "Epoch 8, Loss: 1.7798678874969482\n",
      "Epoch 8, Loss: 1.765401840209961\n",
      "Epoch 8, Loss: 1.6781518459320068\n",
      "Epoch 8, Loss: 1.8307178020477295\n",
      "Epoch 8, Loss: 1.836411714553833\n",
      "Epoch 8, Loss: 1.542923927307129\n",
      "Epoch 8, Loss: 1.8770177364349365\n",
      "Epoch 8, Loss: 1.694838047027588\n",
      "Epoch 8, Loss: 1.75691556930542\n",
      "Epoch 8, Loss: 2.0119826793670654\n",
      "Epoch 8, Loss: 1.5733164548873901\n",
      "Epoch 8, Loss: 1.6613985300064087\n",
      "Epoch 8, Loss: 1.997071385383606\n",
      "Epoch 8, Loss: 1.8558690547943115\n",
      "Epoch 8, Loss: 2.210253953933716\n",
      "Epoch 8, Loss: 1.8985011577606201\n",
      "Epoch 8, Loss: 1.8689332008361816\n",
      "Epoch 8, Loss: 2.125857353210449\n",
      "Epoch 8, Loss: 1.5886222124099731\n",
      "Epoch 8, Loss: 1.7265617847442627\n",
      "Epoch 8, Loss: 2.004760265350342\n",
      "Epoch 8, Loss: 2.09104585647583\n",
      "Epoch 8, Loss: 1.7674143314361572\n",
      "Epoch 8, Loss: 1.8477851152420044\n",
      "Epoch 8, Loss: 1.6147642135620117\n",
      "Epoch 8, Loss: 1.862278699874878\n",
      "Epoch 8, Loss: 1.6040771007537842\n",
      "Epoch 8, Loss: 1.829068899154663\n",
      "Epoch 8, Loss: 1.9732334613800049\n",
      "Epoch 8, Loss: 1.8234519958496094\n",
      "Epoch 8, Loss: 1.8911738395690918\n",
      "Epoch 8, Loss: 1.9524037837982178\n",
      "Epoch 9, Loss: 1.8026721477508545\n",
      "Epoch 9, Loss: 1.947157859802246\n",
      "Epoch 9, Loss: 1.8701598644256592\n",
      "Epoch 9, Loss: 1.7559078931808472\n",
      "Epoch 9, Loss: 1.8814951181411743\n",
      "Epoch 9, Loss: 2.0384762287139893\n",
      "Epoch 9, Loss: 1.7481486797332764\n",
      "Epoch 9, Loss: 1.8737422227859497\n",
      "Epoch 9, Loss: 1.595520257949829\n",
      "Epoch 9, Loss: 2.081171989440918\n",
      "Epoch 9, Loss: 1.815319299697876\n",
      "Epoch 9, Loss: 1.774174451828003\n",
      "Epoch 9, Loss: 1.589937686920166\n",
      "Epoch 9, Loss: 1.7759284973144531\n",
      "Epoch 9, Loss: 1.870037317276001\n",
      "Epoch 9, Loss: 1.8635592460632324\n",
      "Epoch 9, Loss: 1.7453901767730713\n",
      "Epoch 9, Loss: 1.9143924713134766\n",
      "Epoch 9, Loss: 1.7775554656982422\n",
      "Epoch 9, Loss: 1.545814871788025\n",
      "Epoch 9, Loss: 1.975567102432251\n",
      "Epoch 9, Loss: 1.7996883392333984\n",
      "Epoch 9, Loss: 1.846970796585083\n",
      "Epoch 9, Loss: 1.7446575164794922\n",
      "Epoch 9, Loss: 1.570383071899414\n",
      "Epoch 9, Loss: 1.9101741313934326\n",
      "Epoch 9, Loss: 1.8735082149505615\n",
      "Epoch 9, Loss: 1.770935297012329\n",
      "Epoch 9, Loss: 1.7784125804901123\n",
      "Epoch 9, Loss: 2.003066062927246\n",
      "Epoch 9, Loss: 2.0186522006988525\n",
      "Epoch 9, Loss: 1.6751210689544678\n",
      "Epoch 9, Loss: 2.1349663734436035\n",
      "Epoch 9, Loss: 2.09006929397583\n",
      "Epoch 9, Loss: 1.81313157081604\n",
      "Epoch 9, Loss: 1.7797285318374634\n",
      "Epoch 9, Loss: 2.084275245666504\n",
      "Epoch 9, Loss: 1.8574817180633545\n",
      "Epoch 9, Loss: 2.0754523277282715\n",
      "Epoch 9, Loss: 1.816594123840332\n",
      "Epoch 9, Loss: 1.9496755599975586\n",
      "Epoch 9, Loss: 1.9808660745620728\n",
      "Epoch 9, Loss: 2.060225009918213\n",
      "Epoch 9, Loss: 1.926077127456665\n",
      "Epoch 9, Loss: 1.8358361721038818\n",
      "Epoch 9, Loss: 1.7081491947174072\n",
      "Epoch 9, Loss: 1.9862351417541504\n",
      "Epoch 9, Loss: 1.8058985471725464\n",
      "Epoch 9, Loss: 1.9432381391525269\n",
      "Epoch 9, Loss: 1.7887024879455566\n",
      "Epoch 9, Loss: 1.7492647171020508\n",
      "Epoch 9, Loss: 1.9016716480255127\n",
      "Epoch 9, Loss: 1.8774112462997437\n",
      "Epoch 9, Loss: 1.9320838451385498\n",
      "Epoch 9, Loss: 1.9156293869018555\n",
      "Epoch 9, Loss: 1.8853933811187744\n",
      "Epoch 9, Loss: 1.5533795356750488\n",
      "Epoch 9, Loss: 1.786606788635254\n",
      "Epoch 9, Loss: 1.6414326429367065\n",
      "Epoch 9, Loss: 2.0229856967926025\n",
      "Epoch 9, Loss: 1.760289192199707\n",
      "Epoch 9, Loss: 1.904477834701538\n",
      "Epoch 9, Loss: 1.9082521200180054\n",
      "Epoch 9, Loss: 1.6349538564682007\n",
      "Epoch 9, Loss: 1.9568891525268555\n",
      "Epoch 9, Loss: 1.8143625259399414\n",
      "Epoch 9, Loss: 2.0236563682556152\n",
      "Epoch 9, Loss: 1.6757125854492188\n",
      "Epoch 9, Loss: 1.8140935897827148\n",
      "Epoch 9, Loss: 1.8197762966156006\n",
      "Epoch 9, Loss: 1.7146470546722412\n",
      "Epoch 9, Loss: 2.066734552383423\n",
      "Epoch 9, Loss: 2.018728733062744\n",
      "Epoch 9, Loss: 1.6737794876098633\n",
      "Epoch 9, Loss: 2.225471019744873\n",
      "Epoch 10, Loss: 1.9629192352294922\n",
      "Epoch 10, Loss: 1.8415757417678833\n",
      "Epoch 10, Loss: 1.930553674697876\n",
      "Epoch 10, Loss: 1.9291186332702637\n",
      "Epoch 10, Loss: 1.9940848350524902\n",
      "Epoch 10, Loss: 1.7266120910644531\n",
      "Epoch 10, Loss: 1.845244288444519\n",
      "Epoch 10, Loss: 1.9175758361816406\n",
      "Epoch 10, Loss: 1.9889297485351562\n",
      "Epoch 10, Loss: 1.8078668117523193\n",
      "Epoch 10, Loss: 1.7035276889801025\n",
      "Epoch 10, Loss: 1.947035789489746\n",
      "Epoch 10, Loss: 1.9560019969940186\n",
      "Epoch 10, Loss: 2.1754038333892822\n",
      "Epoch 10, Loss: 1.822047472000122\n",
      "Epoch 10, Loss: 2.0463008880615234\n",
      "Epoch 10, Loss: 1.4765517711639404\n",
      "Epoch 10, Loss: 1.7081854343414307\n",
      "Epoch 10, Loss: 2.1370182037353516\n",
      "Epoch 10, Loss: 1.8666996955871582\n",
      "Epoch 10, Loss: 1.9321656227111816\n",
      "Epoch 10, Loss: 1.8096075057983398\n",
      "Epoch 10, Loss: 2.0150034427642822\n",
      "Epoch 10, Loss: 1.6449322700500488\n",
      "Epoch 10, Loss: 1.679031491279602\n",
      "Epoch 10, Loss: 1.8395392894744873\n",
      "Epoch 10, Loss: 1.8757718801498413\n",
      "Epoch 10, Loss: 1.867540717124939\n",
      "Epoch 10, Loss: 1.7835896015167236\n",
      "Epoch 10, Loss: 1.8522794246673584\n",
      "Epoch 10, Loss: 1.8971401453018188\n",
      "Epoch 10, Loss: 1.950797438621521\n",
      "Epoch 10, Loss: 1.7866932153701782\n",
      "Epoch 10, Loss: 1.833306074142456\n",
      "Epoch 10, Loss: 1.79802668094635\n",
      "Epoch 10, Loss: 1.7865689992904663\n",
      "Epoch 10, Loss: 1.8702943325042725\n",
      "Epoch 10, Loss: 1.782210350036621\n",
      "Epoch 10, Loss: 2.0130090713500977\n",
      "Epoch 10, Loss: 1.879634141921997\n",
      "Epoch 10, Loss: 1.9881399869918823\n",
      "Epoch 10, Loss: 1.496943712234497\n",
      "Epoch 10, Loss: 1.842494010925293\n",
      "Epoch 10, Loss: 2.1511759757995605\n",
      "Epoch 10, Loss: 1.6942012310028076\n",
      "Epoch 10, Loss: 1.9200265407562256\n",
      "Epoch 10, Loss: 2.038668632507324\n",
      "Epoch 10, Loss: 1.933950662612915\n",
      "Epoch 10, Loss: 1.8096771240234375\n",
      "Epoch 10, Loss: 1.8982269763946533\n",
      "Epoch 10, Loss: 1.9153094291687012\n",
      "Epoch 10, Loss: 1.724579930305481\n",
      "Epoch 10, Loss: 1.9262585639953613\n",
      "Epoch 10, Loss: 1.690908670425415\n",
      "Epoch 10, Loss: 1.801356315612793\n",
      "Epoch 10, Loss: 1.9253276586532593\n",
      "Epoch 10, Loss: 1.550126314163208\n",
      "Epoch 10, Loss: 1.8538196086883545\n",
      "Epoch 10, Loss: 1.5154211521148682\n",
      "Epoch 10, Loss: 1.4756035804748535\n",
      "Epoch 10, Loss: 1.8035318851470947\n",
      "Epoch 10, Loss: 1.850376844406128\n",
      "Epoch 10, Loss: 1.6878345012664795\n",
      "Epoch 10, Loss: 2.1966729164123535\n",
      "Epoch 10, Loss: 1.695852518081665\n",
      "Epoch 10, Loss: 1.8329718112945557\n",
      "Epoch 10, Loss: 1.7457423210144043\n",
      "Epoch 10, Loss: 1.869145393371582\n",
      "Epoch 10, Loss: 1.7703866958618164\n",
      "Epoch 10, Loss: 1.905935525894165\n",
      "Epoch 10, Loss: 1.9020271301269531\n",
      "Epoch 10, Loss: 1.7501087188720703\n",
      "Epoch 10, Loss: 1.8831088542938232\n",
      "Epoch 10, Loss: 1.7882437705993652\n",
      "Epoch 10, Loss: 2.3089218139648438\n",
      "Epoch 11, Loss: 1.9377491474151611\n",
      "Epoch 11, Loss: 2.010895252227783\n",
      "Epoch 11, Loss: 1.8799426555633545\n",
      "Epoch 11, Loss: 2.2297580242156982\n",
      "Epoch 11, Loss: 1.7782862186431885\n",
      "Epoch 11, Loss: 1.7216174602508545\n",
      "Epoch 11, Loss: 1.7813971042633057\n",
      "Epoch 11, Loss: 1.667766809463501\n",
      "Epoch 11, Loss: 1.701308250427246\n",
      "Epoch 11, Loss: 1.7259151935577393\n",
      "Epoch 11, Loss: 1.6255385875701904\n",
      "Epoch 11, Loss: 1.9215919971466064\n",
      "Epoch 11, Loss: 1.736626386642456\n",
      "Epoch 11, Loss: 1.9177849292755127\n",
      "Epoch 11, Loss: 2.075436592102051\n",
      "Epoch 11, Loss: 1.7806612253189087\n",
      "Epoch 11, Loss: 1.8236793279647827\n",
      "Epoch 11, Loss: 1.9018605947494507\n",
      "Epoch 11, Loss: 2.0731663703918457\n",
      "Epoch 11, Loss: 1.8148905038833618\n",
      "Epoch 11, Loss: 1.9309606552124023\n",
      "Epoch 11, Loss: 1.672480583190918\n",
      "Epoch 11, Loss: 1.7920377254486084\n",
      "Epoch 11, Loss: 1.6832118034362793\n",
      "Epoch 11, Loss: 1.9499735832214355\n",
      "Epoch 11, Loss: 1.6099731922149658\n",
      "Epoch 11, Loss: 1.7459311485290527\n",
      "Epoch 11, Loss: 1.6948941946029663\n",
      "Epoch 11, Loss: 1.919564962387085\n",
      "Epoch 11, Loss: 1.9408769607543945\n",
      "Epoch 11, Loss: 1.9210060834884644\n",
      "Epoch 11, Loss: 1.9655897617340088\n",
      "Epoch 11, Loss: 1.6755144596099854\n",
      "Epoch 11, Loss: 1.8623807430267334\n",
      "Epoch 11, Loss: 1.9934961795806885\n",
      "Epoch 11, Loss: 2.0026297569274902\n",
      "Epoch 11, Loss: 1.9373373985290527\n",
      "Epoch 11, Loss: 1.7825181484222412\n",
      "Epoch 11, Loss: 1.7417123317718506\n",
      "Epoch 11, Loss: 1.7856533527374268\n",
      "Epoch 11, Loss: 1.7835700511932373\n",
      "Epoch 11, Loss: 2.101567268371582\n",
      "Epoch 11, Loss: 2.081106424331665\n",
      "Epoch 11, Loss: 1.7278029918670654\n",
      "Epoch 11, Loss: 1.8504234552383423\n",
      "Epoch 11, Loss: 1.5330729484558105\n",
      "Epoch 11, Loss: 1.8665995597839355\n",
      "Epoch 11, Loss: 1.8714113235473633\n",
      "Epoch 11, Loss: 1.866583228111267\n",
      "Epoch 11, Loss: 1.6924500465393066\n",
      "Epoch 11, Loss: 1.8211960792541504\n",
      "Epoch 11, Loss: 1.6997005939483643\n",
      "Epoch 11, Loss: 1.7211730480194092\n",
      "Epoch 11, Loss: 1.796947956085205\n",
      "Epoch 11, Loss: 1.8201954364776611\n",
      "Epoch 11, Loss: 1.7447047233581543\n",
      "Epoch 11, Loss: 1.8760871887207031\n",
      "Epoch 11, Loss: 1.7619473934173584\n",
      "Epoch 11, Loss: 1.5500993728637695\n",
      "Epoch 11, Loss: 2.103997230529785\n",
      "Epoch 11, Loss: 1.9989839792251587\n",
      "Epoch 11, Loss: 1.9508870840072632\n",
      "Epoch 11, Loss: 2.051023006439209\n",
      "Epoch 11, Loss: 2.186275005340576\n",
      "Epoch 11, Loss: 2.030634641647339\n",
      "Epoch 11, Loss: 1.7429050207138062\n",
      "Epoch 11, Loss: 1.6228125095367432\n",
      "Epoch 11, Loss: 2.123779058456421\n",
      "Epoch 11, Loss: 1.6553839445114136\n",
      "Epoch 11, Loss: 1.9440569877624512\n",
      "Epoch 11, Loss: 1.6443437337875366\n",
      "Epoch 11, Loss: 1.904848575592041\n",
      "Epoch 11, Loss: 2.0947299003601074\n",
      "Epoch 11, Loss: 1.5517735481262207\n",
      "Epoch 11, Loss: 2.0636117458343506\n",
      "Epoch 12, Loss: 2.1713128089904785\n",
      "Epoch 12, Loss: 1.7894103527069092\n",
      "Epoch 12, Loss: 1.6313506364822388\n",
      "Epoch 12, Loss: 1.9501301050186157\n",
      "Epoch 12, Loss: 1.7969386577606201\n",
      "Epoch 12, Loss: 1.9781548976898193\n",
      "Epoch 12, Loss: 1.5766396522521973\n",
      "Epoch 12, Loss: 1.8774346113204956\n",
      "Epoch 12, Loss: 1.79582679271698\n",
      "Epoch 12, Loss: 1.679713487625122\n",
      "Epoch 12, Loss: 2.0157628059387207\n",
      "Epoch 12, Loss: 1.7532954216003418\n",
      "Epoch 12, Loss: 2.1162607669830322\n",
      "Epoch 12, Loss: 1.955918550491333\n",
      "Epoch 12, Loss: 1.9082304239273071\n",
      "Epoch 12, Loss: 2.10036039352417\n",
      "Epoch 12, Loss: 1.7280879020690918\n",
      "Epoch 12, Loss: 1.980769395828247\n",
      "Epoch 12, Loss: 1.868640661239624\n",
      "Epoch 12, Loss: 1.7075042724609375\n",
      "Epoch 12, Loss: 1.9633491039276123\n",
      "Epoch 12, Loss: 2.0034918785095215\n",
      "Epoch 12, Loss: 1.7553291320800781\n",
      "Epoch 12, Loss: 1.8003978729248047\n",
      "Epoch 12, Loss: 1.9205410480499268\n",
      "Epoch 12, Loss: 1.612473964691162\n",
      "Epoch 12, Loss: 1.837148666381836\n",
      "Epoch 12, Loss: 1.8094271421432495\n",
      "Epoch 12, Loss: 1.7018076181411743\n",
      "Epoch 12, Loss: 1.5113985538482666\n",
      "Epoch 12, Loss: 1.6965253353118896\n",
      "Epoch 12, Loss: 1.5368273258209229\n",
      "Epoch 12, Loss: 1.7655560970306396\n",
      "Epoch 12, Loss: 1.774263858795166\n",
      "Epoch 12, Loss: 1.7237634658813477\n",
      "Epoch 12, Loss: 1.7653493881225586\n",
      "Epoch 12, Loss: 1.9225698709487915\n",
      "Epoch 12, Loss: 2.094430923461914\n",
      "Epoch 12, Loss: 1.6303715705871582\n",
      "Epoch 12, Loss: 1.9632941484451294\n",
      "Epoch 12, Loss: 1.729641318321228\n",
      "Epoch 12, Loss: 1.9651539325714111\n",
      "Epoch 12, Loss: 1.9719891548156738\n",
      "Epoch 12, Loss: 2.038480758666992\n",
      "Epoch 12, Loss: 1.9629676342010498\n",
      "Epoch 12, Loss: 1.981674075126648\n",
      "Epoch 12, Loss: 1.7832603454589844\n",
      "Epoch 12, Loss: 1.8630088567733765\n",
      "Epoch 12, Loss: 1.8709716796875\n",
      "Epoch 12, Loss: 1.7263920307159424\n",
      "Epoch 12, Loss: 2.2954142093658447\n",
      "Epoch 12, Loss: 1.8586671352386475\n",
      "Epoch 12, Loss: 1.9204885959625244\n",
      "Epoch 12, Loss: 1.7332587242126465\n",
      "Epoch 12, Loss: 1.7948052883148193\n",
      "Epoch 12, Loss: 1.5476267337799072\n",
      "Epoch 12, Loss: 1.804221510887146\n",
      "Epoch 12, Loss: 1.818251371383667\n",
      "Epoch 12, Loss: 1.915248990058899\n",
      "Epoch 12, Loss: 1.7464960813522339\n",
      "Epoch 12, Loss: 2.055506944656372\n",
      "Epoch 12, Loss: 1.6460438966751099\n",
      "Epoch 12, Loss: 1.978116750717163\n",
      "Epoch 12, Loss: 1.9551019668579102\n",
      "Epoch 12, Loss: 1.7890088558197021\n",
      "Epoch 12, Loss: 1.9398527145385742\n",
      "Epoch 12, Loss: 1.7663922309875488\n",
      "Epoch 12, Loss: 1.7732212543487549\n",
      "Epoch 12, Loss: 1.8659570217132568\n",
      "Epoch 12, Loss: 1.7222263813018799\n",
      "Epoch 12, Loss: 1.7797551155090332\n",
      "Epoch 12, Loss: 1.979943871498108\n",
      "Epoch 12, Loss: 1.9516366720199585\n",
      "Epoch 12, Loss: 1.7979074716567993\n",
      "Epoch 12, Loss: 1.9749720096588135\n",
      "Epoch 13, Loss: 1.7654696702957153\n",
      "Epoch 13, Loss: 1.9863789081573486\n",
      "Epoch 13, Loss: 1.9023798704147339\n",
      "Epoch 13, Loss: 1.6989822387695312\n",
      "Epoch 13, Loss: 1.7895188331604004\n",
      "Epoch 13, Loss: 1.8119032382965088\n",
      "Epoch 13, Loss: 2.124732732772827\n",
      "Epoch 13, Loss: 1.7130547761917114\n",
      "Epoch 13, Loss: 1.8038049936294556\n",
      "Epoch 13, Loss: 1.8832590579986572\n",
      "Epoch 13, Loss: 1.8053828477859497\n",
      "Epoch 13, Loss: 1.5435774326324463\n",
      "Epoch 13, Loss: 1.706190586090088\n",
      "Epoch 13, Loss: 1.6362457275390625\n",
      "Epoch 13, Loss: 1.8234572410583496\n",
      "Epoch 13, Loss: 1.8510515689849854\n",
      "Epoch 13, Loss: 1.6872062683105469\n",
      "Epoch 13, Loss: 1.789318561553955\n",
      "Epoch 13, Loss: 1.8097398281097412\n",
      "Epoch 13, Loss: 1.8869452476501465\n",
      "Epoch 13, Loss: 1.9890358448028564\n",
      "Epoch 13, Loss: 1.8524775505065918\n",
      "Epoch 13, Loss: 1.7810568809509277\n",
      "Epoch 13, Loss: 1.8399101495742798\n",
      "Epoch 13, Loss: 1.9355987310409546\n",
      "Epoch 13, Loss: 1.7148644924163818\n",
      "Epoch 13, Loss: 1.8582093715667725\n",
      "Epoch 13, Loss: 2.1022989749908447\n",
      "Epoch 13, Loss: 1.7783125638961792\n",
      "Epoch 13, Loss: 1.8312397003173828\n",
      "Epoch 13, Loss: 1.6397732496261597\n",
      "Epoch 13, Loss: 1.9608421325683594\n",
      "Epoch 13, Loss: 1.7610223293304443\n",
      "Epoch 13, Loss: 1.7347493171691895\n",
      "Epoch 13, Loss: 2.151801109313965\n",
      "Epoch 13, Loss: 1.9735171794891357\n",
      "Epoch 13, Loss: 1.78230619430542\n",
      "Epoch 13, Loss: 1.6799473762512207\n",
      "Epoch 13, Loss: 1.749032735824585\n",
      "Epoch 13, Loss: 1.5806182622909546\n",
      "Epoch 13, Loss: 1.7793312072753906\n",
      "Epoch 13, Loss: 1.789149284362793\n",
      "Epoch 13, Loss: 2.3355636596679688\n",
      "Epoch 13, Loss: 1.8556814193725586\n",
      "Epoch 13, Loss: 1.754974126815796\n",
      "Epoch 13, Loss: 1.8941932916641235\n",
      "Epoch 13, Loss: 1.815951943397522\n",
      "Epoch 13, Loss: 2.06643009185791\n",
      "Epoch 13, Loss: 2.004363536834717\n",
      "Epoch 13, Loss: 1.759962797164917\n",
      "Epoch 13, Loss: 1.9933040142059326\n",
      "Epoch 13, Loss: 1.7129212617874146\n",
      "Epoch 13, Loss: 1.8394267559051514\n",
      "Epoch 13, Loss: 1.6236766576766968\n",
      "Epoch 13, Loss: 1.7995946407318115\n",
      "Epoch 13, Loss: 1.6898231506347656\n",
      "Epoch 13, Loss: 2.0526022911071777\n",
      "Epoch 13, Loss: 1.6649394035339355\n",
      "Epoch 13, Loss: 2.037876605987549\n",
      "Epoch 13, Loss: 1.8090053796768188\n",
      "Epoch 13, Loss: 2.0253353118896484\n",
      "Epoch 13, Loss: 1.8719278573989868\n",
      "Epoch 13, Loss: 1.7593355178833008\n",
      "Epoch 13, Loss: 1.9662050008773804\n",
      "Epoch 13, Loss: 1.8825039863586426\n",
      "Epoch 13, Loss: 1.8162522315979004\n",
      "Epoch 13, Loss: 1.6011900901794434\n",
      "Epoch 13, Loss: 2.072510242462158\n",
      "Epoch 13, Loss: 1.6821303367614746\n",
      "Epoch 13, Loss: 1.890411138534546\n",
      "Epoch 13, Loss: 1.7615342140197754\n",
      "Epoch 13, Loss: 2.1248483657836914\n",
      "Epoch 13, Loss: 1.7922513484954834\n",
      "Epoch 13, Loss: 1.99570894241333\n",
      "Epoch 13, Loss: 2.007584571838379\n",
      "Epoch 14, Loss: 1.8718159198760986\n",
      "Epoch 14, Loss: 1.5428739786148071\n",
      "Epoch 14, Loss: 2.139531373977661\n",
      "Epoch 14, Loss: 1.6447211503982544\n",
      "Epoch 14, Loss: 1.7884197235107422\n",
      "Epoch 14, Loss: 2.0703678131103516\n",
      "Epoch 14, Loss: 1.756840467453003\n",
      "Epoch 14, Loss: 1.8320517539978027\n",
      "Epoch 14, Loss: 1.8104352951049805\n",
      "Epoch 14, Loss: 1.7840503454208374\n",
      "Epoch 14, Loss: 1.5705885887145996\n",
      "Epoch 14, Loss: 1.7870160341262817\n",
      "Epoch 14, Loss: 1.9529885053634644\n",
      "Epoch 14, Loss: 1.5163464546203613\n",
      "Epoch 14, Loss: 1.6855707168579102\n",
      "Epoch 14, Loss: 2.088776111602783\n",
      "Epoch 14, Loss: 1.9619171619415283\n",
      "Epoch 14, Loss: 1.658991813659668\n",
      "Epoch 14, Loss: 2.0515859127044678\n",
      "Epoch 14, Loss: 1.5219777822494507\n",
      "Epoch 14, Loss: 1.7505590915679932\n",
      "Epoch 14, Loss: 1.7314906120300293\n",
      "Epoch 14, Loss: 1.9684079885482788\n",
      "Epoch 14, Loss: 1.8200055360794067\n",
      "Epoch 14, Loss: 1.8788959980010986\n",
      "Epoch 14, Loss: 1.795616865158081\n",
      "Epoch 14, Loss: 1.8632203340530396\n",
      "Epoch 14, Loss: 1.5880110263824463\n",
      "Epoch 14, Loss: 1.8974850177764893\n",
      "Epoch 14, Loss: 1.7877686023712158\n",
      "Epoch 14, Loss: 1.783573865890503\n",
      "Epoch 14, Loss: 1.8645691871643066\n",
      "Epoch 14, Loss: 1.7414631843566895\n",
      "Epoch 14, Loss: 1.7708992958068848\n",
      "Epoch 14, Loss: 1.824110507965088\n",
      "Epoch 14, Loss: 1.8501079082489014\n",
      "Epoch 14, Loss: 1.6513051986694336\n",
      "Epoch 14, Loss: 1.6808300018310547\n",
      "Epoch 14, Loss: 1.9435759782791138\n",
      "Epoch 14, Loss: 1.9560731649398804\n",
      "Epoch 14, Loss: 2.174919605255127\n",
      "Epoch 14, Loss: 1.8495874404907227\n",
      "Epoch 14, Loss: 1.692905306816101\n",
      "Epoch 14, Loss: 1.7811694145202637\n",
      "Epoch 14, Loss: 1.7437530755996704\n",
      "Epoch 14, Loss: 1.9918856620788574\n",
      "Epoch 14, Loss: 2.02449369430542\n",
      "Epoch 14, Loss: 1.7309513092041016\n",
      "Epoch 14, Loss: 1.6292126178741455\n",
      "Epoch 14, Loss: 1.9192641973495483\n",
      "Epoch 14, Loss: 1.9431514739990234\n",
      "Epoch 14, Loss: 1.8679192066192627\n",
      "Epoch 14, Loss: 2.051811695098877\n",
      "Epoch 14, Loss: 1.8932541608810425\n",
      "Epoch 14, Loss: 1.8228853940963745\n",
      "Epoch 14, Loss: 1.9018771648406982\n",
      "Epoch 14, Loss: 1.9623842239379883\n",
      "Epoch 14, Loss: 1.880332112312317\n",
      "Epoch 14, Loss: 2.278536796569824\n",
      "Epoch 14, Loss: 1.6816151142120361\n",
      "Epoch 14, Loss: 1.7946807146072388\n",
      "Epoch 14, Loss: 1.8700954914093018\n",
      "Epoch 14, Loss: 1.9525814056396484\n",
      "Epoch 14, Loss: 1.869354248046875\n",
      "Epoch 14, Loss: 1.889620065689087\n",
      "Epoch 14, Loss: 1.843184232711792\n",
      "Epoch 14, Loss: 1.9343656301498413\n",
      "Epoch 14, Loss: 1.7108995914459229\n",
      "Epoch 14, Loss: 1.7511121034622192\n",
      "Epoch 14, Loss: 1.968155860900879\n",
      "Epoch 14, Loss: 1.8934061527252197\n",
      "Epoch 14, Loss: 2.009735107421875\n",
      "Epoch 14, Loss: 1.7878743410110474\n",
      "Epoch 14, Loss: 1.9340745210647583\n",
      "Epoch 14, Loss: 2.0203299522399902\n",
      "Epoch 15, Loss: 1.9295427799224854\n",
      "Epoch 15, Loss: 1.8166422843933105\n",
      "Epoch 15, Loss: 1.7819478511810303\n",
      "Epoch 15, Loss: 2.042156219482422\n",
      "Epoch 15, Loss: 2.1640851497650146\n",
      "Epoch 15, Loss: 1.8607449531555176\n",
      "Epoch 15, Loss: 1.859925389289856\n",
      "Epoch 15, Loss: 1.9317288398742676\n",
      "Epoch 15, Loss: 1.9320932626724243\n",
      "Epoch 15, Loss: 1.906726360321045\n",
      "Epoch 15, Loss: 1.8743088245391846\n",
      "Epoch 15, Loss: 1.6991446018218994\n",
      "Epoch 15, Loss: 1.9898213148117065\n",
      "Epoch 15, Loss: 1.7605818510055542\n",
      "Epoch 15, Loss: 1.92096745967865\n",
      "Epoch 15, Loss: 1.9807006120681763\n",
      "Epoch 15, Loss: 1.997694969177246\n",
      "Epoch 15, Loss: 1.9103420972824097\n",
      "Epoch 15, Loss: 1.901597499847412\n",
      "Epoch 15, Loss: 1.749666690826416\n",
      "Epoch 15, Loss: 1.9323523044586182\n",
      "Epoch 15, Loss: 1.8066182136535645\n",
      "Epoch 15, Loss: 1.74981689453125\n",
      "Epoch 15, Loss: 1.6411972045898438\n",
      "Epoch 15, Loss: 2.057516574859619\n",
      "Epoch 15, Loss: 1.905775547027588\n",
      "Epoch 15, Loss: 2.0079712867736816\n",
      "Epoch 15, Loss: 1.854982614517212\n",
      "Epoch 15, Loss: 1.5351072549819946\n",
      "Epoch 15, Loss: 1.8823800086975098\n",
      "Epoch 15, Loss: 2.0092062950134277\n",
      "Epoch 15, Loss: 1.8837709426879883\n",
      "Epoch 15, Loss: 1.6439166069030762\n",
      "Epoch 15, Loss: 1.8912510871887207\n",
      "Epoch 15, Loss: 1.9734011888504028\n",
      "Epoch 15, Loss: 1.8835153579711914\n",
      "Epoch 15, Loss: 1.8308459520339966\n",
      "Epoch 15, Loss: 1.7230767011642456\n",
      "Epoch 15, Loss: 1.566833257675171\n",
      "Epoch 15, Loss: 1.856021523475647\n",
      "Epoch 15, Loss: 1.6321260929107666\n",
      "Epoch 15, Loss: 1.6193418502807617\n",
      "Epoch 15, Loss: 2.0079565048217773\n",
      "Epoch 15, Loss: 2.0504541397094727\n",
      "Epoch 15, Loss: 1.9003784656524658\n",
      "Epoch 15, Loss: 1.9415974617004395\n",
      "Epoch 15, Loss: 1.7352641820907593\n",
      "Epoch 15, Loss: 1.7621760368347168\n",
      "Epoch 15, Loss: 1.9235659837722778\n",
      "Epoch 15, Loss: 1.877841830253601\n",
      "Epoch 15, Loss: 2.212634563446045\n",
      "Epoch 15, Loss: 1.7959764003753662\n",
      "Epoch 15, Loss: 1.7312835454940796\n",
      "Epoch 15, Loss: 1.6697092056274414\n",
      "Epoch 15, Loss: 1.728649616241455\n",
      "Epoch 15, Loss: 1.5922887325286865\n",
      "Epoch 15, Loss: 2.0319461822509766\n",
      "Epoch 15, Loss: 1.7822320461273193\n",
      "Epoch 15, Loss: 1.8201992511749268\n",
      "Epoch 15, Loss: 1.665544867515564\n",
      "Epoch 15, Loss: 1.7560274600982666\n",
      "Epoch 15, Loss: 1.8074281215667725\n",
      "Epoch 15, Loss: 1.9950064420700073\n",
      "Epoch 15, Loss: 1.8637254238128662\n",
      "Epoch 15, Loss: 1.726853609085083\n",
      "Epoch 15, Loss: 1.7966070175170898\n",
      "Epoch 15, Loss: 1.7948455810546875\n",
      "Epoch 15, Loss: 1.6988673210144043\n",
      "Epoch 15, Loss: 1.6856387853622437\n",
      "Epoch 15, Loss: 1.7443325519561768\n",
      "Epoch 15, Loss: 1.6482257843017578\n",
      "Epoch 15, Loss: 1.8323500156402588\n",
      "Epoch 15, Loss: 1.7795995473861694\n",
      "Epoch 15, Loss: 1.6321868896484375\n",
      "Epoch 15, Loss: 1.9574567079544067\n",
      "Epoch 16, Loss: 2.068999767303467\n",
      "Epoch 16, Loss: 1.929826021194458\n",
      "Epoch 16, Loss: 1.738339900970459\n",
      "Epoch 16, Loss: 1.8771255016326904\n",
      "Epoch 16, Loss: 1.9119138717651367\n",
      "Epoch 16, Loss: 1.9141952991485596\n",
      "Epoch 16, Loss: 1.7719306945800781\n",
      "Epoch 16, Loss: 1.7510876655578613\n",
      "Epoch 16, Loss: 1.7067575454711914\n",
      "Epoch 16, Loss: 1.7317471504211426\n",
      "Epoch 16, Loss: 1.6087462902069092\n",
      "Epoch 16, Loss: 1.6504240036010742\n",
      "Epoch 16, Loss: 1.7987070083618164\n",
      "Epoch 16, Loss: 1.7025644779205322\n",
      "Epoch 16, Loss: 1.8089864253997803\n",
      "Epoch 16, Loss: 1.651078462600708\n",
      "Epoch 16, Loss: 1.7805743217468262\n",
      "Epoch 16, Loss: 1.7059683799743652\n",
      "Epoch 16, Loss: 1.5735783576965332\n",
      "Epoch 16, Loss: 2.009798526763916\n",
      "Epoch 16, Loss: 1.934056043624878\n",
      "Epoch 16, Loss: 1.858574628829956\n",
      "Epoch 16, Loss: 2.080221176147461\n",
      "Epoch 16, Loss: 1.662861943244934\n",
      "Epoch 16, Loss: 1.7328025102615356\n",
      "Epoch 16, Loss: 1.8253120183944702\n",
      "Epoch 16, Loss: 2.0157201290130615\n",
      "Epoch 16, Loss: 1.8805897235870361\n",
      "Epoch 16, Loss: 2.17631459236145\n",
      "Epoch 16, Loss: 1.6743721961975098\n",
      "Epoch 16, Loss: 2.0107502937316895\n",
      "Epoch 16, Loss: 1.5352001190185547\n",
      "Epoch 16, Loss: 1.7125842571258545\n",
      "Epoch 16, Loss: 2.1080527305603027\n",
      "Epoch 16, Loss: 1.8009917736053467\n",
      "Epoch 16, Loss: 1.7579728364944458\n",
      "Epoch 16, Loss: 1.782212495803833\n",
      "Epoch 16, Loss: 1.5804775953292847\n",
      "Epoch 16, Loss: 1.9143621921539307\n",
      "Epoch 16, Loss: 1.8970801830291748\n",
      "Epoch 16, Loss: 1.8638532161712646\n",
      "Epoch 16, Loss: 1.880013108253479\n",
      "Epoch 16, Loss: 1.7708172798156738\n",
      "Epoch 16, Loss: 1.9950846433639526\n",
      "Epoch 16, Loss: 1.7413966655731201\n",
      "Epoch 16, Loss: 1.7202014923095703\n",
      "Epoch 16, Loss: 1.9809234142303467\n",
      "Epoch 16, Loss: 1.84135901927948\n",
      "Epoch 16, Loss: 1.8389979600906372\n",
      "Epoch 16, Loss: 2.0933032035827637\n",
      "Epoch 16, Loss: 1.698164463043213\n",
      "Epoch 16, Loss: 2.0598530769348145\n",
      "Epoch 16, Loss: 1.9147005081176758\n",
      "Epoch 16, Loss: 2.0191798210144043\n",
      "Epoch 16, Loss: 1.7543541193008423\n",
      "Epoch 16, Loss: 1.6950799226760864\n",
      "Epoch 16, Loss: 2.029402732849121\n",
      "Epoch 16, Loss: 2.0896761417388916\n",
      "Epoch 16, Loss: 1.6941418647766113\n",
      "Epoch 16, Loss: 1.715854287147522\n",
      "Epoch 16, Loss: 1.8094289302825928\n",
      "Epoch 16, Loss: 1.7868695259094238\n",
      "Epoch 16, Loss: 1.7381231784820557\n",
      "Epoch 16, Loss: 1.6593832969665527\n",
      "Epoch 16, Loss: 1.982069969177246\n",
      "Epoch 16, Loss: 1.9165323972702026\n",
      "Epoch 16, Loss: 2.1581740379333496\n",
      "Epoch 16, Loss: 1.936625361442566\n",
      "Epoch 16, Loss: 2.0744469165802\n",
      "Epoch 16, Loss: 1.7522821426391602\n",
      "Epoch 16, Loss: 1.690978765487671\n",
      "Epoch 16, Loss: 1.8119677305221558\n",
      "Epoch 16, Loss: 1.686753273010254\n",
      "Epoch 16, Loss: 1.7076294422149658\n",
      "Epoch 16, Loss: 1.836514949798584\n",
      "Epoch 17, Loss: 1.9938573837280273\n",
      "Epoch 17, Loss: 1.7669975757598877\n",
      "Epoch 17, Loss: 1.7646684646606445\n",
      "Epoch 17, Loss: 2.1503264904022217\n",
      "Epoch 17, Loss: 1.9991244077682495\n",
      "Epoch 17, Loss: 1.5779969692230225\n",
      "Epoch 17, Loss: 1.7174395322799683\n",
      "Epoch 17, Loss: 1.7207956314086914\n",
      "Epoch 17, Loss: 1.954491376876831\n",
      "Epoch 17, Loss: 1.684624195098877\n",
      "Epoch 17, Loss: 1.8232121467590332\n",
      "Epoch 17, Loss: 1.7932450771331787\n",
      "Epoch 17, Loss: 1.9349265098571777\n",
      "Epoch 17, Loss: 1.7835643291473389\n",
      "Epoch 17, Loss: 1.860737919807434\n",
      "Epoch 17, Loss: 1.8243656158447266\n",
      "Epoch 17, Loss: 1.8786742687225342\n",
      "Epoch 17, Loss: 1.834202527999878\n",
      "Epoch 17, Loss: 1.7994858026504517\n",
      "Epoch 17, Loss: 1.5126008987426758\n",
      "Epoch 17, Loss: 1.663341760635376\n",
      "Epoch 17, Loss: 1.4932773113250732\n",
      "Epoch 17, Loss: 1.732130527496338\n",
      "Epoch 17, Loss: 2.14703631401062\n",
      "Epoch 17, Loss: 2.0970077514648438\n",
      "Epoch 17, Loss: 1.6017827987670898\n",
      "Epoch 17, Loss: 1.6960077285766602\n",
      "Epoch 17, Loss: 2.067171096801758\n",
      "Epoch 17, Loss: 1.65114426612854\n",
      "Epoch 17, Loss: 1.9541665315628052\n",
      "Epoch 17, Loss: 1.6275826692581177\n",
      "Epoch 17, Loss: 1.8549222946166992\n",
      "Epoch 17, Loss: 1.8430123329162598\n",
      "Epoch 17, Loss: 1.9963561296463013\n",
      "Epoch 17, Loss: 1.6543351411819458\n",
      "Epoch 17, Loss: 1.9028165340423584\n",
      "Epoch 17, Loss: 1.8733867406845093\n",
      "Epoch 17, Loss: 1.953256368637085\n",
      "Epoch 17, Loss: 2.129310369491577\n",
      "Epoch 17, Loss: 1.7513108253479004\n",
      "Epoch 17, Loss: 1.770547866821289\n",
      "Epoch 17, Loss: 1.7593097686767578\n",
      "Epoch 17, Loss: 2.0403666496276855\n",
      "Epoch 17, Loss: 1.6945818662643433\n",
      "Epoch 17, Loss: 1.8977837562561035\n",
      "Epoch 17, Loss: 1.930467963218689\n",
      "Epoch 17, Loss: 2.032939910888672\n",
      "Epoch 17, Loss: 1.6837034225463867\n",
      "Epoch 17, Loss: 1.8079907894134521\n",
      "Epoch 17, Loss: 1.9550869464874268\n",
      "Epoch 17, Loss: 1.7147136926651\n",
      "Epoch 17, Loss: 1.7135694026947021\n",
      "Epoch 17, Loss: 1.7939726114273071\n",
      "Epoch 17, Loss: 1.971191644668579\n",
      "Epoch 17, Loss: 1.7652547359466553\n",
      "Epoch 17, Loss: 1.6765742301940918\n",
      "Epoch 17, Loss: 1.7637807130813599\n",
      "Epoch 17, Loss: 2.0521020889282227\n",
      "Epoch 17, Loss: 1.914954423904419\n",
      "Epoch 17, Loss: 1.9040241241455078\n",
      "Epoch 17, Loss: 1.9430972337722778\n",
      "Epoch 17, Loss: 1.8089200258255005\n",
      "Epoch 17, Loss: 1.9691141843795776\n",
      "Epoch 17, Loss: 1.9996278285980225\n",
      "Epoch 17, Loss: 1.9251059293746948\n",
      "Epoch 17, Loss: 1.7136974334716797\n",
      "Epoch 17, Loss: 1.8632073402404785\n",
      "Epoch 17, Loss: 1.6929491758346558\n",
      "Epoch 17, Loss: 1.8166145086288452\n",
      "Epoch 17, Loss: 1.8427990674972534\n",
      "Epoch 17, Loss: 1.6982742547988892\n",
      "Epoch 17, Loss: 1.6530061960220337\n",
      "Epoch 17, Loss: 1.896479606628418\n",
      "Epoch 17, Loss: 1.7401528358459473\n",
      "Epoch 17, Loss: 1.9809482097625732\n",
      "Epoch 18, Loss: 1.9204654693603516\n",
      "Epoch 18, Loss: 1.8456169366836548\n",
      "Epoch 18, Loss: 1.7420927286148071\n",
      "Epoch 18, Loss: 1.8141943216323853\n",
      "Epoch 18, Loss: 1.7877973318099976\n",
      "Epoch 18, Loss: 1.690155267715454\n",
      "Epoch 18, Loss: 1.9858648777008057\n",
      "Epoch 18, Loss: 1.4810519218444824\n",
      "Epoch 18, Loss: 1.781299352645874\n",
      "Epoch 18, Loss: 1.744745135307312\n",
      "Epoch 18, Loss: 1.812891960144043\n",
      "Epoch 18, Loss: 1.6872552633285522\n",
      "Epoch 18, Loss: 1.6319636106491089\n",
      "Epoch 18, Loss: 1.8823857307434082\n",
      "Epoch 18, Loss: 1.797630786895752\n",
      "Epoch 18, Loss: 1.711787223815918\n",
      "Epoch 18, Loss: 1.6095430850982666\n",
      "Epoch 18, Loss: 1.5072729587554932\n",
      "Epoch 18, Loss: 1.8489490747451782\n",
      "Epoch 18, Loss: 1.7723463773727417\n",
      "Epoch 18, Loss: 1.6783688068389893\n",
      "Epoch 18, Loss: 2.0232815742492676\n",
      "Epoch 18, Loss: 1.9917957782745361\n",
      "Epoch 18, Loss: 1.6134917736053467\n",
      "Epoch 18, Loss: 2.010411262512207\n",
      "Epoch 18, Loss: 1.8901357650756836\n",
      "Epoch 18, Loss: 1.8800135850906372\n",
      "Epoch 18, Loss: 1.7264928817749023\n",
      "Epoch 18, Loss: 1.7180848121643066\n",
      "Epoch 18, Loss: 1.7001140117645264\n",
      "Epoch 18, Loss: 1.80635666847229\n",
      "Epoch 18, Loss: 2.2091002464294434\n",
      "Epoch 18, Loss: 2.036503791809082\n",
      "Epoch 18, Loss: 1.8661634922027588\n",
      "Epoch 18, Loss: 1.7460309267044067\n",
      "Epoch 18, Loss: 1.7821911573410034\n",
      "Epoch 18, Loss: 1.72200608253479\n",
      "Epoch 18, Loss: 1.7046847343444824\n",
      "Epoch 18, Loss: 1.967250943183899\n",
      "Epoch 18, Loss: 1.708918571472168\n",
      "Epoch 18, Loss: 1.868480920791626\n",
      "Epoch 18, Loss: 2.1691696643829346\n",
      "Epoch 18, Loss: 1.981007695198059\n",
      "Epoch 18, Loss: 1.8120481967926025\n",
      "Epoch 18, Loss: 1.8489161729812622\n",
      "Epoch 18, Loss: 1.8504905700683594\n",
      "Epoch 18, Loss: 1.9085462093353271\n",
      "Epoch 18, Loss: 1.8694489002227783\n",
      "Epoch 18, Loss: 1.7881474494934082\n",
      "Epoch 18, Loss: 1.6824707984924316\n",
      "Epoch 18, Loss: 1.7748113870620728\n",
      "Epoch 18, Loss: 1.8872663974761963\n",
      "Epoch 18, Loss: 1.7737247943878174\n",
      "Epoch 18, Loss: 1.9052205085754395\n",
      "Epoch 18, Loss: 1.715191125869751\n",
      "Epoch 18, Loss: 1.6734883785247803\n",
      "Epoch 18, Loss: 1.763946771621704\n",
      "Epoch 18, Loss: 1.7270711660385132\n",
      "Epoch 18, Loss: 2.0242526531219482\n",
      "Epoch 18, Loss: 1.9138286113739014\n",
      "Epoch 18, Loss: 1.6913294792175293\n",
      "Epoch 18, Loss: 1.9230140447616577\n",
      "Epoch 18, Loss: 1.9061572551727295\n",
      "Epoch 18, Loss: 1.8077465295791626\n",
      "Epoch 18, Loss: 1.946300983428955\n",
      "Epoch 18, Loss: 1.8871207237243652\n",
      "Epoch 18, Loss: 1.7296736240386963\n",
      "Epoch 18, Loss: 1.872040033340454\n",
      "Epoch 18, Loss: 1.982566475868225\n",
      "Epoch 18, Loss: 1.996246337890625\n",
      "Epoch 18, Loss: 1.7865324020385742\n",
      "Epoch 18, Loss: 1.7206861972808838\n",
      "Epoch 18, Loss: 1.8698045015335083\n",
      "Epoch 18, Loss: 2.2645139694213867\n",
      "Epoch 18, Loss: 2.195615291595459\n",
      "Epoch 19, Loss: 1.8683006763458252\n",
      "Epoch 19, Loss: 1.7222017049789429\n",
      "Epoch 19, Loss: 2.0092639923095703\n",
      "Epoch 19, Loss: 1.7157597541809082\n",
      "Epoch 19, Loss: 1.9843404293060303\n",
      "Epoch 19, Loss: 1.8519957065582275\n",
      "Epoch 19, Loss: 1.9417301416397095\n",
      "Epoch 19, Loss: 1.7621123790740967\n",
      "Epoch 19, Loss: 1.7563996315002441\n",
      "Epoch 19, Loss: 1.7545620203018188\n",
      "Epoch 19, Loss: 1.8323262929916382\n",
      "Epoch 19, Loss: 1.8249151706695557\n",
      "Epoch 19, Loss: 1.6429616212844849\n",
      "Epoch 19, Loss: 1.7153143882751465\n",
      "Epoch 19, Loss: 2.011453866958618\n",
      "Epoch 19, Loss: 1.7941877841949463\n",
      "Epoch 19, Loss: 1.853135108947754\n",
      "Epoch 19, Loss: 1.9624593257904053\n",
      "Epoch 19, Loss: 1.6954264640808105\n",
      "Epoch 19, Loss: 1.717714548110962\n",
      "Epoch 19, Loss: 1.8559272289276123\n",
      "Epoch 19, Loss: 1.6921252012252808\n",
      "Epoch 19, Loss: 1.6662657260894775\n",
      "Epoch 19, Loss: 1.8415120840072632\n",
      "Epoch 19, Loss: 1.930326223373413\n",
      "Epoch 19, Loss: 1.8639212846755981\n",
      "Epoch 19, Loss: 1.973313331604004\n",
      "Epoch 19, Loss: 1.6907093524932861\n",
      "Epoch 19, Loss: 1.7251579761505127\n",
      "Epoch 19, Loss: 1.8337363004684448\n",
      "Epoch 19, Loss: 1.965663194656372\n",
      "Epoch 19, Loss: 1.8315439224243164\n",
      "Epoch 19, Loss: 1.6943868398666382\n",
      "Epoch 19, Loss: 1.681151032447815\n",
      "Epoch 19, Loss: 1.7622730731964111\n",
      "Epoch 19, Loss: 1.8478052616119385\n",
      "Epoch 19, Loss: 1.674638032913208\n",
      "Epoch 19, Loss: 1.9590504169464111\n",
      "Epoch 19, Loss: 1.72606360912323\n",
      "Epoch 19, Loss: 1.9614183902740479\n",
      "Epoch 19, Loss: 1.881296157836914\n",
      "Epoch 19, Loss: 1.5313372611999512\n",
      "Epoch 19, Loss: 1.818332314491272\n",
      "Epoch 19, Loss: 1.7003631591796875\n",
      "Epoch 19, Loss: 1.9932360649108887\n",
      "Epoch 19, Loss: 1.778167486190796\n",
      "Epoch 19, Loss: 1.6387441158294678\n",
      "Epoch 19, Loss: 1.8644418716430664\n",
      "Epoch 19, Loss: 1.7207860946655273\n",
      "Epoch 19, Loss: 2.0754404067993164\n",
      "Epoch 19, Loss: 1.8587446212768555\n",
      "Epoch 19, Loss: 1.7348830699920654\n",
      "Epoch 19, Loss: 1.7474498748779297\n",
      "Epoch 19, Loss: 1.773680567741394\n",
      "Epoch 19, Loss: 1.9631967544555664\n",
      "Epoch 19, Loss: 1.8547906875610352\n",
      "Epoch 19, Loss: 1.7713983058929443\n",
      "Epoch 19, Loss: 2.011962890625\n",
      "Epoch 19, Loss: 1.86147141456604\n",
      "Epoch 19, Loss: 1.9485509395599365\n",
      "Epoch 19, Loss: 1.847538709640503\n",
      "Epoch 19, Loss: 1.9817450046539307\n",
      "Epoch 19, Loss: 2.054576873779297\n",
      "Epoch 19, Loss: 1.933065414428711\n",
      "Epoch 19, Loss: 1.9794631004333496\n",
      "Epoch 19, Loss: 1.7794793844223022\n",
      "Epoch 19, Loss: 1.6906375885009766\n",
      "Epoch 19, Loss: 1.6103085279464722\n",
      "Epoch 19, Loss: 1.8856298923492432\n",
      "Epoch 19, Loss: 2.0341668128967285\n",
      "Epoch 19, Loss: 1.8158609867095947\n",
      "Epoch 19, Loss: 1.9286518096923828\n",
      "Epoch 19, Loss: 1.7113981246948242\n",
      "Epoch 19, Loss: 1.6956804990768433\n",
      "Epoch 19, Loss: 2.001805543899536\n",
      "Epoch 20, Loss: 1.7694785594940186\n",
      "Epoch 20, Loss: 1.655023217201233\n",
      "Epoch 20, Loss: 2.019408941268921\n",
      "Epoch 20, Loss: 1.646350383758545\n",
      "Epoch 20, Loss: 1.711985468864441\n",
      "Epoch 20, Loss: 1.57261323928833\n",
      "Epoch 20, Loss: 1.8888309001922607\n",
      "Epoch 20, Loss: 1.9172934293746948\n",
      "Epoch 20, Loss: 2.0060343742370605\n",
      "Epoch 20, Loss: 1.5986111164093018\n",
      "Epoch 20, Loss: 1.7920348644256592\n",
      "Epoch 20, Loss: 2.0363762378692627\n",
      "Epoch 20, Loss: 1.9996020793914795\n",
      "Epoch 20, Loss: 1.7926936149597168\n",
      "Epoch 20, Loss: 1.92753267288208\n",
      "Epoch 20, Loss: 1.743477463722229\n",
      "Epoch 20, Loss: 1.6599934101104736\n",
      "Epoch 20, Loss: 2.1211724281311035\n",
      "Epoch 20, Loss: 1.8912818431854248\n",
      "Epoch 20, Loss: 1.7202355861663818\n",
      "Epoch 20, Loss: 1.7496427297592163\n",
      "Epoch 20, Loss: 1.7106492519378662\n",
      "Epoch 20, Loss: 1.761338472366333\n",
      "Epoch 20, Loss: 1.9979969263076782\n",
      "Epoch 20, Loss: 1.9386000633239746\n",
      "Epoch 20, Loss: 1.7205677032470703\n",
      "Epoch 20, Loss: 2.036942720413208\n",
      "Epoch 20, Loss: 1.8331868648529053\n",
      "Epoch 20, Loss: 2.0383243560791016\n",
      "Epoch 20, Loss: 1.7112482786178589\n",
      "Epoch 20, Loss: 2.0157999992370605\n",
      "Epoch 20, Loss: 1.8990013599395752\n",
      "Epoch 20, Loss: 1.8410871028900146\n",
      "Epoch 20, Loss: 1.78192138671875\n",
      "Epoch 20, Loss: 2.070986270904541\n",
      "Epoch 20, Loss: 1.8690770864486694\n",
      "Epoch 20, Loss: 1.8768619298934937\n",
      "Epoch 20, Loss: 1.93918776512146\n",
      "Epoch 20, Loss: 1.8139472007751465\n",
      "Epoch 20, Loss: 1.777625322341919\n",
      "Epoch 20, Loss: 2.1189117431640625\n",
      "Epoch 20, Loss: 1.9491010904312134\n",
      "Epoch 20, Loss: 1.910980224609375\n",
      "Epoch 20, Loss: 1.8762285709381104\n",
      "Epoch 20, Loss: 2.0537757873535156\n",
      "Epoch 20, Loss: 1.866700291633606\n",
      "Epoch 20, Loss: 1.8854002952575684\n",
      "Epoch 20, Loss: 1.8797041177749634\n",
      "Epoch 20, Loss: 1.7285990715026855\n",
      "Epoch 20, Loss: 1.5248228311538696\n",
      "Epoch 20, Loss: 1.6857962608337402\n",
      "Epoch 20, Loss: 1.7499186992645264\n",
      "Epoch 20, Loss: 1.7877745628356934\n",
      "Epoch 20, Loss: 1.6627659797668457\n",
      "Epoch 20, Loss: 1.7621023654937744\n",
      "Epoch 20, Loss: 1.7778278589248657\n",
      "Epoch 20, Loss: 1.602459192276001\n",
      "Epoch 20, Loss: 1.8253962993621826\n",
      "Epoch 20, Loss: 1.648210048675537\n",
      "Epoch 20, Loss: 1.7676066160202026\n",
      "Epoch 20, Loss: 1.768808126449585\n",
      "Epoch 20, Loss: 1.7555267810821533\n",
      "Epoch 20, Loss: 1.810715675354004\n",
      "Epoch 20, Loss: 1.826758861541748\n",
      "Epoch 20, Loss: 1.8984785079956055\n",
      "Epoch 20, Loss: 1.685511827468872\n",
      "Epoch 20, Loss: 1.7407970428466797\n",
      "Epoch 20, Loss: 1.632223129272461\n",
      "Epoch 20, Loss: 1.861058235168457\n",
      "Epoch 20, Loss: 2.010946750640869\n",
      "Epoch 20, Loss: 1.9022936820983887\n",
      "Epoch 20, Loss: 1.9681180715560913\n",
      "Epoch 20, Loss: 1.7292639017105103\n",
      "Epoch 20, Loss: 1.5047614574432373\n",
      "Epoch 20, Loss: 1.8521156311035156\n",
      "Epoch 21, Loss: 1.8135859966278076\n",
      "Epoch 21, Loss: 1.6896708011627197\n",
      "Epoch 21, Loss: 1.8666990995407104\n",
      "Epoch 21, Loss: 2.0508766174316406\n",
      "Epoch 21, Loss: 1.7872593402862549\n",
      "Epoch 21, Loss: 1.4892833232879639\n",
      "Epoch 21, Loss: 1.394902229309082\n",
      "Epoch 21, Loss: 2.0048165321350098\n",
      "Epoch 21, Loss: 1.5998129844665527\n",
      "Epoch 21, Loss: 1.6328212022781372\n",
      "Epoch 21, Loss: 1.5578348636627197\n",
      "Epoch 21, Loss: 1.6846919059753418\n",
      "Epoch 21, Loss: 1.8274743556976318\n",
      "Epoch 21, Loss: 1.576347827911377\n",
      "Epoch 21, Loss: 1.5268609523773193\n",
      "Epoch 21, Loss: 1.8894596099853516\n",
      "Epoch 21, Loss: 1.6518867015838623\n",
      "Epoch 21, Loss: 1.9180669784545898\n",
      "Epoch 21, Loss: 1.7941076755523682\n",
      "Epoch 21, Loss: 1.8513561487197876\n",
      "Epoch 21, Loss: 1.853682279586792\n",
      "Epoch 21, Loss: 2.1735455989837646\n",
      "Epoch 21, Loss: 2.0216426849365234\n",
      "Epoch 21, Loss: 1.8360044956207275\n",
      "Epoch 21, Loss: 1.9089879989624023\n",
      "Epoch 21, Loss: 1.8247802257537842\n",
      "Epoch 21, Loss: 1.839177131652832\n",
      "Epoch 21, Loss: 1.9602952003479004\n",
      "Epoch 21, Loss: 1.7508853673934937\n",
      "Epoch 21, Loss: 1.7023096084594727\n",
      "Epoch 21, Loss: 1.7633439302444458\n",
      "Epoch 21, Loss: 1.7677541971206665\n",
      "Epoch 21, Loss: 1.8379627466201782\n",
      "Epoch 21, Loss: 1.9520212411880493\n",
      "Epoch 21, Loss: 1.7911105155944824\n",
      "Epoch 21, Loss: 1.8508867025375366\n",
      "Epoch 21, Loss: 1.9266471862792969\n",
      "Epoch 21, Loss: 1.5842585563659668\n",
      "Epoch 21, Loss: 1.885223388671875\n",
      "Epoch 21, Loss: 1.8534538745880127\n",
      "Epoch 21, Loss: 1.8833458423614502\n",
      "Epoch 21, Loss: 1.8032387495040894\n",
      "Epoch 21, Loss: 1.8892550468444824\n",
      "Epoch 21, Loss: 1.7192411422729492\n",
      "Epoch 21, Loss: 2.026703357696533\n",
      "Epoch 21, Loss: 1.9265270233154297\n",
      "Epoch 21, Loss: 1.7708394527435303\n",
      "Epoch 21, Loss: 1.8939690589904785\n",
      "Epoch 21, Loss: 1.7983964681625366\n",
      "Epoch 21, Loss: 1.5838745832443237\n",
      "Epoch 21, Loss: 1.8484621047973633\n",
      "Epoch 21, Loss: 2.026979923248291\n",
      "Epoch 21, Loss: 1.703039288520813\n",
      "Epoch 21, Loss: 1.6983987092971802\n",
      "Epoch 21, Loss: 1.8378536701202393\n",
      "Epoch 21, Loss: 2.0770273208618164\n",
      "Epoch 21, Loss: 1.7013661861419678\n",
      "Epoch 21, Loss: 2.0547268390655518\n",
      "Epoch 21, Loss: 1.8257325887680054\n",
      "Epoch 21, Loss: 1.8452095985412598\n",
      "Epoch 21, Loss: 1.8416112661361694\n",
      "Epoch 21, Loss: 1.8293055295944214\n",
      "Epoch 21, Loss: 1.8229968547821045\n",
      "Epoch 21, Loss: 2.0851898193359375\n",
      "Epoch 21, Loss: 1.7173984050750732\n",
      "Epoch 21, Loss: 1.8338842391967773\n",
      "Epoch 21, Loss: 1.7951844930648804\n",
      "Epoch 21, Loss: 1.7595080137252808\n",
      "Epoch 21, Loss: 1.9729132652282715\n",
      "Epoch 21, Loss: 1.7550303936004639\n",
      "Epoch 21, Loss: 2.0184519290924072\n",
      "Epoch 21, Loss: 1.986128807067871\n",
      "Epoch 21, Loss: 1.6828935146331787\n",
      "Epoch 21, Loss: 1.9007925987243652\n",
      "Epoch 21, Loss: 2.069394826889038\n",
      "Epoch 22, Loss: 1.870553970336914\n",
      "Epoch 22, Loss: 1.8452982902526855\n",
      "Epoch 22, Loss: 1.9858555793762207\n",
      "Epoch 22, Loss: 1.8190685510635376\n",
      "Epoch 22, Loss: 1.6126707792282104\n",
      "Epoch 22, Loss: 1.96498441696167\n",
      "Epoch 22, Loss: 1.7686796188354492\n",
      "Epoch 22, Loss: 1.7740740776062012\n",
      "Epoch 22, Loss: 1.7445141077041626\n",
      "Epoch 22, Loss: 2.013918399810791\n",
      "Epoch 22, Loss: 1.8788470029830933\n",
      "Epoch 22, Loss: 2.028463840484619\n",
      "Epoch 22, Loss: 1.5915229320526123\n",
      "Epoch 22, Loss: 1.9605690240859985\n",
      "Epoch 22, Loss: 1.9055933952331543\n",
      "Epoch 22, Loss: 1.790869116783142\n",
      "Epoch 22, Loss: 1.685316801071167\n",
      "Epoch 22, Loss: 1.7588419914245605\n",
      "Epoch 22, Loss: 1.8993457555770874\n",
      "Epoch 22, Loss: 1.4853458404541016\n",
      "Epoch 22, Loss: 1.7575275897979736\n",
      "Epoch 22, Loss: 2.110175609588623\n",
      "Epoch 22, Loss: 1.8597092628479004\n",
      "Epoch 22, Loss: 1.8011550903320312\n",
      "Epoch 22, Loss: 1.6281977891921997\n",
      "Epoch 22, Loss: 1.8018817901611328\n",
      "Epoch 22, Loss: 1.7457449436187744\n",
      "Epoch 22, Loss: 1.9116723537445068\n",
      "Epoch 22, Loss: 1.9358859062194824\n",
      "Epoch 22, Loss: 2.0745625495910645\n",
      "Epoch 22, Loss: 1.8016912937164307\n",
      "Epoch 22, Loss: 1.8296844959259033\n",
      "Epoch 22, Loss: 2.187885046005249\n",
      "Epoch 22, Loss: 1.5950018167495728\n",
      "Epoch 22, Loss: 1.9030821323394775\n",
      "Epoch 22, Loss: 1.7761383056640625\n",
      "Epoch 22, Loss: 1.7499072551727295\n",
      "Epoch 22, Loss: 2.1786649227142334\n",
      "Epoch 22, Loss: 1.6509478092193604\n",
      "Epoch 22, Loss: 1.6025584936141968\n",
      "Epoch 22, Loss: 1.767555594444275\n",
      "Epoch 22, Loss: 2.1312918663024902\n",
      "Epoch 22, Loss: 1.6619948148727417\n",
      "Epoch 22, Loss: 1.9474294185638428\n",
      "Epoch 22, Loss: 1.6505603790283203\n",
      "Epoch 22, Loss: 1.9331320524215698\n",
      "Epoch 22, Loss: 1.8887476921081543\n",
      "Epoch 22, Loss: 1.982342004776001\n",
      "Epoch 22, Loss: 1.663712739944458\n",
      "Epoch 22, Loss: 1.5428907871246338\n",
      "Epoch 22, Loss: 1.5521488189697266\n",
      "Epoch 22, Loss: 1.9119794368743896\n",
      "Epoch 22, Loss: 1.8139013051986694\n",
      "Epoch 22, Loss: 1.605302333831787\n",
      "Epoch 22, Loss: 2.0569779872894287\n",
      "Epoch 22, Loss: 1.7364699840545654\n",
      "Epoch 22, Loss: 1.7461512088775635\n",
      "Epoch 22, Loss: 1.7146179676055908\n",
      "Epoch 22, Loss: 1.542556643486023\n",
      "Epoch 22, Loss: 1.9410172700881958\n",
      "Epoch 22, Loss: 2.0442965030670166\n",
      "Epoch 22, Loss: 1.923435926437378\n",
      "Epoch 22, Loss: 1.8791141510009766\n",
      "Epoch 22, Loss: 1.6563153266906738\n",
      "Epoch 22, Loss: 1.7409260272979736\n",
      "Epoch 22, Loss: 1.8969311714172363\n",
      "Epoch 22, Loss: 1.6445335149765015\n",
      "Epoch 22, Loss: 1.7181628942489624\n",
      "Epoch 22, Loss: 1.9864786863327026\n",
      "Epoch 22, Loss: 1.8167250156402588\n",
      "Epoch 22, Loss: 1.8557246923446655\n",
      "Epoch 22, Loss: 1.6573081016540527\n",
      "Epoch 22, Loss: 2.0547502040863037\n",
      "Epoch 22, Loss: 1.734776258468628\n",
      "Epoch 22, Loss: 1.7446377277374268\n",
      "Epoch 23, Loss: 1.7422857284545898\n",
      "Epoch 23, Loss: 1.8254722356796265\n",
      "Epoch 23, Loss: 1.981986403465271\n",
      "Epoch 23, Loss: 2.0591187477111816\n",
      "Epoch 23, Loss: 1.6756510734558105\n",
      "Epoch 23, Loss: 1.835113763809204\n",
      "Epoch 23, Loss: 1.7299041748046875\n",
      "Epoch 23, Loss: 1.9079813957214355\n",
      "Epoch 23, Loss: 1.7566776275634766\n",
      "Epoch 23, Loss: 1.5510261058807373\n",
      "Epoch 23, Loss: 1.9653515815734863\n",
      "Epoch 23, Loss: 1.913051962852478\n",
      "Epoch 23, Loss: 1.940773844718933\n",
      "Epoch 23, Loss: 1.7641009092330933\n",
      "Epoch 23, Loss: 1.777942419052124\n",
      "Epoch 23, Loss: 1.7170344591140747\n",
      "Epoch 23, Loss: 1.8863656520843506\n",
      "Epoch 23, Loss: 1.6545629501342773\n",
      "Epoch 23, Loss: 1.680991291999817\n",
      "Epoch 23, Loss: 1.9608535766601562\n",
      "Epoch 23, Loss: 1.7227643728256226\n",
      "Epoch 23, Loss: 1.9729200601577759\n",
      "Epoch 23, Loss: 1.706437587738037\n",
      "Epoch 23, Loss: 1.678117275238037\n",
      "Epoch 23, Loss: 1.680054783821106\n",
      "Epoch 23, Loss: 1.558803677558899\n",
      "Epoch 23, Loss: 1.7668806314468384\n",
      "Epoch 23, Loss: 1.9705218076705933\n",
      "Epoch 23, Loss: 1.6472508907318115\n",
      "Epoch 23, Loss: 1.8186360597610474\n",
      "Epoch 23, Loss: 1.9135732650756836\n",
      "Epoch 23, Loss: 1.9525645971298218\n",
      "Epoch 23, Loss: 1.6389390230178833\n",
      "Epoch 23, Loss: 1.9035751819610596\n",
      "Epoch 23, Loss: 1.728798508644104\n",
      "Epoch 23, Loss: 1.6800367832183838\n",
      "Epoch 23, Loss: 1.658358097076416\n",
      "Epoch 23, Loss: 1.8156952857971191\n",
      "Epoch 23, Loss: 2.0080599784851074\n",
      "Epoch 23, Loss: 1.8873484134674072\n",
      "Epoch 23, Loss: 1.8053903579711914\n",
      "Epoch 23, Loss: 1.932051420211792\n",
      "Epoch 23, Loss: 1.7055234909057617\n",
      "Epoch 23, Loss: 2.0754780769348145\n",
      "Epoch 23, Loss: 1.872060775756836\n",
      "Epoch 23, Loss: 1.942394495010376\n",
      "Epoch 23, Loss: 1.3658828735351562\n",
      "Epoch 23, Loss: 1.754814624786377\n",
      "Epoch 23, Loss: 1.8299994468688965\n",
      "Epoch 23, Loss: 1.8261158466339111\n",
      "Epoch 23, Loss: 1.7917189598083496\n",
      "Epoch 23, Loss: 2.038623332977295\n",
      "Epoch 23, Loss: 2.134990692138672\n",
      "Epoch 23, Loss: 1.8569544553756714\n",
      "Epoch 23, Loss: 1.7128818035125732\n",
      "Epoch 23, Loss: 1.9864474534988403\n",
      "Epoch 23, Loss: 1.7668790817260742\n",
      "Epoch 23, Loss: 1.8620226383209229\n",
      "Epoch 23, Loss: 1.7334513664245605\n",
      "Epoch 23, Loss: 1.7602287530899048\n",
      "Epoch 23, Loss: 1.7603752613067627\n",
      "Epoch 23, Loss: 1.942017674446106\n",
      "Epoch 23, Loss: 1.609825611114502\n",
      "Epoch 23, Loss: 1.7834910154342651\n",
      "Epoch 23, Loss: 1.8320651054382324\n",
      "Epoch 23, Loss: 1.868461012840271\n",
      "Epoch 23, Loss: 1.7069674730300903\n",
      "Epoch 23, Loss: 1.8196369409561157\n",
      "Epoch 23, Loss: 1.9268672466278076\n",
      "Epoch 23, Loss: 1.8341467380523682\n",
      "Epoch 23, Loss: 1.936798095703125\n",
      "Epoch 23, Loss: 1.93556809425354\n",
      "Epoch 23, Loss: 2.0125932693481445\n",
      "Epoch 23, Loss: 1.64231276512146\n",
      "Epoch 23, Loss: 1.902884840965271\n",
      "Epoch 24, Loss: 1.798861026763916\n",
      "Epoch 24, Loss: 1.8369510173797607\n",
      "Epoch 24, Loss: 1.591161847114563\n",
      "Epoch 24, Loss: 1.6934345960617065\n",
      "Epoch 24, Loss: 1.7530262470245361\n",
      "Epoch 24, Loss: 1.7062304019927979\n",
      "Epoch 24, Loss: 2.2024307250976562\n",
      "Epoch 24, Loss: 2.2158384323120117\n",
      "Epoch 24, Loss: 1.991214394569397\n",
      "Epoch 24, Loss: 1.7135794162750244\n",
      "Epoch 24, Loss: 1.928910255432129\n",
      "Epoch 24, Loss: 1.9299914836883545\n",
      "Epoch 24, Loss: 1.6647918224334717\n",
      "Epoch 24, Loss: 1.7217826843261719\n",
      "Epoch 24, Loss: 1.7774885892868042\n",
      "Epoch 24, Loss: 1.9104819297790527\n",
      "Epoch 24, Loss: 1.934309482574463\n",
      "Epoch 24, Loss: 1.6295087337493896\n",
      "Epoch 24, Loss: 1.9491009712219238\n",
      "Epoch 24, Loss: 1.6826856136322021\n",
      "Epoch 24, Loss: 1.7550283670425415\n",
      "Epoch 24, Loss: 1.759140133857727\n",
      "Epoch 24, Loss: 2.13718318939209\n",
      "Epoch 24, Loss: 1.8592817783355713\n",
      "Epoch 24, Loss: 1.6145265102386475\n",
      "Epoch 24, Loss: 1.805910587310791\n",
      "Epoch 24, Loss: 1.73297119140625\n",
      "Epoch 24, Loss: 1.6704161167144775\n",
      "Epoch 24, Loss: 1.7064000368118286\n",
      "Epoch 24, Loss: 1.6635591983795166\n",
      "Epoch 24, Loss: 1.6386958360671997\n",
      "Epoch 24, Loss: 1.5995726585388184\n",
      "Epoch 24, Loss: 2.027892827987671\n",
      "Epoch 24, Loss: 1.7010290622711182\n",
      "Epoch 24, Loss: 1.966691017150879\n",
      "Epoch 24, Loss: 1.9416468143463135\n",
      "Epoch 24, Loss: 2.130457878112793\n",
      "Epoch 24, Loss: 1.8852814435958862\n",
      "Epoch 24, Loss: 1.6642554998397827\n",
      "Epoch 24, Loss: 1.971343755722046\n",
      "Epoch 24, Loss: 1.6901053190231323\n",
      "Epoch 24, Loss: 1.720076084136963\n",
      "Epoch 24, Loss: 1.812342882156372\n",
      "Epoch 24, Loss: 1.9436064958572388\n",
      "Epoch 24, Loss: 1.7539215087890625\n",
      "Epoch 24, Loss: 1.7173047065734863\n",
      "Epoch 24, Loss: 1.8130499124526978\n",
      "Epoch 24, Loss: 1.4836539030075073\n",
      "Epoch 24, Loss: 1.9372395277023315\n",
      "Epoch 24, Loss: 1.6170310974121094\n",
      "Epoch 24, Loss: 1.8533023595809937\n",
      "Epoch 24, Loss: 1.796109676361084\n",
      "Epoch 24, Loss: 2.0345797538757324\n",
      "Epoch 24, Loss: 1.8298072814941406\n",
      "Epoch 24, Loss: 1.7831149101257324\n",
      "Epoch 24, Loss: 1.7138513326644897\n",
      "Epoch 24, Loss: 2.0763792991638184\n",
      "Epoch 24, Loss: 1.905701756477356\n",
      "Epoch 24, Loss: 2.002145767211914\n",
      "Epoch 24, Loss: 1.8918960094451904\n",
      "Epoch 24, Loss: 1.8931357860565186\n",
      "Epoch 24, Loss: 1.7953743934631348\n",
      "Epoch 24, Loss: 1.8878908157348633\n",
      "Epoch 24, Loss: 1.7137519121170044\n",
      "Epoch 24, Loss: 1.5078128576278687\n",
      "Epoch 24, Loss: 1.8334603309631348\n",
      "Epoch 24, Loss: 1.696746587753296\n",
      "Epoch 24, Loss: 1.8473446369171143\n",
      "Epoch 24, Loss: 1.6432980298995972\n",
      "Epoch 24, Loss: 1.8751354217529297\n",
      "Epoch 24, Loss: 1.8365899324417114\n",
      "Epoch 24, Loss: 1.7382190227508545\n",
      "Epoch 24, Loss: 1.9723405838012695\n",
      "Epoch 24, Loss: 1.6913279294967651\n",
      "Epoch 24, Loss: 1.9009956121444702\n",
      "Epoch 25, Loss: 1.9896471500396729\n",
      "Epoch 25, Loss: 2.114407539367676\n",
      "Epoch 25, Loss: 1.826059103012085\n",
      "Epoch 25, Loss: 1.766667127609253\n",
      "Epoch 25, Loss: 1.7800102233886719\n",
      "Epoch 25, Loss: 1.6636710166931152\n",
      "Epoch 25, Loss: 1.6369494199752808\n",
      "Epoch 25, Loss: 1.8714690208435059\n",
      "Epoch 25, Loss: 1.5968453884124756\n",
      "Epoch 25, Loss: 1.5956389904022217\n",
      "Epoch 25, Loss: 1.8488130569458008\n",
      "Epoch 25, Loss: 1.6784976720809937\n",
      "Epoch 25, Loss: 1.9598793983459473\n",
      "Epoch 25, Loss: 1.7779386043548584\n",
      "Epoch 25, Loss: 1.8222383260726929\n",
      "Epoch 25, Loss: 1.7827523946762085\n",
      "Epoch 25, Loss: 1.6410224437713623\n",
      "Epoch 25, Loss: 1.7989422082901\n",
      "Epoch 25, Loss: 1.9233365058898926\n",
      "Epoch 25, Loss: 2.0312023162841797\n",
      "Epoch 25, Loss: 1.9448710680007935\n",
      "Epoch 25, Loss: 1.838012933731079\n",
      "Epoch 25, Loss: 1.691584825515747\n",
      "Epoch 25, Loss: 1.8087255954742432\n",
      "Epoch 25, Loss: 1.7437922954559326\n",
      "Epoch 25, Loss: 1.7399901151657104\n",
      "Epoch 25, Loss: 1.9415569305419922\n",
      "Epoch 25, Loss: 1.8150243759155273\n",
      "Epoch 25, Loss: 1.8927356004714966\n",
      "Epoch 25, Loss: 1.5993938446044922\n",
      "Epoch 25, Loss: 1.6965322494506836\n",
      "Epoch 25, Loss: 1.7745482921600342\n",
      "Epoch 25, Loss: 1.843104362487793\n",
      "Epoch 25, Loss: 1.5345079898834229\n",
      "Epoch 25, Loss: 1.6181731224060059\n",
      "Epoch 25, Loss: 1.844405174255371\n",
      "Epoch 25, Loss: 1.683950424194336\n",
      "Epoch 25, Loss: 2.188338041305542\n",
      "Epoch 25, Loss: 1.9005049467086792\n",
      "Epoch 25, Loss: 1.6558678150177002\n",
      "Epoch 25, Loss: 1.7510124444961548\n",
      "Epoch 25, Loss: 1.893757939338684\n",
      "Epoch 25, Loss: 1.8303605318069458\n",
      "Epoch 25, Loss: 1.8022772073745728\n",
      "Epoch 25, Loss: 1.9333865642547607\n",
      "Epoch 25, Loss: 1.6429167985916138\n",
      "Epoch 25, Loss: 1.9958868026733398\n",
      "Epoch 25, Loss: 1.6736962795257568\n",
      "Epoch 25, Loss: 1.8494311571121216\n",
      "Epoch 25, Loss: 1.92887544631958\n",
      "Epoch 25, Loss: 1.8003491163253784\n",
      "Epoch 25, Loss: 1.8885297775268555\n",
      "Epoch 25, Loss: 1.6679812669754028\n",
      "Epoch 25, Loss: 1.597475290298462\n",
      "Epoch 25, Loss: 1.927160382270813\n",
      "Epoch 25, Loss: 1.9294626712799072\n",
      "Epoch 25, Loss: 1.741865634918213\n",
      "Epoch 25, Loss: 1.9328057765960693\n",
      "Epoch 25, Loss: 1.9109121561050415\n",
      "Epoch 25, Loss: 1.7854965925216675\n",
      "Epoch 25, Loss: 1.5998201370239258\n",
      "Epoch 25, Loss: 1.6431753635406494\n",
      "Epoch 25, Loss: 1.8480124473571777\n",
      "Epoch 25, Loss: 1.8505759239196777\n",
      "Epoch 25, Loss: 1.856054663658142\n",
      "Epoch 25, Loss: 1.5622893571853638\n",
      "Epoch 25, Loss: 1.950865626335144\n",
      "Epoch 25, Loss: 2.047661542892456\n",
      "Epoch 25, Loss: 2.2055039405822754\n",
      "Epoch 25, Loss: 1.4926972389221191\n",
      "Epoch 25, Loss: 1.8914172649383545\n",
      "Epoch 25, Loss: 2.0282106399536133\n",
      "Epoch 25, Loss: 1.6367154121398926\n",
      "Epoch 25, Loss: 1.9155900478363037\n",
      "Epoch 25, Loss: 2.1080150604248047\n",
      "Epoch 26, Loss: 1.8153209686279297\n",
      "Epoch 26, Loss: 1.6774929761886597\n",
      "Epoch 26, Loss: 1.8439068794250488\n",
      "Epoch 26, Loss: 1.6201446056365967\n",
      "Epoch 26, Loss: 1.7349718809127808\n",
      "Epoch 26, Loss: 1.6394591331481934\n",
      "Epoch 26, Loss: 2.074551582336426\n",
      "Epoch 26, Loss: 1.699303388595581\n",
      "Epoch 26, Loss: 1.614182710647583\n",
      "Epoch 26, Loss: 1.528028130531311\n",
      "Epoch 26, Loss: 1.6293690204620361\n",
      "Epoch 26, Loss: 1.7744219303131104\n",
      "Epoch 26, Loss: 1.8446812629699707\n",
      "Epoch 26, Loss: 1.6121013164520264\n",
      "Epoch 26, Loss: 1.9291484355926514\n",
      "Epoch 26, Loss: 1.5674179792404175\n",
      "Epoch 26, Loss: 1.8312610387802124\n",
      "Epoch 26, Loss: 1.8629778623580933\n",
      "Epoch 26, Loss: 1.8759833574295044\n",
      "Epoch 26, Loss: 1.8957445621490479\n",
      "Epoch 26, Loss: 1.633297324180603\n",
      "Epoch 26, Loss: 1.8431744575500488\n",
      "Epoch 26, Loss: 1.5516847372055054\n",
      "Epoch 26, Loss: 1.7961387634277344\n",
      "Epoch 26, Loss: 1.874096393585205\n",
      "Epoch 26, Loss: 1.8162994384765625\n",
      "Epoch 26, Loss: 1.9598875045776367\n",
      "Epoch 26, Loss: 1.8645741939544678\n",
      "Epoch 26, Loss: 1.7972344160079956\n",
      "Epoch 26, Loss: 1.9964609146118164\n",
      "Epoch 26, Loss: 2.1117587089538574\n",
      "Epoch 26, Loss: 1.7811894416809082\n",
      "Epoch 26, Loss: 1.8680624961853027\n",
      "Epoch 26, Loss: 1.7147302627563477\n",
      "Epoch 26, Loss: 1.734452724456787\n",
      "Epoch 26, Loss: 1.8786530494689941\n",
      "Epoch 26, Loss: 1.7647827863693237\n",
      "Epoch 26, Loss: 1.891462802886963\n",
      "Epoch 26, Loss: 1.842794418334961\n",
      "Epoch 26, Loss: 2.2666218280792236\n",
      "Epoch 26, Loss: 1.8402878046035767\n",
      "Epoch 26, Loss: 1.9537944793701172\n",
      "Epoch 26, Loss: 1.7449650764465332\n",
      "Epoch 26, Loss: 1.8452296257019043\n",
      "Epoch 26, Loss: 1.818387508392334\n",
      "Epoch 26, Loss: 1.8575654029846191\n",
      "Epoch 26, Loss: 1.7454768419265747\n",
      "Epoch 26, Loss: 1.9045180082321167\n",
      "Epoch 26, Loss: 1.5886633396148682\n",
      "Epoch 26, Loss: 1.7358994483947754\n",
      "Epoch 26, Loss: 1.7172727584838867\n",
      "Epoch 26, Loss: 1.9489893913269043\n",
      "Epoch 26, Loss: 1.6041029691696167\n",
      "Epoch 26, Loss: 2.2196872234344482\n",
      "Epoch 26, Loss: 1.6184728145599365\n",
      "Epoch 26, Loss: 1.785599708557129\n",
      "Epoch 26, Loss: 1.8997247219085693\n",
      "Epoch 26, Loss: 1.6466453075408936\n",
      "Epoch 26, Loss: 1.829726219177246\n",
      "Epoch 26, Loss: 1.8806891441345215\n",
      "Epoch 26, Loss: 1.6405538320541382\n",
      "Epoch 26, Loss: 1.9828474521636963\n",
      "Epoch 26, Loss: 1.8505380153656006\n",
      "Epoch 26, Loss: 1.652714729309082\n",
      "Epoch 26, Loss: 1.8316686153411865\n",
      "Epoch 26, Loss: 1.787857174873352\n",
      "Epoch 26, Loss: 1.8021881580352783\n",
      "Epoch 26, Loss: 1.5267119407653809\n",
      "Epoch 26, Loss: 2.0701589584350586\n",
      "Epoch 26, Loss: 1.7873244285583496\n",
      "Epoch 26, Loss: 1.7555546760559082\n",
      "Epoch 26, Loss: 1.9408562183380127\n",
      "Epoch 26, Loss: 1.7429940700531006\n",
      "Epoch 26, Loss: 1.929203987121582\n",
      "Epoch 26, Loss: 2.0495049953460693\n",
      "Epoch 27, Loss: 1.8110077381134033\n",
      "Epoch 27, Loss: 1.9628002643585205\n",
      "Epoch 27, Loss: 1.7176767587661743\n",
      "Epoch 27, Loss: 1.8382563591003418\n",
      "Epoch 27, Loss: 1.7455129623413086\n",
      "Epoch 27, Loss: 1.790006160736084\n",
      "Epoch 27, Loss: 1.6205730438232422\n",
      "Epoch 27, Loss: 1.5918350219726562\n",
      "Epoch 27, Loss: 1.909780502319336\n",
      "Epoch 27, Loss: 1.842044472694397\n",
      "Epoch 27, Loss: 1.8255808353424072\n",
      "Epoch 27, Loss: 1.6981940269470215\n",
      "Epoch 27, Loss: 1.6749553680419922\n",
      "Epoch 27, Loss: 1.7351884841918945\n",
      "Epoch 27, Loss: 2.053617238998413\n",
      "Epoch 27, Loss: 1.7782093286514282\n",
      "Epoch 27, Loss: 1.9067299365997314\n",
      "Epoch 27, Loss: 1.7956972122192383\n",
      "Epoch 27, Loss: 1.7672942876815796\n",
      "Epoch 27, Loss: 1.8093066215515137\n",
      "Epoch 27, Loss: 1.8373277187347412\n",
      "Epoch 27, Loss: 1.8559114933013916\n",
      "Epoch 27, Loss: 1.8016932010650635\n",
      "Epoch 27, Loss: 1.7237766981124878\n",
      "Epoch 27, Loss: 1.839024305343628\n",
      "Epoch 27, Loss: 1.8780591487884521\n",
      "Epoch 27, Loss: 1.6231290102005005\n",
      "Epoch 27, Loss: 1.858351707458496\n",
      "Epoch 27, Loss: 1.8254270553588867\n",
      "Epoch 27, Loss: 1.674926519393921\n",
      "Epoch 27, Loss: 1.637366533279419\n",
      "Epoch 27, Loss: 1.7464725971221924\n",
      "Epoch 27, Loss: 1.8355274200439453\n",
      "Epoch 27, Loss: 1.6925244331359863\n",
      "Epoch 27, Loss: 1.7596534490585327\n",
      "Epoch 27, Loss: 2.1901659965515137\n",
      "Epoch 27, Loss: 1.8321645259857178\n",
      "Epoch 27, Loss: 2.188352346420288\n",
      "Epoch 27, Loss: 1.6451287269592285\n",
      "Epoch 27, Loss: 1.7405858039855957\n",
      "Epoch 27, Loss: 2.0229034423828125\n",
      "Epoch 27, Loss: 2.174149513244629\n",
      "Epoch 27, Loss: 1.831770420074463\n",
      "Epoch 27, Loss: 1.7788681983947754\n",
      "Epoch 27, Loss: 1.9332019090652466\n",
      "Epoch 27, Loss: 2.1808457374572754\n",
      "Epoch 27, Loss: 1.7812168598175049\n",
      "Epoch 27, Loss: 1.995071291923523\n",
      "Epoch 27, Loss: 1.7977545261383057\n",
      "Epoch 27, Loss: 1.8488283157348633\n",
      "Epoch 27, Loss: 1.6925923824310303\n",
      "Epoch 27, Loss: 1.7648603916168213\n",
      "Epoch 27, Loss: 1.6888359785079956\n",
      "Epoch 27, Loss: 1.9970734119415283\n",
      "Epoch 27, Loss: 1.7849466800689697\n",
      "Epoch 27, Loss: 1.6395174264907837\n",
      "Epoch 27, Loss: 1.739574909210205\n",
      "Epoch 27, Loss: 1.8340728282928467\n",
      "Epoch 27, Loss: 1.5656931400299072\n",
      "Epoch 27, Loss: 1.8784005641937256\n",
      "Epoch 27, Loss: 1.854987382888794\n",
      "Epoch 27, Loss: 1.895180583000183\n",
      "Epoch 27, Loss: 1.784543514251709\n",
      "Epoch 27, Loss: 1.702530860900879\n",
      "Epoch 27, Loss: 1.4852499961853027\n",
      "Epoch 27, Loss: 1.7191498279571533\n",
      "Epoch 27, Loss: 2.1697146892547607\n",
      "Epoch 27, Loss: 1.8638827800750732\n",
      "Epoch 27, Loss: 1.681654930114746\n",
      "Epoch 27, Loss: 1.7146368026733398\n",
      "Epoch 27, Loss: 1.5776251554489136\n",
      "Epoch 27, Loss: 1.5467209815979004\n",
      "Epoch 27, Loss: 1.6875560283660889\n",
      "Epoch 27, Loss: 1.7386168241500854\n",
      "Epoch 27, Loss: 2.011976718902588\n",
      "Epoch 28, Loss: 1.8009706735610962\n",
      "Epoch 28, Loss: 1.876266598701477\n",
      "Epoch 28, Loss: 1.811287522315979\n",
      "Epoch 28, Loss: 1.5170118808746338\n",
      "Epoch 28, Loss: 1.9005755186080933\n",
      "Epoch 28, Loss: 2.0621423721313477\n",
      "Epoch 28, Loss: 1.7487480640411377\n",
      "Epoch 28, Loss: 1.957848072052002\n",
      "Epoch 28, Loss: 1.7574057579040527\n",
      "Epoch 28, Loss: 1.6856353282928467\n",
      "Epoch 28, Loss: 1.785539984703064\n",
      "Epoch 28, Loss: 1.8167314529418945\n",
      "Epoch 28, Loss: 1.9397685527801514\n",
      "Epoch 28, Loss: 1.870568037033081\n",
      "Epoch 28, Loss: 1.6282237768173218\n",
      "Epoch 28, Loss: 1.6363235712051392\n",
      "Epoch 28, Loss: 1.6983764171600342\n",
      "Epoch 28, Loss: 1.71719491481781\n",
      "Epoch 28, Loss: 1.8394569158554077\n",
      "Epoch 28, Loss: 1.7611463069915771\n",
      "Epoch 28, Loss: 1.7033109664916992\n",
      "Epoch 28, Loss: 1.8181467056274414\n",
      "Epoch 28, Loss: 2.016256332397461\n",
      "Epoch 28, Loss: 1.8802343606948853\n",
      "Epoch 28, Loss: 1.879213809967041\n",
      "Epoch 28, Loss: 1.7621216773986816\n",
      "Epoch 28, Loss: 1.9503026008605957\n",
      "Epoch 28, Loss: 1.8207530975341797\n",
      "Epoch 28, Loss: 1.750388503074646\n",
      "Epoch 28, Loss: 1.8079766035079956\n",
      "Epoch 28, Loss: 1.6812443733215332\n",
      "Epoch 28, Loss: 1.6420142650604248\n",
      "Epoch 28, Loss: 1.6828689575195312\n",
      "Epoch 28, Loss: 2.078625440597534\n",
      "Epoch 28, Loss: 1.6342761516571045\n",
      "Epoch 28, Loss: 1.9060662984848022\n",
      "Epoch 28, Loss: 1.6847541332244873\n",
      "Epoch 28, Loss: 1.7517051696777344\n",
      "Epoch 28, Loss: 2.136345863342285\n",
      "Epoch 28, Loss: 1.6789219379425049\n",
      "Epoch 28, Loss: 1.7794032096862793\n",
      "Epoch 28, Loss: 1.8266642093658447\n",
      "Epoch 28, Loss: 1.8567183017730713\n",
      "Epoch 28, Loss: 2.129063129425049\n",
      "Epoch 28, Loss: 1.977696180343628\n",
      "Epoch 28, Loss: 1.71468186378479\n",
      "Epoch 28, Loss: 1.6456202268600464\n",
      "Epoch 28, Loss: 1.5310691595077515\n",
      "Epoch 28, Loss: 1.7304277420043945\n",
      "Epoch 28, Loss: 1.9935965538024902\n",
      "Epoch 28, Loss: 1.8946343660354614\n",
      "Epoch 28, Loss: 1.8519575595855713\n",
      "Epoch 28, Loss: 1.8941882848739624\n",
      "Epoch 28, Loss: 1.7515665292739868\n",
      "Epoch 28, Loss: 1.7883604764938354\n",
      "Epoch 28, Loss: 1.8280184268951416\n",
      "Epoch 28, Loss: 1.9804068803787231\n",
      "Epoch 28, Loss: 1.672371745109558\n",
      "Epoch 28, Loss: 1.836968183517456\n",
      "Epoch 28, Loss: 1.9787662029266357\n",
      "Epoch 28, Loss: 1.5600073337554932\n",
      "Epoch 28, Loss: 1.76702082157135\n",
      "Epoch 28, Loss: 1.7580931186676025\n",
      "Epoch 28, Loss: 1.7151072025299072\n",
      "Epoch 28, Loss: 1.9522411823272705\n",
      "Epoch 28, Loss: 1.679774284362793\n",
      "Epoch 28, Loss: 1.6839303970336914\n",
      "Epoch 28, Loss: 1.6385741233825684\n",
      "Epoch 28, Loss: 1.785728096961975\n",
      "Epoch 28, Loss: 1.9343838691711426\n",
      "Epoch 28, Loss: 1.9240357875823975\n",
      "Epoch 28, Loss: 1.881070613861084\n",
      "Epoch 28, Loss: 1.8766547441482544\n",
      "Epoch 28, Loss: 1.666544795036316\n",
      "Epoch 28, Loss: 1.948503851890564\n",
      "Epoch 29, Loss: 1.674638271331787\n",
      "Epoch 29, Loss: 1.7706818580627441\n",
      "Epoch 29, Loss: 1.8074983358383179\n",
      "Epoch 29, Loss: 1.7410691976547241\n",
      "Epoch 29, Loss: 1.9462412595748901\n",
      "Epoch 29, Loss: 1.901652216911316\n",
      "Epoch 29, Loss: 1.889703392982483\n",
      "Epoch 29, Loss: 1.8129773139953613\n",
      "Epoch 29, Loss: 1.7679728269577026\n",
      "Epoch 29, Loss: 1.8489727973937988\n",
      "Epoch 29, Loss: 1.8674383163452148\n",
      "Epoch 29, Loss: 1.4410158395767212\n",
      "Epoch 29, Loss: 1.7121789455413818\n",
      "Epoch 29, Loss: 1.923780918121338\n",
      "Epoch 29, Loss: 1.8023775815963745\n",
      "Epoch 29, Loss: 1.8192826509475708\n",
      "Epoch 29, Loss: 1.7731435298919678\n",
      "Epoch 29, Loss: 1.6848104000091553\n",
      "Epoch 29, Loss: 1.8236494064331055\n",
      "Epoch 29, Loss: 2.0152041912078857\n",
      "Epoch 29, Loss: 1.7208080291748047\n",
      "Epoch 29, Loss: 1.8651390075683594\n",
      "Epoch 29, Loss: 1.8022788763046265\n",
      "Epoch 29, Loss: 1.8758490085601807\n",
      "Epoch 29, Loss: 1.972489356994629\n",
      "Epoch 29, Loss: 1.70769202709198\n",
      "Epoch 29, Loss: 1.8510465621948242\n",
      "Epoch 29, Loss: 1.7262096405029297\n",
      "Epoch 29, Loss: 1.7362110614776611\n",
      "Epoch 29, Loss: 1.8632616996765137\n",
      "Epoch 29, Loss: 2.076690196990967\n",
      "Epoch 29, Loss: 1.9067955017089844\n",
      "Epoch 29, Loss: 2.0135719776153564\n",
      "Epoch 29, Loss: 1.5636166334152222\n",
      "Epoch 29, Loss: 1.829999327659607\n",
      "Epoch 29, Loss: 1.9409915208816528\n",
      "Epoch 29, Loss: 2.0446555614471436\n",
      "Epoch 29, Loss: 1.8728089332580566\n",
      "Epoch 29, Loss: 1.7348570823669434\n",
      "Epoch 29, Loss: 1.598858118057251\n",
      "Epoch 29, Loss: 1.769437313079834\n",
      "Epoch 29, Loss: 1.9376657009124756\n",
      "Epoch 29, Loss: 1.838419795036316\n",
      "Epoch 29, Loss: 1.766333818435669\n",
      "Epoch 29, Loss: 1.6787605285644531\n",
      "Epoch 29, Loss: 1.8963227272033691\n",
      "Epoch 29, Loss: 1.6635515689849854\n",
      "Epoch 29, Loss: 2.1762173175811768\n",
      "Epoch 29, Loss: 2.0296740531921387\n",
      "Epoch 29, Loss: 1.657254934310913\n",
      "Epoch 29, Loss: 1.6904516220092773\n",
      "Epoch 29, Loss: 1.8088250160217285\n",
      "Epoch 29, Loss: 1.5584383010864258\n",
      "Epoch 29, Loss: 1.7022817134857178\n",
      "Epoch 29, Loss: 1.704890489578247\n",
      "Epoch 29, Loss: 1.8099960088729858\n",
      "Epoch 29, Loss: 1.6852413415908813\n",
      "Epoch 29, Loss: 1.710664987564087\n",
      "Epoch 29, Loss: 1.6819045543670654\n",
      "Epoch 29, Loss: 1.5216104984283447\n",
      "Epoch 29, Loss: 1.770725965499878\n",
      "Epoch 29, Loss: 1.754225492477417\n",
      "Epoch 29, Loss: 1.7112056016921997\n",
      "Epoch 29, Loss: 1.9496339559555054\n",
      "Epoch 29, Loss: 1.7991478443145752\n",
      "Epoch 29, Loss: 1.8746100664138794\n",
      "Epoch 29, Loss: 1.8649942874908447\n",
      "Epoch 29, Loss: 1.907639741897583\n",
      "Epoch 29, Loss: 1.8746191263198853\n",
      "Epoch 29, Loss: 1.6485543251037598\n",
      "Epoch 29, Loss: 1.8799071311950684\n",
      "Epoch 29, Loss: 1.6291546821594238\n",
      "Epoch 29, Loss: 1.7647442817687988\n",
      "Epoch 29, Loss: 1.7365959882736206\n",
      "Epoch 29, Loss: 1.815549612045288\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "linear_mapper.to(device).train()\n",
    "linear_optimizer = torch.optim.Adam(linear_mapper.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "batch_size = 64 if LANGUAGE == \"fr\" else 16\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    name_format = r\"linear_mapper_.*(\\d+)\\.pth$\"\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if re.search(name_format, f)]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(name_format, x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(name_format, checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            linear_mapper.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "prepare_folders()\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    features_dataset = features_dataset.shuffle()\n",
    "    for i in range(0, features_dataset.num_rows, batch_size):\n",
    "        batch_data = features_dataset[i:i + batch_size]\n",
    "\n",
    "\n",
    "        input_lengths = torch.zeros(batch_size, dtype=torch.uint32)\n",
    "        max_len = max(map(len, batch_data[\"features\"]))\n",
    "\n",
    "        input_batch = torch.zeros((batch_size, max_len, wavlm_model.config.hidden_size)).to(device)\n",
    "        for i, feat in enumerate(batch_data[\"features\"]):\n",
    "            input_batch[i, :feat.shape[0]] = feat\n",
    "            input_lengths[i] = feat.shape[0]\n",
    "\n",
    "        log_probs = linear_mapper(\n",
    "            input_batch.reshape(\n",
    "                (batch_size, -1, wavlm_model.config.hidden_size)\n",
    "            )\n",
    "        )\n",
    "        losses = []\n",
    "\n",
    "        for coder in (\"1\", \"2\"):\n",
    "            targets = [phoneme_recognizer.tokenize(string, lenient=True) for string in batch_data[f\"target_phonemes{coder}\"]]\n",
    "            target_lengths = torch.zeros(batch_size, dtype=torch.uint8)\n",
    "            max_len = max(map(lambda x: x.shape[0], targets))\n",
    "\n",
    "            target_batch = torch.zeros((batch_size, max_len))\n",
    "            for i, target in enumerate(targets):\n",
    "                target_batch[i, :target.shape[0]] = target\n",
    "                target_lengths[i] = target.shape[0]\n",
    "\n",
    "            losses.append(F.ctc_loss(\n",
    "                log_probs.transpose(0, 1),\n",
    "                target_batch,\n",
    "                input_lengths=torch.tensor([x.shape[0] for x in log_probs]),\n",
    "                target_lengths=target_lengths\n",
    "            ))\n",
    "\n",
    "        loss = (losses[0] + losses[1]) / 2\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        for logs, target_phons1, target_phons2 in zip(phoneme_recognizer.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"], batch_data[\"target_phonemes2\"]):\n",
    "            write_to_csv(\n",
    "                [\n",
    "                    increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1), \"\".join(target_phons2)\n",
    "                ]\n",
    "            )\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        increment += 1\n",
    "    torch.save(\n",
    "        linear_mapper.state_dict(),\n",
    "        f\"{MODEL_DIR}/linear_mapper_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e2537",
   "metadata": {},
   "source": [
    "## View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcce0445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "\n",
    "with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        losses.append(float(row[3]))\n",
    "\n",
    "plt.plot(losses, label=\"Loss over training step\", linestyle='dotted')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df87423",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we want a phoneme recognition.\n",
    "It means to train the last layer of the model to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b488c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4194483757019043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.888230800628662\n",
      "Epoch 0, Loss: 1.146170735359192\n",
      "Epoch 0, Loss: 6.249912738800049\n",
      "Epoch 0, Loss: 1.2091354131698608\n",
      "Epoch 0, Loss: 3.8059654235839844\n",
      "Epoch 0, Loss: 2.063833713531494\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "phoneme_recognizer.train()\n",
    "linear_optimizer = torch.optim.Adam(\n",
    "    phoneme_recognizer.phoneme_classifier.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_ctc_loss(log_probs, target_sequence):\n",
    "    \"\"\"Calculates CTC loss.\"\"\"\n",
    "    # Create input_lengths and target_lengths tensors\n",
    "    input_lengths = torch.tensor([1])  # Batch size of 1\n",
    "    target_lengths = torch.tensor([1])  # Batch size of 1\n",
    "\n",
    "    # Calculate CTC loss\n",
    "    loss = F.ctc_loss(\n",
    "        log_probs,\n",
    "        target_sequence,\n",
    "        input_lengths=input_lengths,\n",
    "        target_lengths=target_lengths\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if f.endswith(\".pth\")]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(r\"(\\d+)\\.pth$\", x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(r\"(\\d+)\\.pth$\", checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            phoneme_recognizer.phoneme_classifier.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "prepare_folders()\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "\n",
    "# Freeze the wavlm model\n",
    "for param in phoneme_recognizer.wavlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(dataset.shuffle().select(range(50))):\n",
    "        inputs = preprocess_audios(data)\n",
    "        log_probs = phoneme_recognizer(inputs)\n",
    "        split_phonemes = smart_split_coder(data[\"phoneme_sequence\"][0])\n",
    "        target = phoneme_recognizer.tokenize(split_phonemes)\n",
    "        loss = calculate_ctc_loss(log_probs[0], target.reshape([1, -1]))\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        write_to_csv(\n",
    "            [\n",
    "                increment, epoch, i, loss.item(),\n",
    "                \"\".join(phoneme_recognizer.classify_to_phonemes(log_probs)[0]),\n",
    "                \"\".join(split_phonemes)\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        increment += 1\n",
    "        torch.save(\n",
    "            phoneme_recognizer.phoneme_classifier.state_dict(),\n",
    "            f\"{MODEL_DIR}/phoneme_classifier_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d7f10",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "We have a model roughly trained for phonemes.\n",
    "We want a binary classification though.\n",
    "We won't do that for now as it would be an end-to-end pipeline, defeating the purpose of the created pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
