{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b6c7c4",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Let's create a model first, with some vocab.\n",
    "The output is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f115131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2685.28s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.12/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.venv/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa numpy soundfile torch torchaudio datasets transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d917c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities shape: torch.Size([1, 292, 50])\n",
      "Recognized phoneme sequence: ['y', '[UNK]', 'k', 'ŋ', '(', 'ŋ', 'ʃ', 'k', 'œ', 'k', 'l', '[PAD]', 'ɜ', 'l', 'o', '̪', 'w', '[PAD]', 'e', '[UNK]', 'ɛ', '[UNK]', '̪', '[PAD]', 'ɛ', 'b', '(', 'o', 'k', 'w', 'l', 'ʌ', 'ŋ', 'u', 'ʌ', 'u', 'y', '[PAD]', 'ɑ', 'ʌ', 'w', 'ŋ', 'z', 'b', 'l', 'œ', 'ɡ', 'ɛ', 'l', 'd', 'ɛ', 'b', 'e', 'ʁ', 'a', 'ɛ', 'ː', 'l', 'ɛ', 'ɔ', 'y', 'n', 'ŋ', 'n', 'y', 't', 'y', 'l', 'y', 'b', 'ɒ', 'œ', 'y', 'ʌ', '[UNK]', 'l', '(', 'ʌ', '[PAD]', 'l', 'ɛ', 'y', 'v', 'o', 'e', 'ɛ', 'ɜ', 'e', 'l', 'ɛ', 'n', 'ɲ', 'ʌ', 'ɪ', 'y', 'a', 'y', 'a', ')', 'ʊ', 'ɾ', '(', 'p', 'l', 'ɑ', 'ɾ', 'ɪ', 'w', 'w', '̪', 'ʌ', 'j', '[UNK]', 'ʌ', '<blank>', '[PAD]', 'v', 'b', 'w', 'i', 'b', 'ʊ', 'ɪ', '[UNK]', 't', 'b', '<blank>', '[PAD]', 'ɾ', 'ɡ', 'b', 'ʌ', 'y', 'w', '(', 'ɛ', 'b', 'v', 'o', 'l', '[UNK]', 'p', 'e', ')', 'w', 'ʌ', 'y', 'k', 'y', 'œ', 'ɾ', '[UNK]', 'y', 'œ', 'ɛ', 'b', 'e', 'l', '<blank>', 'l', 'œ', 'l', 'ɛ', '(', 'ɑ', 'ɡ', 'ɛ', 'ʃ', 'ː', 'w', 'ʊ']\n",
      "Transcript for reference: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# PhonemeRecognizer: WavLM + CTC for phoneme speech recognition\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# Load vocab from file\n",
    "with open(\"phoneme_tokenizer/vocab.json\") as vocab_file:\n",
    "    vocab = json.load(vocab_file)\n",
    "\n",
    "# IT + FR phonemes + blank\n",
    "VOCAB = {\n",
    "  \"0\": \"ʒ\",\n",
    "  \"1\": \"ɹ\",\n",
    "  \"2\": \"j\",\n",
    "  \"3\": \"d\",\n",
    "  \"4\": \"ɲ\",\n",
    "  \"5\": \"ʌ\",\n",
    "  \"6\": \"[UNK]\",\n",
    "  \"7\": \"ɒ\",\n",
    "  \"8\": \"ɐ\",\n",
    "  \"9\": \"ʃ\",\n",
    "  \"10\": \"ɔ\",\n",
    "  \"11\": \"f\",\n",
    "  \"12\": \"ø\",\n",
    "  \"13\": \"z\",\n",
    "  \"14\": \"ŋ\",\n",
    "  \"15\": \"i\",\n",
    "  \"16\": \"u\",\n",
    "  \"17\": \"̃\",\n",
    "  \"18\": \"o\",\n",
    "  \"19\": \"œ\",\n",
    "  \"20\": \"a\",\n",
    "  \"21\": \"(\",\n",
    "  \"22\": \"ə\",\n",
    "  \"23\": \"ɜ\",\n",
    "  \"24\": \"ɾ\",\n",
    "  \"25\": \"ː\",\n",
    "  \"26\": \"̪\",\n",
    "  \"27\": \"e\",\n",
    "  \"28\": \"b\",\n",
    "  \"29\": \"ʁ\",\n",
    "  \"30\": \"w\",\n",
    "  \"31\": \"n\",\n",
    "  \"32\": \"p\",\n",
    "  \"33\": \"y\",\n",
    "  \"34\": \"ɡ\",\n",
    "  \"35\": \"ɪ\",\n",
    "  \"36\": \"r\",\n",
    "  \"37\": \"v\",\n",
    "  \"38\": \"t\",\n",
    "  \"39\": \")\",\n",
    "  \"40\": \"m\",\n",
    "  \"41\": \"k\",\n",
    "  \"42\": \"ʊ\",\n",
    "  \"43\": \"ʎ\",\n",
    "  \"44\": \"ɑ\",\n",
    "  \"45\": \"s\",\n",
    "  \"46\": \"l\",\n",
    "  \"47\": \"[PAD]\",\n",
    "  \"48\": \"ɛ\",\n",
    "  \"49\": '<blank>' # blank token for CTC\n",
    "}\n",
    "PHONEME_DICT = {v: int(k) for k, v in VOCAB.items()}\n",
    "\n",
    "NUM_PHONEMES = len(PHONEME_DICT)\n",
    "\n",
    "class PhonemeRecognizer(nn.Module):\n",
    "    def __init__(self, wavlm_model, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "        self.wavlm = wavlm_model\n",
    "\n",
    "        # Get the hidden size from the WavLM model\n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(hidden_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get WavLM embeddings\n",
    "        outputs = self.wavlm(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    phoneme = list(PHONEME_DICT.keys())[list(PHONEME_DICT.values()).index(p)]\n",
    "                    seq.append(phoneme)\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "\n",
    "\n",
    "    def recognize(self, inputs, beam_width=100):\n",
    "        \"\"\"Perform phoneme recognition with beam search decoding\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get log probabilities\n",
    "            log_probs = self(inputs)\n",
    "\n",
    "            return self.classify_to_phonemes(log_probs)\n",
    "\n",
    "    def tokenize(self, char_list):\n",
    "        \"\"\"Go from a list of characters to a list of indices.\"\"\"\n",
    "        return torch.tensor([PHONEME_DICT[x] for x in char_list])\n",
    "    \n",
    "    def get_embedding(self, char_list):\n",
    "        tokens = self.tokenize(char_list)\n",
    "        out_tensor = torch.zeros((len(tokens), len(PHONEME_DICT)))\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            out_tensor[i, token_id] = 1\n",
    "        return out_tensor\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# Method A: Using the PhonemeRecognizer for speech-to-phoneme ASR\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1. Load the feature extractor and model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "# Create the phoneme recognizer with the WavLM model\n",
    "phoneme_recognizer = PhonemeRecognizer(wavlm_model)\n",
    "phoneme_recognizer.eval()  # disable dropout, etc.\n",
    "\n",
    "# 2. Load an example audio file (here using a small demo from `datasets`)\n",
    "#    The `audio[\"array\"]` is a NumPy array of floats; sampling_rate is an int.\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "audio_sample = ds[0][\"audio\"][\"array\"]\n",
    "sr = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "def get_audio_features(data_row):\n",
    "    audio_sample = data_row[\"audio\"][\"array\"]\n",
    "    sr = data_row[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    # 3. Preprocess (pad/truncate + batch‐dim)\n",
    "    inputs = feature_extractor(\n",
    "        audio_sample,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",        # => PyTorch tensors\n",
    "        padding=True,               # pad to longest in batch\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def run_inference(data_row, model):\n",
    "    \"\"\"Return log probs and most likely phonemes.\"\"\"\n",
    "    inputs = get_audio_features(data_row)\n",
    "\n",
    "    # 4. Inference for phoneme recognition\n",
    "    with torch.no_grad():\n",
    "        # Get phoneme log probabilities\n",
    "        log_probs = model(inputs)\n",
    "\n",
    "        # Recognize phoneme sequence\n",
    "        phoneme_sequences = model.recognize(inputs)\n",
    "\n",
    "    return log_probs, phoneme_sequences\n",
    "\n",
    "log_probs, phoneme_sequences = run_inference(ds[0], phoneme_recognizer)\n",
    "\n",
    "# Print output\n",
    "print(\"Log probabilities shape:\", log_probs.shape)  # (batch_size, seq_len, num_phonemes)\n",
    "print(\"Recognized phoneme sequence:\", phoneme_sequences[0])\n",
    "print(\"Transcript for reference:\", ds[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5207f7",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load our data in a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c65d3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'phoneme_sequence'],\n",
      "    num_rows: 932\n",
      "})\n",
      "{'path': 'Hackathon_ASR/2_Audiofiles/Decoding_IT_T1/1001_edugame2023_59aa8ecf74c44db2adf56d71d1705cf5_1de23ac3deaf4b4d8c7db6d0cc9d6bfe.wav', 'array': array([0.        , 0.        , 0.        , ..., 0.00378418, 0.00424194,\n",
      "       0.        ], shape=(364544,)), 'sampling_rate': 16000}\n",
      "['vuzo[PAD]seɡa[PAD]klofɛno[PAD]raviʎo[PAD]da[PAD]pe[PAD]tarse[PAD]doridzːa[PAD]prateʎa[PAD]aː[PAD]ɛrɾe[PAD]lo[PAD]beɲole[PAD]fla[PAD]vɛstro[PAD]kʊɲaripːo']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, Features, Sequence, Value\n",
    "\n",
    "# 1. Location of your CSV\n",
    "# csv_file = \"train_phonemes_clean.csv\"  # replace with your path\n",
    "csv_file = \"ground_truth_it_coder_2.csv\"  # replace with your path\n",
    "\n",
    "\n",
    "# 2. Define initial features: audio paths as plain strings, phonemes as plain strings\n",
    "features = Features({\n",
    "    \"file_name\": Value(\"string\"),\n",
    "    \"phoneme_sequence\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "# 3. Load the CSV into a DatasetDict (default split is 'train')\n",
    "ds_dict = load_dataset(\"csv\", data_files=csv_file, features=features)\n",
    "dataset = ds_dict[\"train\"]\n",
    "\n",
    "# 4. Rename the audio-path column to 'audio' (required by Audio feature)\n",
    "dataset = dataset.rename_column(\"file_name\", \"audio\")\n",
    "\n",
    "# 5. Cast 'audio' to the Audio type (will load the file when you access it)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# 6. Map + split phoneme strings into lists\n",
    "def split_phonemes(example):\n",
    "    # assume phonemes are space-separated, e.g. \"AH0 T EH1 S T\"\n",
    "    example[\"phoneme_sequence\"] = example[\"phoneme_sequence\"].split()\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(split_phonemes)\n",
    "\n",
    "# 7. Cast the phoneme_sequence column to a Sequence of strings\n",
    "dataset = dataset.cast_column(\n",
    "    \"phoneme_sequence\",\n",
    "    Sequence(feature=Value(\"string\"))\n",
    ")\n",
    "\n",
    "# Now 'dataset' has:\n",
    "#   - dataset[i][\"audio\"] → { \"array\": np.ndarray, \"sampling_rate\": 16000 }\n",
    "#   - dataset[i][\"phoneme_sequence\"] → list of strings\n",
    "print(dataset)\n",
    "print(dataset[0][\"audio\"])\n",
    "print(dataset[0][\"phoneme_sequence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba490e59",
   "metadata": {},
   "source": [
    "Just a simple test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f02e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'same values'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import warnings\n",
    "\n",
    "wavfile, sampling_rate = torchaudio.load(\"Hackathon_ASR/2_Audiofiles/Decoding_IT_T1/1001_edugame2023_59aa8ecf74c44db2adf56d71d1705cf5_1de23ac3deaf4b4d8c7db6d0cc9d6bfe.wav\")\n",
    "if sampling_rate != 16000:\n",
    "    warnings.warn(f\"Sampling rate should be 16000 Hz, is {sampling_rate} Hz\")\n",
    "\n",
    "\"same values\" if torch.all(dataset[0][\"audio\"][\"array\"] == wavfile) else \"different data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf53be",
   "metadata": {},
   "source": [
    "## Putting stuff together\n",
    "\n",
    "Now we run the model on our in-house dataset. We also define a scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bb0ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = []\n",
    "losses = []\n",
    "scored_dataset = dataset.select(range(3))\n",
    "\n",
    "\n",
    "def smart_split_coder(sentence):\n",
    "    output = []\n",
    "    in_brackets = False\n",
    "    for char in sentence:\n",
    "        if in_brackets:\n",
    "            output[-1] += char\n",
    "        else:\n",
    "            output.append(char)\n",
    "\n",
    "        if char == '[':\n",
    "            in_brackets = True\n",
    "        elif char == ']':\n",
    "            in_brackets = False\n",
    "    return output\n",
    "\n",
    "\n",
    "for data in scored_dataset:\n",
    "    log_probs, phonemes = run_inference(data, phoneme_recognizer)\n",
    "    coding = smart_split_coder(data[\"phoneme_sequence\"][0])\n",
    "    loss = len(phonemes[0]) - np.sum([p in coding for p in phonemes[0]])\n",
    "    transcriptions.append(phonemes[0])\n",
    "    losses.append(loss)\n",
    "    \n",
    "scored_dataset = scored_dataset.add_column(\"transcription\", transcriptions).add_column(\"loss\", losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135be514",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ea706d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 204, 'transcription': ['ɐ', '[UNK]', 'k', 'w', 'ʊ', 'j', 'w', 'ʌ', 'j', 'ʌ', 'k', 'ʌ', 'k', '̃', 'w', 'k', '̃', 'ɪ', 'œ', 'w', 'd', 'œ', 'w', 'œ', 'w', 'ɪ', 'œ', '[UNK]', '(', 'ʊ', '(', 'l', '(', 'ʊ', 'e', 'w', 'ɑ', 'w', 'e', '̪', 's', 'ɪ', 'k', 'œ', 'k', 'w', 'œ', 'ŋ', 'ə', '̪', 'ʌ', 'ŋ', 'w', 'ŋ', 'w', 'j', 'œ', 'a', 'ː', '̪', 'b', 'l', 'ɑ', 'ː', 'a', 'd', 'a', 'd', 'ː', 'b', '̪', 'ʊ', 't', '[PAD]', 'z', '[PAD]', 'v', 'b', 'l', 'œ', 'o', 'ː', 'l', 'w', '[UNK]', 'n', 'ʁ', 'ɪ', 'ɛ', 'y', 'ŋ', 'ɑ', '<blank>', 'ɑ', 'ː', 'k', 'ɪ', '[PAD]', 't', 'k', 'ʁ', '[PAD]', 'n', 'ʁ', 'ŋ', '(', 'o', 'ʊ', 'l', 'ː', 'l', 'e', 'l', 'e', 'l', '̪', 'ɪ', 'ʊ', ')', 'w', '<blank>', '[PAD]', 'ɒ', 'ɛ', 'ŋ', 'œ', 'k', 'z', '[UNK]', 'œ', '[UNK]', 'z', '[UNK]', '[PAD]', 'ɛ', 'ː', '(', 'ʊ', 'l', 'ː', 'ɛ', 'l', 'ɪ', 'z', 'k', '[PAD]', 'ʃ', 'z', 'ɪ', 'œ', '[PAD]', 'e', 'j', 'd', '̃', 'b', 'ŋ', 'v', 'l', 'u', 'ʊ', 'ʃ', 's', 'o', 'l', 'z', 'ɡ', 'z', 'l', 's', 'ɐ', 'b', 'k', 'œ', 'ʃ', 'ɪ', 'œ', 'e', 'b', 'ɑ', 'v', 'œ', '̃', 'ɪ', 'l', 'e', 'd', 'ː', 'd', 'ː', 'e', 'ɔ', 'ɡ', 'd', 'ɑ', 'ː', 'ɾ', 'l', '̪', 'ŋ', 'y', '̪', 'ɑ', 'e', 'ɑ', 'z', 'ɑ', 'w', 's', 'ɐ', '̪', ')', 'k', 'ʊ', 'j', 'ʊ', 'ɪ', 'k', 'œ', '[PAD]', 'ʃ', 'ɪ', 'ɛ', 'ŋ', '[PAD]', 'ɑ', 't', 'z', '[UNK]', 'ø', 'l', 'œ', 'k', 'ɪ', 'v', 'ŋ', 'e', 'l', 'ʊ', 'l', '[UNK]', 'l', 'ɪ', '[UNK]', 'œ', 'j', 'u', 'k', '[PAD]', 'z', 'l', 'd', 'b', 'l', '[PAD]', 'o', 'k', 'z', 'ʊ', 't', '[UNK]', 't', '[UNK]', 't', '[UNK]', 't', '[UNK]', 'n', 'ʃ', '[PAD]', 'm', 'ŋ', 'l', 'ɪ', 'l', '[UNK]', 'l', 'ʌ', 'ɒ', 'ʌ', 'ɜ', 'l', 'a', '̃', 'ɑ', 'w', 'l', 'w', '[UNK]', 'l', 'ɒ', 'w', '[UNK]', 'ɛ', 'n', 'ʁ', 'ŋ', 'ɑ', 'o', 'l', 'ɛ', 'ɡ', 'l', 's', '[UNK]', 'ɪ', '[UNK]', 'z', 'k', 'œ', 'u', '[PAD]', '(', 't', '[UNK]', 'ʌ', '[UNK]', '[PAD]', 'œ', '[UNK]', 'z', 'n', 'ɛ', 'z', 'ɛ', '̃', 'ʎ', '̃', 'i', 'j', '̃', 'ɛ', 't', 'ɾ', 'ɪ', 'l', 'z', 'l', 'e', 'ɑ', 'w', 'ɡ', 'l', '[UNK]', '(', 's', 'w', '[UNK]', 's', 'ɪ', 'œ', 'w', 'd', 'a', 'ɛ', 'a', '[UNK]', 'ɛ', 'ː', 'l', 'w', 'ɔ', '[UNK]', 'n', 'z', 'œ', '[PAD]', 'e', 'ŋ', 'v', 'z', '[PAD]', '̪', 'b', 'œ', 'l', 'ɑ', 'ː', 'ɑ', 's', 'l', 'œ', 'ɛ', 'o', 'k', 'l', '[PAD]', 'ŋ', 'ː', '(', 'o', 'l', 'e', '̪', 's', '̪', 'l', 'ø', '̪', 'ŋ', '̪', 'w', 'k', '̪', 'ŋ', 'ɐ', 'ɔ', 'l', '[UNK]', 'ɑ', '[UNK]', '[PAD]', 'ɒ', '̃', 'ɑ', 'z', 'ɛ', '[UNK]', 't', 'ɾ', 'l', '[UNK]', 'ɛ', '[UNK]', 'ɛ', 'œ', 'l', '̪', 'ɛ', 'd', 'ɛ', 'd', 'ɛ', 'd', 'ɛ', 'd', 'ɛ', 'd', 'ɛ', 'ʊ']}\n",
      "{'loss': 247, 'transcription': ['ʃ', 'l', 'ʃ', 'b', 'ɡ', 'w', 'l', '[UNK]', 'k', '[UNK]', 'k', '[UNK]', 'm', 'k', 'e', 'k', 'e', '[UNK]', 'e', '[UNK]', 'w', '[UNK]', 'ɪ', 'e', 'l', '(', '[UNK]', 'k', 'œ', 'e', 'ʊ', 'w', 'a', 'z', 'ɪ', 'œ', ')', 'z', '[UNK]', 'œ', 'w', 'a', 'ɑ', 'z', 'ɒ', 'l', 'œ', 'l', 'œ', 'ː', 'ɐ', 'ɪ', 'l', '̪', 'ŋ', '̪', 'l', 'œ', '[UNK]', '[PAD]', 'k', 'z', 'k', 'œ', 'ɡ', 'ə', 'z', '[UNK]', 'j', ')', '[PAD]', 'a', '<blank>', 'œ', 'l', '[UNK]', 'œ', 'ɛ', 'n', 'z', 'œ', 'ɛ', 'ŋ', 'ɐ', 'ɑ', '̃', 'œ', '[PAD]', 'œ', 'r', 'k', 'œ', '[PAD]', 'œ', 'ː', 'w', 'ɪ', 'ŋ', 'ɔ', 'ŋ', '(', 'ʊ', 'l', 'e', 'l', 's', 'ɪ', ')', 'œ', 'k', '̪', 'k', '[UNK]', 'ː', 'b', 'ː', 'z', '[UNK]', 'ɪ', '[PAD]', 'k', 'œ', 'ː', 'w', 'l', 'ʌ', 'y', 'v', 'ː', '[UNK]', 'k', 'ɜ', 'ː', 'w', 'v', 'z', '[PAD]', 't', 'k', 'ʎ', 'i', 'n', 'ŋ', 'œ', 'ɛ', 'ŋ', 'ɐ', 'ʎ', 'ɑ', 'l', 'œ', 'ɪ', 'ɒ', 'e', '[UNK]', 'j', 'ɑ', 'ɛ', ')', '[UNK]', 'd', 'ɾ', 'l', 'ʊ', 'l', 'e', 'œ', 'ɪ', 'œ', 'e', 'ɛ', 'ɑ', 'ɛ', 'œ', 'z', 'œ', 'i', 'k', 'ʊ', 'ʌ', 'ʊ', 'l', 'k', 'œ', 'o', 'k', 'z', '[UNK]', '[PAD]', 'œ', '[PAD]', 'ɜ', '[PAD]', '̃', 'd', '̃', 'ɐ', 'ɑ', 't', 'ɛ', 'ŋ', '[UNK]', 'ɾ', 'l', 'e', 'ɑ', 'e', 'ɪ', 'ʊ', 'e', 'k', 'œ', '(', 'ə', 'ɛ', 'b', 'ɑ', 'l', 'v', 'l', 'r', 'l', 'œ', 'u', '(', 'ː', 'b', '[UNK]', 'l', 'ː', 'ɒ', 'ɛ', 'ː', 'ə', '̪', 'ɛ', 'ŋ', '[PAD]', 'ɑ', 't', '[UNK]', '[PAD]', 't', '[PAD]', 't', 'œ', 'k', 'ɪ', 'ɛ', 'ŋ', 'ɑ', 'l', 'z', 'l', 'ɑ', 'e', 'w', 'e', 'ɑ', 'e', 'ɡ', 'e', 's', 'ɪ', '̃', 'k', '(', 'k', 'ɜ', 'ø', 'k', 'ɪ', 'ɜ', 'e', 'b', 'ɑ', 'n', 'ɛ', 'ʌ', 'ɒ', '[UNK]', '̪', '[UNK]', '̪', 'z', '[UNK]', 'k', '[PAD]', 'k', 'ʃ', 'ɪ', 'œ', '̃', 'b', 'ŋ', 't', 'u', 'œ', 'u', 'ŋ', '(', '̪', 'ɜ', 'ʊ', 'ɜ', 'e', 'w', 'e', 'w', '̃', 'ɐ', '̃', 'w', '̃', 'ɔ', 'k', 'œ', 'ø', 'ɹ', 'ʃ', 'ɪ', '̪', 'ɐ', 'd', 'e', 'l', 'œ', 'l', 'k', 'œ', 'o', 'k', 'z', '̪', 'ʊ', 'l', 'k', 'œ', 'k', 'œ', 'ɜ', '[UNK]', 'ŋ', '̃', 'ɑ', 'v', 't', 'ɪ', 'ɾ', 'ɒ', 'l', 'ɡ', 'l', 'œ', 'o', 'k', 'u', 'b', 'f', 'd', 's', 'l', 'ʊ', 'l', 'e', 'ə', 'e', 'l', 'e', 'ɪ', 'ɑ', 'k', '[PAD]', 'u', 'ɐ', 'z', '[UNK]', '[PAD]', 'l', 'ʌ', 'l', '[UNK]', 'ɜ', 'u', 'ɪ', 'ɛ', 'ː', 'ŋ', '(', 'l', 'ʊ', 'l', 's', 'ʊ', 'z', 'œ', 'ɜ', 'z', 't', 'ɑ', '[UNK]', 'z', 'l', 'ɑ', 'j', '[UNK]', 'd', '[UNK]', 'a', 'd', 'ː', 'b', 'l', 'e', 'ŋ', 'ɑ', 'l', 'ɪ', 'd', 'œ', '[UNK]', 'd', '[UNK]', 'ɪ', 'd', '[UNK]', 'l', '[UNK]', 'k', 'œ', ')', '̪', 'ʊ', '[UNK]', 'ɛ', 'ŋ', 't', 'ʌ', '[UNK]', 'ɛ', '[UNK]', '[PAD]', 'e', 'ŋ', 'v', '[PAD]', 'ŋ', 'ɛ', 'b', 'œ', 'a', 'ː', 'a', 'ː', 'a', 'ː', 'a', 'l', 'œ', '(', 'o', 'k', 'l', 'ɪ', 'ɛ', 'ŋ', 'ɑ', '(', 'o', 'ɜ', 'l', 'e', 'ɡ', 'l', 's', 'ɪ', 'ɑ', 'ʊ', 'w', 'ɑ', 'w', '<blank>', 'ː', 'l', '̪', 'ŋ', 'f', 'ɑ', 'ʁ', 'a', 'b', 'k', 'l', 'œ', 'e', 'ʌ', 'b', 'y', 'l', 'œ', '[PAD]', 'œ', 'ː', 'w', 'ɛ', 'ɐ', 'ʎ', 'j', '̃', 't', 'ɡ', 'ʊ', 's', 'ɐ', 'ʊ', 'ɐ', 'ː', 'k', 'ɒ', 'ɛ', 'ɐ', 'd', 'ɛ', '(', 'ʊ', '[UNK]', 'ɪ', '[UNK]', 'ɪ', 'ʊ', 'œ', 'l', 'ɪ', '[PAD]', 'œ', 'k', 'z', 'œ', 'u', 'y', 'ʌ', '[UNK]', 'ɛ', '[UNK]', 'ɛ', '[UNK]', 'ʊ', 'ɛ', 't', '[UNK]', 'ɛ', 'ɪ', 'd', 'ʌ', 'w']}\n",
      "{'loss': 266, 'transcription': ['ɪ', 'l', '[UNK]', 'z', 'ʌ', 'ɡ', 'ʌ', 'ɡ', 'ʌ', 'ʊ', 'ɡ', 'd', 'ɡ', 'd', 'œ', 'd', 'ɛ', 'ɜ', 'ɛ', 'œ', '̃', '[UNK]', 'ɛ', 'œ', '[UNK]', 'ɛ', 'œ', 'l', '[UNK]', 'l', '[UNK]', 'ɛ', 'ɜ', 'ː', 'l', '[UNK]', 'l', 'œ', '(', 'k', '[UNK]', 'e', '[UNK]', 'e', '̪', 'e', 'ɪ', 'œ', '̪', 'w', 'ː', 'ɑ', 's', 'l', 'œ', 'ɛ', 'd', 'ɛ', 'd', 'ʌ', 'ɪ', 'ɛ', 'œ', 'ɛ', 'l', 'z', 'œ', 'e', 'ɑ', '[UNK]', 'ʊ', '[UNK]', 'ʊ', 'z', 'ʊ', '[UNK]', 's', 'ɪ', 'ʊ', 'w', 'ː', 'ɐ', 's', 'l', 'k', 'z', '[UNK]', 'l', 'ɪ', 'ɾ', 'u', 'ɐ', '̪', 'k', 'v', 'z', '[UNK]', '[PAD]', 'e', 'œ', 'e', 'b', 'ʎ', '[PAD]', 'œ', '(', 'ʃ', 'z', 'ɛ', 'ː', 't', 'b', 'f', 'ʊ', 'l', 'ɛ', 'l', 'ɪ', '[UNK]', 'ɑ', '[UNK]', 'l', '[UNK]', 'e', 's', 'ɪ', 'k', '[UNK]', 'ɑ', '[UNK]', '[PAD]', 'ɑ', '[PAD]', 'z', 't', 'ɪ', 'ɑ', 'ː', 'ɑ', 'ɒ', '[UNK]', 'ɔ', 'œ', 'u', 'ʃ', 'z', 'k', '[PAD]', 'l', 'v', 'ŋ', 'ə', 'l', 'ŋ', 'e', '̪', 'k', 'œ', 'l', 'w', 'y', 'l', 'ɜ', 'œ', 'u', '[PAD]', 'ɑ', 'l', 'ɒ', 'œ', 'ɹ', 'œ', 'l', 'd', 'ŋ', 'e', 'l', 'v', 'ɑ', '[UNK]', 's', 'ʌ', 't', 'ɪ', 'ɾ', 'l', 'k', 'ɡ', 'l', 'k', 'z', 'k', 'u', 'ɐ', 'ɪ', 'ŋ', 'ɑ', 'ʊ', 'ŋ', 'ɜ', 'e', 'ɪ', 'v', 'ɪ', 'ɒ', 'k', 'œ', 'ɜ', 'l', '[UNK]', 'b', 'ː', 'y', 'l', 'k', 'œ', 'w', 'l', '̪', 'ŋ', 'j', 'ɑ', 'a', '<blank>', '̪', 'l', 'œ', 'l', 'œ', 'e', 'ɛ', 'b', 'n', 'z', 'œ', 'm', 'v', 'ŋ', 'e', 'l', 'œ', 'l', 'ɑ', 'e', 'ɑ', 'e', 'ɑ', 'w', 'e', 'l', 's', 'ɪ', 'k', 'œ', 'ʃ', 'œ', 'l', 'j', 'd', '̃', 'b', '̃', 'ɑ', 'n', 't', '[UNK]', 'z', 'm', 'ŋ', 'ɑ', 'l', 'ʌ', 'œ', 'l', 'œ', 'l', 's', '[PAD]', 'b', 'ʃ', 'œ', 'ʊ', 'ɑ', '[UNK]', 'ɛ', 'ː', 'ɑ', 'ɛ', 'b', 'ɑ', 'l', 'l', 'e', 'l', 'e', 'ʊ', 'œ', 'a', 'ː', 'a', 'ɛ', 'ː', 'ɑ', 'œ', 'e', 'j', 'd', 'b', '̃', 'n', '̃', 'v', 'ɐ', 'l', 'r', 'w', 'ɐ', 'œ', 'l', 'e', 'ʊ', 'k', 'y', 'k', '̃', 'e', 'œ', 'j', 'k', 'e', 'k', 'y', '[UNK]', 'œ', 'ɑ', 'ɪ', 'ɑ', 't', '[PAD]', 'l', 'œ', 'k', 'œ', '[PAD]', 'ʃ', '[PAD]', 'ɪ', 'ɒ', 'ʎ', '̃', 't', '[UNK]', 'z', '[UNK]', 'z', '̪', '[PAD]', '̪', 't', 'ɒ', 'l', 'z', '[UNK]', 'œ', 'ɾ', 'd', 'ɹ', 'œ', 'v', 'e', 'ŋ', 'ɑ', 'l', 'ɛ', 'l', 'ɛ', 'e', 'd', 'ɛ', 'd', 'ɛ', 'd', 'ɛ', 'l', 'ɜ', 'œ', 'ø', 'k', 'ɪ', 'v', 'ŋ', 'b', 'd', 'ɑ', 'ː', 'ɑ', 'ɛ', 'ː', 'z', 'l', 'k', 'œ', 'o', 'k', 'z', 't', 'k', 'm', 'z', 'ŋ', 'm', 'v', 'ŋ', 'ɑ', 'e', 'l', 'œ', 'l', 'w', 'l', 'w', 'z', 's', 'z', 'ɪ', 'œ', '̪', 'd', 'ɑ', 'l', '[UNK]', 'ɛ', 'ɹ', 'ː', 'ʊ', 'v', 'l', 'z', '[UNK]', 't', '[UNK]', 't', '[UNK]', 'ɒ', 'j', 'w', '[PAD]', 'ŋ', 'd', 'e', 'ʊ', 'l', 'e', 's', 'ɪ', 'k', '[PAD]', 'ʃ', 'z', '[PAD]', 't', 'œ', 'ɐ', 'd', 'a', 'd', 'b', '[PAD]', '̪', '(', 't', 'r', 'l', 'ʊ', 'ɒ', 'l', 'ʌ', '[UNK]', 'ʌ', 'z', 'œ', '[UNK]', 'ɛ', 'd', 'u', 'ɪ', '̪', 'ŋ', 'ʊ', 'ɜ', 'l', 'e', 's', 'l', 'ɪ', 'l', '[UNK]', 'œ', 'ɜ', 'ː', 'k', 'l', '[PAD]', 'b', 'ɑ', 'v', 'l', 'œ', 'u', '̪', 'ʊ', 'ɔ', 'ɑ', 't', 'ɾ', 'l', 'œ', 'k', 'a', 'w', 'z', '[UNK]', 't', '[UNK]', 't', '[UNK]', '[PAD]', 'ɾ', '[PAD]', 'ʁ', 'ŋ', 'o', 'ʊ', 'e', 'l', 'e', 'ɑ', 'w', 'e', 's', 'k', 'œ', 'ø', 'w', 'k', 'l', 'b', 'l', 'd', 'ɑ', 'd', '[UNK]', 'ː', 'l', 'b', 'ɑ', 'l', 'ɛ', 'b', 'ɐ', 'ɑ', 'l', 'z', 'l', 'œ', '(', 'u', 't', 'ɡ', 'd', 'ɾ', 'l', 'ɛ', 'w', 'l', 'w', 'ɛ', 'd', 'ɛ', 'œ', 'ɛ', 'œ', 'ɛ', 'œ', '[UNK]', 'ɛ', '[UNK]', 'ɛ', '[UNK]', 'ɛ', 'd', 'ɛ', 'd', '̃', 'd', 'ɛ', 'd', 'ɛ', 'd', 'w']}\n"
     ]
    }
   ],
   "source": [
    "for data in scored_dataset.select_columns([\"loss\", \"transcription\"]):\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df87423",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we want a phoneme recognition.\n",
    "It means to train the last layer of the model to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b488c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4194483757019043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.888230800628662\n",
      "Epoch 0, Loss: 1.146170735359192\n",
      "Epoch 0, Loss: 6.249912738800049\n",
      "Epoch 0, Loss: 1.2091354131698608\n",
      "Epoch 0, Loss: 3.8059654235839844\n",
      "Epoch 0, Loss: 2.063833713531494\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "phoneme_recognizer.train()\n",
    "linear_optimizer = torch.optim.Adam(phoneme_recognizer.phoneme_classifier.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def calculate_ctc_loss(log_probs, target_sequence):\n",
    "    \"\"\"Calculates CTC loss.\"\"\"\n",
    "    # Create input_lengths and target_lengths tensors\n",
    "    input_lengths = torch.tensor([1])  # Batch size of 1\n",
    "    target_lengths = torch.tensor([1])  # Batch size of 1\n",
    "\n",
    "    # Calculate CTC loss\n",
    "    loss = F.ctc_loss(\n",
    "        log_probs,\n",
    "        target_sequence,\n",
    "        input_lengths=input_lengths,\n",
    "        target_lengths=target_lengths\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if f.endswith(\".pth\")]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(r\"(\\d+)\\.pth$\", x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(r\"(\\d+)\\.pth$\", checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            phoneme_recognizer.phoneme_classifier.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "prepare_folders()\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "\n",
    "# Freeze the wavlm model\n",
    "for param in phoneme_recognizer.wavlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(dataset.shuffle().select(range(50))):\n",
    "        inputs = get_audio_features(data)\n",
    "        log_probs = phoneme_recognizer(inputs)\n",
    "        split_phonemes = smart_split_coder(data[\"phoneme_sequence\"][0])\n",
    "        target = phoneme_recognizer.tokenize(split_phonemes)\n",
    "        loss = calculate_ctc_loss(log_probs[0], target.reshape([1, -1]))\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        write_to_csv(\n",
    "            [\n",
    "                increment, epoch, i, loss.item(),\n",
    "                \"\".join(phoneme_recognizer.classify_to_phonemes(log_probs)[0]),\n",
    "                \"\".join(split_phonemes)\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        increment += 1\n",
    "        torch.save(\n",
    "            phoneme_recognizer.phoneme_classifier.state_dict(),\n",
    "            f\"{MODEL_DIR}/phoneme_classifier_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d7f10",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "We have a model roughly trained for phonemes.\n",
    "We want a binary classification though.\n",
    "We won't do that for now as it would be an end-to-end pipeline, defeating the purpose of the created pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
