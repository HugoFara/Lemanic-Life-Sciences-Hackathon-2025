{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d917c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities shape: torch.Size([1, 292, 42])\n",
      "Recognized phoneme sequence: ['OW', 'M', 'OW', 'AH', 'IY', 'AW', 'NG', 'IY', 'SIL', 'DH', 'G', 'L', 'SH', 'P', 'M', 'V', 'SH', 'M', 'UH', 'SH', 'F', 'OW', 'T', 'K', 'S', 'M', 'V', 'F', 'EY', 'V', 'AY', 'OW', 'TH', 'SIL', 'M', 'CH', 'UW', 'B', 'NG', 'B', 'ZH', 'B', 'EY', 'ER', 'T', 'D', 'HH', 'IY', 'SIL', 'CH', 'G', 'F', 'D', 'B', 'P', 'CH', 'T', 'CH', 'K', 'G', 'SH', 'G', 'F', 'OW', 'ZH', 'M', 'V', 'D', 'T', 'G', 'NG', 'OY', 'K', 'IH', 'IY', 'K', 'G', 'R', 'Z', 'IY', 'ER', 'R', 'N', 'T', 'IH', 'OW', 'JH', 'NG', 'OY', 'SH', 'G', 'G', 'M', 'V', 'SH', 'P', 'G', 'D', 'F', 'T', 'OY', 'SH', 'NG', 'OW', 'B', 'ZH', 'EH', 'Y', 'SH', 'L', 'AY', 'G', 'IY', 'UW', 'SH', 'D', 'F', 'SP', 'IY', 'CH', 'F', 'OW', 'K', 'L', 'AY', 'SH', 'OW', 'R', 'AW', 'OW', 'CH', 'G', 'CH', 'T', 'D', 'IH', 'IY', 'SIL', 'AW', 'IH', 'OW', 'P', 'G', 'IY', 'Z', 'F', 'NG', 'DH', 'OW', 'Z', 'M', 'Z', 'V']\n",
      "Transcript for reference: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# PhonemeRecognizer: WavLM + CTC for phoneme speech recognition\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# Define a list of English phonemes (ARPABET format) + blank token for CTC\n",
    "PHONEMES = ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY',\n",
    "           'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P',\n",
    "           'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', 'SIL', 'SP']\n",
    "# Add blank token for CTC\n",
    "PHONEME_DICT = {p: i for i, p in enumerate(['<blank>'] + PHONEMES)}\n",
    "NUM_PHONEMES = len(PHONEME_DICT)\n",
    "\n",
    "class PhonemeRecognizer(nn.Module):\n",
    "    def __init__(self, wavlm_model, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "        self.wavlm = wavlm_model\n",
    "\n",
    "        # Get the hidden size from the WavLM model\n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(hidden_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get WavLM embeddings\n",
    "        outputs = self.wavlm(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def recognize(self, inputs, beam_width=100):\n",
    "        \"\"\"Perform phoneme recognition with beam search decoding\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get log probabilities\n",
    "            log_probs = self(inputs)\n",
    "\n",
    "            # Convert to CPU for decoding\n",
    "            log_probs_cpu = log_probs.cpu().detach().numpy()\n",
    "\n",
    "            # Simple greedy decoding (for demonstration)\n",
    "            # In a real system, you would use beam search with ctcdecode\n",
    "            predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "            phoneme_sequences = []\n",
    "            for pred_seq in predictions:\n",
    "                seq = []\n",
    "                prev = -1\n",
    "                for p in pred_seq:\n",
    "                    # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                    if p != 0 and p != prev:\n",
    "                        # Convert index back to phoneme\n",
    "                        phoneme = list(PHONEME_DICT.keys())[list(PHONEME_DICT.values()).index(p)]\n",
    "                        seq.append(phoneme)\n",
    "                    prev = p\n",
    "                phoneme_sequences.append(seq)\n",
    "\n",
    "            return phoneme_sequences\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# Method A: Using the PhonemeRecognizer for speech-to-phoneme ASR\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1. Load the feature extractor and model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "# Create the phoneme recognizer with the WavLM model\n",
    "phoneme_recognizer = PhonemeRecognizer(wavlm_model)\n",
    "phoneme_recognizer.eval()  # disable dropout, etc.\n",
    "\n",
    "# 2. Load an example audio file (here using a small demo from `datasets`)\n",
    "#    The `audio[\"array\"]` is a NumPy array of floats; sampling_rate is an int.\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "audio_sample = ds[0][\"audio\"][\"array\"]\n",
    "sr = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "# 3. Preprocess (pad/truncate + batch‐dim)\n",
    "inputs = feature_extractor(\n",
    "    audio_sample,\n",
    "    sampling_rate=sr,\n",
    "    return_tensors=\"pt\",        # => PyTorch tensors\n",
    "    padding=True,               # pad to longest in batch\n",
    ")\n",
    "\n",
    "# 4. Inference for phoneme recognition\n",
    "with torch.no_grad():\n",
    "    # Get phoneme log probabilities\n",
    "    log_probs = phoneme_recognizer(inputs)\n",
    "\n",
    "    # Recognize phoneme sequence\n",
    "    phoneme_sequences = phoneme_recognizer.recognize(inputs)\n",
    "\n",
    "# Print output\n",
    "print(\"Log probabilities shape:\", log_probs.shape)  # (batch_size, seq_len, num_phonemes)\n",
    "print(\"Recognized phoneme sequence:\", phoneme_sequences[0])\n",
    "print(\"Transcript for reference:\", ds[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65d3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'phoneme_sequence'],\n",
      "    num_rows: 932\n",
      "})\n",
      "{'path': 'Hackathon_ASR/2_Audiofiles/Decoding_IT_T1/1001_edugame2023_59aa8ecf74c44db2adf56d71d1705cf5_1de23ac3deaf4b4d8c7db6d0cc9d6bfe.wav', 'array': array([0.        , 0.        , 0.        , ..., 0.00378418, 0.00424194,\n",
      "       0.        ], shape=(364544,)), 'sampling_rate': 16000}\n",
      "['vuzo[PAD]seɡa[PAD]klofɛno[PAD]raviʎo[PAD]da[PAD]pe[PAD]tarse[PAD]doridzːa[PAD]prateʎa[PAD]aː[PAD]ɛrɾe[PAD]lo[PAD]beɲole[PAD]fla[PAD]vɛstro[PAD]kʊɲaripːo']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Audio, Features, Sequence, Value\n",
    "\n",
    "# 1. Location of your CSV\n",
    "# csv_file = \"train_phonemes_clean.csv\"  # replace with your path\n",
    "csv_file = \"ground_truth_it_coder_2.csv\"  # replace with your path\n",
    "\n",
    "\n",
    "# 2. Define initial features: audio paths as plain strings, phonemes as plain strings\n",
    "features = Features({\n",
    "    \"file_name\": Value(\"string\"),\n",
    "    \"phoneme_sequence\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "# 3. Load the CSV into a DatasetDict (default split is 'train')\n",
    "ds_dict = load_dataset(\"csv\", data_files=csv_file, features=features)\n",
    "dataset = ds_dict[\"train\"]\n",
    "\n",
    "# 4. Rename the audio-path column to 'audio' (required by Audio feature)\n",
    "dataset = dataset.rename_column(\"file_name\", \"audio\")\n",
    "\n",
    "# 5. Cast 'audio' to the Audio type (will load the file when you access it)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# 6. Map + split phoneme strings into lists\n",
    "def split_phonemes(example):\n",
    "    # assume phonemes are space-separated, e.g. \"AH0 T EH1 S T\"\n",
    "    example[\"phoneme_sequence\"] = example[\"phoneme_sequence\"].split()\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(split_phonemes)\n",
    "\n",
    "# 7. Cast the phoneme_sequence column to a Sequence of strings\n",
    "dataset = dataset.cast_column(\n",
    "    \"phoneme_sequence\",\n",
    "    Sequence(feature=Value(\"string\"))\n",
    ")\n",
    "\n",
    "# Now 'dataset' has:\n",
    "#   - dataset[i][\"audio\"] → { \"array\": np.ndarray, \"sampling_rate\": 16000 }\n",
    "#   - dataset[i][\"phoneme_sequence\"] → list of strings\n",
    "print(dataset)\n",
    "print(dataset[0][\"audio\"])\n",
    "print(dataset[0][\"phoneme_sequence\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f02e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364544,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "wavfile, sampling_rate = librosa.load(\"Hackathon_ASR/2_Audiofiles/Decoding_IT_T1/1001_edugame2023_59aa8ecf74c44db2adf56d71d1705cf5_1de23ac3deaf4b4d8c7db6d0cc9d6bfe.wav\", sr=16000)\n",
    "wavfile.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
