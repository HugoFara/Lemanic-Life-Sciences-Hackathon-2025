{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b6c7c4",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Let's create a model first, with some vocab.\n",
    "The output is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f115131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6829.36s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
      "Collecting jiwer\n",
      "  Using cached jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.12/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.venv/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting click>=8.1.8 (from jiwer)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Using cached jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [jiwer]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.1.8 jiwer-3.1.0 rapidfuzz-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa numpy soundfile torch torchaudio datasets evaluate transformers matplotlib jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a560f",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First, let's load our data in a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ded48737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset datasets/phonemized_fr_common\n",
      "Dataset({\n",
      "    features: ['audio', 'sentence', 'age', 'accent', 'target_phonemes1'],\n",
      "    num_rows: 8192\n",
      "})\n",
      "['ʒ', 'ə', 'n', 'ə', 's', 'ɛ', 'd', 'u', 's', 'a', 'm', 'a', 'ʒ', 'ɛ', 's', 't', 'e', 'v', 'ə', 'n', 'ɛ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import soundfile\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "import ipa_encoder\n",
    "from src.phonemizer import commonvoice, text_phonemizer\n",
    "\n",
    "LANGUAGE = (\"fr\", \"it\")[0]\n",
    "USE_IN_HOUSE = False\n",
    "DATASETS_DIR = \"datasets\"\n",
    "\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "# 1. Location of your CSV\n",
    "def get_in_house_dataset(language):\n",
    "    audio_files_path = \"Hackathon_ASR/2_Audiofiles/\" + {\n",
    "        \"fr\": \"Phoneme_Deletion_FR\",\n",
    "        \"it\": \"Decoding_IT\"\n",
    "    }[language] + \"_T1/\"\n",
    "\n",
    "    dataset_path = f\"datasets/phonemized_{language}.csv\"\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Regenerating IPA CSV file\")\n",
    "        ipa_encoder.regenerate_ipa_csv(language)\n",
    "\n",
    "\n",
    "    # 2. Define initial features: audio paths as plain strings, phonemes as plain strings\n",
    "    features = datasets.Features({\n",
    "        \"file_name\": datasets.Value(\"string\"),\n",
    "        \"phonemes_coder1\": datasets.Value(\"string\"),\n",
    "        \"phonemes_coder2\": datasets.Value(\"string\")\n",
    "    })\n",
    "\n",
    "    # 3. Load the CSV into a DatasetDict (default split is 'train')\n",
    "    dataset = datasets.load_dataset(\"csv\", data_files=dataset_path, features=features, split=\"train\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda data_row: {\"audio\": audio_files_path + data_row[\"file_name\"]},\n",
    "        desc=\"Select audio files path\"\n",
    "    )\n",
    "\n",
    "\n",
    "    print(dataset.num_rows, \"rows before filtering\")\n",
    "    dataset = dataset.filter(check_audios_valid, desc=\"Filtering out unreadable files\")\n",
    "    print(dataset.num_rows, \"rows after filtering\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        split_phonemes, \n",
    "        remove_columns=[\"phonemes_coder1\", \"phonemes_coder2\"],\n",
    "        desc=\"Phonemize data\"\n",
    "    )\n",
    "\n",
    "    # 7. Cast the phoneme_sequence column to a Sequence of strings\n",
    "    dataset = dataset.cast_column(\n",
    "        \"target_phonemes1\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    ).cast_column(\n",
    "        \"target_phonemes2\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_common_voice_phonemized_dataset(language, limit=-1):\n",
    "    \"\"\"Phonemized dataset from common voice.\"\"\"\n",
    "    dataset = commonvoice.get_phonemized_datasets([language], limit_items=limit)[language]\n",
    "    return dataset.map(\n",
    "        lambda x: {\"target_phonemes1\": x.split(\" \")},\n",
    "        input_columns=[\"target_phonemes1\"]\n",
    "    ).cast_column(\n",
    "        \"target_phonemes1\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    )\n",
    "\n",
    "\n",
    "# 6. Map + split phoneme strings into lists\n",
    "def split_in_bracket(string):\n",
    "    output = []\n",
    "    in_brackets = False\n",
    "    if string is None:\n",
    "        return output\n",
    "    for char in string:\n",
    "        if in_brackets:\n",
    "            output[-1] += char\n",
    "        else:\n",
    "            output.append(char)\n",
    "\n",
    "        if char == '[':\n",
    "            in_brackets = True\n",
    "        elif char == ']':\n",
    "            if output[-1] not in VOCAB:\n",
    "                print(f\"Removing {output.pop()}\")\n",
    "            in_brackets = False\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_phonemes(data_row):\n",
    "    \"\"\"Split each phoneme into a list.\"\"\"\n",
    "    data_row[\"target_phonemes1\"] = split_in_bracket(data_row[\"phonemes_coder1\"])\n",
    "    data_row[\"target_phonemes2\"] = split_in_bracket(data_row[\"phonemes_coder2\"])\n",
    "    return data_row\n",
    "\n",
    "def check_audios_valid(data_row):\n",
    "    \"\"\"Mark file invalid when it cannot be read.\"\"\"\n",
    "    if not os.path.exists(data_row[\"audio\"]):\n",
    "        return False\n",
    "    \n",
    "    with open(data_row[\"audio\"], 'rb') as file:\n",
    "        try:\n",
    "            soundfile.read(file)\n",
    "            return True\n",
    "        except soundfile.LibsndfileError:\n",
    "            return False\n",
    "    \n",
    "\n",
    "def create_phonemized_dataset(max_rows=-1):\n",
    "    clear_cache()\n",
    "    if USE_IN_HOUSE:\n",
    "        dataset = get_in_house_dataset(LANGUAGE)\n",
    "    else:\n",
    "        dataset = get_common_voice_phonemized_dataset(LANGUAGE, max_rows)\n",
    "\n",
    "    # 5. Cast 'audio' to the Audio type (will load the file when you access it)\n",
    "    dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_path = f\"{DATASETS_DIR}/phonemized_{LANGUAGE}{'' if USE_IN_HOUSE else '_common'}\"\n",
    "\n",
    "num_audios = 8192\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = datasets.load_from_disk(dataset_path)\n",
    "    if dataset.num_rows < num_audios:\n",
    "        dataset = create_phonemized_dataset(num_audios)\n",
    "        dataset.save_to_disk(dataset_path)\n",
    "        print(f\"Saved to {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"Using existing dataset {dataset_path}\")\n",
    "else:\n",
    "    dataset = create_phonemized_dataset(num_audios)\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "    print(f\"Saved to {dataset_path}\")\n",
    "\n",
    "# Regenerate the vocabulary if new phonemes\n",
    "coders = ((\"1\", \"2\") if USE_IN_HOUSE else (\"1\", ))\n",
    "phonemes = set()\n",
    "for coder in coders:\n",
    "    for phonemes_list in dataset[f\"target_phonemes{coder}\"]:\n",
    "        phonemes.update(phonemes_list)\n",
    "\n",
    "# IT + FR phonemes\n",
    "PHONEMES_DICT = text_phonemizer.check_regenerate_vocabulary(phonemes)\n",
    "VOCAB = tuple(PHONEMES_DICT.keys())\n",
    "\n",
    "# Now 'dataset' has:\n",
    "#   - dataset[i][\"audio\"] → { \"array\": np.ndarray, \"sampling_rate\": 16000 }\n",
    "#   - dataset[i][\"target_phonemes1\"] → list of strings\n",
    "print(dataset)\n",
    "print(dataset[0][\"target_phonemes1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe00d86",
   "metadata": {},
   "source": [
    "## Define a new linear layer\n",
    "\n",
    "As a speed-up, we simply create a linear layer to map from the extracted features to the phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d917c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size 769 Num phonemes 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities shape: (1, 154, 768)\n",
      "Recognized phoneme sequence: kœ̃ʼʼaũkũi̯ũuyʼ..ʼɥʼɥʼoʔʼoɔ̃ʼtʼoʼɥɔʼɥʼai̯ɑỹəũẽnvẽʼoʂt͡ʂɥỹ[UNK]nỹʼaøʼiʼũʏ̃kʼũʒnĩ̯ɔEɔEɑ̃ʼt͡ʂʐtEỹʔũnoʏ̃ʼɥũkʼnɑEʼooʼʼont͡ʂœ̃ʼɔəɔoʼʼoĩũỹiʼiũẽĩĩ̯ɛ̃ʐɛœ̃ʼœ̃ũkʂkũkũkʼokũi̯əʼʼo\n",
      "Transcript for reference: Je ne sais d'où Sa Majesté venait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# PhonemeRecognizer: WavLM + CTC for phoneme speech recognition\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "NUM_PHONEMES = len(VOCAB)\n",
    "\n",
    "class PhonemeMapper(nn.Module):\n",
    "    def __init__(self, features_size, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(features_size, num_phonemes)\n",
    "\n",
    "    def language_classifer(self, language):\n",
    "        \"\"\"Return a float identifying each known language.\"\"\"\n",
    "        if language == \"fr\":\n",
    "            return 0\n",
    "        if language == \"it\":\n",
    "            return 1\n",
    "        return 0.5\n",
    "\n",
    "    def forward(self, input_values, language):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        :param torch.Tensor input_values: Extracted features batch to map.\n",
    "        :param str | Iterable language: Language for the batch, or the language for each element in the batch.\n",
    "        :return torch.Tensor: Log of the probabilities for each phoneme.\n",
    "        \"\"\"\n",
    "        input_batch = torch.empty(\n",
    "            (input_values.shape[0], input_values.shape[1], input_values.shape[2] + 1),\n",
    "            dtype=input_values.dtype, device=input_values.device\n",
    "        )\n",
    "\n",
    "        if isinstance(language, str):\n",
    "            lang_val = [self.language_classifer(language) for _ in range(input_values.shape[1])]\n",
    "        else:\n",
    "            lang_val = [self.language_classifer(lang) for lang in language]\n",
    "\n",
    "        input_batch[:, :, 0] = lang_val\n",
    "        input_batch[:, :, 1:] = input_values\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(input_batch)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def tokenize(self, char_list, lenient=False):\n",
    "        \"\"\"\n",
    "        Go from a list of characters to a list of indices.\n",
    "        \n",
    "        :param list[str] char_list: Characters top be mapped.\n",
    "        :param bool lenient: If True, characters not in vocab are mapped to [UNK] \n",
    "        \"\"\"\n",
    "        if not lenient:\n",
    "            return torch.tensor([PHONEMES_DICT[x] for x in char_list])\n",
    "        \n",
    "        return torch.tensor([PHONEMES_DICT[x] if x in PHONEMES_DICT else PHONEMES_DICT[\"[UNK]\"] for x in char_list])\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    seq.append(VOCAB[p])\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_audios(batch):\n",
    "    \"\"\"Preprocess the audio files (pad/truncate + batch-dim).\"\"\"\n",
    "    for data_row in batch:\n",
    "        if data_row[\"sampling_rate\"] != 16000:\n",
    "            raise NotImplementedError(\n",
    "                f\"No sampling rate can be different from 16000, is {data_row[\"sampling_rate\"]}\"\n",
    "            )\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        [data_row[\"array\"] for data_row in batch],\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,       # pad to longest in batch\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def run_inference(batch, model, language=\"fr\"):\n",
    "    \"\"\"Return log probs and most likely phonemes.\"\"\"\n",
    "    inputs = preprocess_audios(batch[\"audio\"])\n",
    "\n",
    "    # 4. Inference for phoneme recognition\n",
    "    with torch.no_grad():\n",
    "        # Get phoneme log probabilities\n",
    "        log_probs = model(**inputs).last_hidden_state\n",
    "\n",
    "        # Recognize phoneme sequence\n",
    "        phonemes_sequences = linear_mapper.classify_to_phonemes(linear_mapper(log_probs, language))\n",
    "\n",
    "    return {\"log_probs\": log_probs, \"phonemes\": phonemes_sequences}\n",
    "\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# Method A: Using the PhonemeRecognizer for speech-to-phoneme ASR\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1. Load the feature extractor and model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "linear_mapper = PhonemeMapper(wavlm_model.config.hidden_size + 1, NUM_PHONEMES)\n",
    "\n",
    "print(\"Input size\", wavlm_model.config.hidden_size + 1, \"Num phonemes\", NUM_PHONEMES)\n",
    "\n",
    "# Create the phoneme recognizer with the WavLM model\n",
    "\n",
    "# 2. Load an example audio file\n",
    "audio_sample = dataset[0][\"audio\"][\"array\"]\n",
    "sr = dataset[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "\n",
    "predicted = dataset.select(range(1)).map(\n",
    "    lambda data_row: run_inference(data_row, wavlm_model, \"fr\"),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Print output\n",
    "print(\"Log probabilities shape:\", np.shape(predicted[\"log_probs\"]))  # (batch_size, seq_len, num_phonemes)\n",
    "print(\"Recognized phoneme sequence:\", \"\".join(predicted[\"phonemes\"][0]))\n",
    "print(\"Transcript for reference:\", dataset[0][\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf53be",
   "metadata": {},
   "source": [
    "## Putting stuff together\n",
    "\n",
    "Now we run the model on our in-house dataset.\n",
    "We will extract the features so that it is easier to work with latter on.\n",
    "For this version we don't train the model, only a fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bb0ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset datasets/features_fr_common\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'age', 'accent', 'target_phonemes1', 'features'],\n",
       "    num_rows: 8192\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def audio_processor(batch):\n",
    "    preprocessed = preprocess_audios(batch)\n",
    "\n",
    "    return {\n",
    "        \"features\": wavlm_model(\n",
    "            input_values=preprocessed[\"input_values\"].to(device),\n",
    "            attention_mask=preprocessed[\"attention_mask\"].to(device)\n",
    "        ).last_hidden_state\n",
    "    }\n",
    "\n",
    "\n",
    "def regenerate_audio_features(dataset):\n",
    "    wavlm_model.eval()\n",
    "    wavlm_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_dataset = (\n",
    "            dataset\n",
    "            .map(\n",
    "                audio_processor,\n",
    "                batched=True,\n",
    "                input_columns=[\"audio\"],\n",
    "                remove_columns=[\"audio\"],\n",
    "                batch_size=30 if LANGUAGE == \"fr\" else 15,\n",
    "                desc=\"Extracting audio features\"\n",
    "            )\n",
    "            .with_format(\"torch\")\n",
    "        )\n",
    "    return features_dataset\n",
    "\n",
    "\n",
    "dataset_path = f\"{DATASETS_DIR}/features_{LANGUAGE}{'' if USE_IN_HOUSE else '_common'}\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    features_dataset = datasets.load_from_disk(dataset_path)\n",
    "    if features_dataset.num_rows < num_audios:\n",
    "        features_dataset = regenerate_audio_features(dataset)\n",
    "        features_dataset.save_to_disk(dataset_path)\n",
    "        print(f\"Saved to {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"Using existing dataset {dataset_path}\")\n",
    "else:\n",
    "    features_dataset = regenerate_audio_features(dataset)\n",
    "    features_dataset.save_to_disk(dataset_path)\n",
    "    print(f\"Saved to {dataset_path}\")\n",
    "\n",
    "features_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc4765",
   "metadata": {},
   "source": [
    "## Transfer learning (Linear model training)\n",
    "\n",
    "Now that the linear layer is ready, we can simply train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bfb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 18/128 [00:03<00:19,  5.62it/s, Epoch=0, Loss=1.64]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i + batch_size > features_dataset.num_rows:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m batch_data = \u001b[43mfeatures_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     73\u001b[39m input_lengths = torch.empty(batch_size, dtype=torch.uint32, device=device)\n\u001b[32m     74\u001b[39m max_len = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, batch_data[\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:410\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:119\u001b[39m, in \u001b[36mTorchFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    117\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.numpy_arrow_extractor().extract_batch(pa_table)\n\u001b[32m    118\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    121\u001b[39m     batch[column_name] = \u001b[38;5;28mself\u001b[39m._consolidate(batch[column_name])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:102\u001b[39m, in \u001b[36mTorchFormatter.recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:522\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    519\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    520\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m    521\u001b[39m mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\n\u001b[32m    524\u001b[39m ]\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    526\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:383\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    381\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function([data_struct])[\u001b[32m0\u001b[39m]\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    385\u001b[39m     batched\n\u001b[32m    386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    387\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    388\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    389\u001b[39m ):\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:96\u001b[39m, in \u001b[36mTorchFormatter._recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np.ndarray):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_struct.dtype == \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:102\u001b[39m, in \u001b[36mTorchFormatter.recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:494\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    493\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    496\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:96\u001b[39m, in \u001b[36mTorchFormatter._recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np.ndarray):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_struct.dtype == \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir, base_name=\"\"):\n",
    "    increment = -1\n",
    "    name_format = fr\"{base_name}_.*(\\d+)\\.pth$\"\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if re.search(name_format, f)]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(name_format, x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(name_format, checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            linear_mapper.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "prepare_folders()\n",
    "clear_cache()\n",
    "\n",
    "linear_mapper.to(device).train()\n",
    "linear_optimizer = torch.optim.Adam(linear_mapper.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "batch_size = 64 if LANGUAGE == \"fr\" else 16\n",
    "base_name = \"linear_mapper\"\n",
    "increment = load_last_checkpoint(MODEL_DIR, base_name)\n",
    "coders = ((\"1\", \"2\") if USE_IN_HOUSE else (\"1\", ))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(600):\n",
    "    features_dataset = features_dataset.shuffle()\n",
    "    progress_bar = tqdm.trange(0, features_dataset.num_rows, batch_size)\n",
    "    for i in progress_bar:\n",
    "        # Skip the incomplete batches as there would change all sizes\n",
    "        if i + batch_size > features_dataset.num_rows:\n",
    "            continue\n",
    "        batch_data = features_dataset[i:i + batch_size]\n",
    "\n",
    "        input_lengths = torch.empty(batch_size, dtype=torch.uint32, device=device)\n",
    "        max_len = max(map(len, batch_data[\"features\"]))\n",
    "\n",
    "        input_batch = torch.zeros((batch_size, max_len, wavlm_model.config.hidden_size), device=device)\n",
    "        \n",
    "        for j, feat in enumerate(batch_data[\"features\"]):\n",
    "            input_batch[j, :feat.shape[0]] = feat\n",
    "            input_lengths[j] = feat.shape[0]\n",
    "\n",
    "        log_probs = linear_mapper(input_batch, LANGUAGE)\n",
    "        losses = []\n",
    "\n",
    "        for coder in coders:\n",
    "            targets = [linear_mapper.tokenize(string, lenient=True) for string in batch_data[f\"target_phonemes{coder}\"]]\n",
    "            target_lengths = torch.empty(batch_size, dtype=torch.uint8, device=device)\n",
    "            max_len = max(map(lambda x: x.shape[0], targets))\n",
    "\n",
    "            target_batch = torch.zeros((batch_size, max_len), device=device)\n",
    "            for j, target in enumerate(targets):\n",
    "                target_batch[j, :target.shape[0]] = target\n",
    "                target_lengths[j] = target.shape[0]\n",
    "\n",
    "            input_lengths = torch.tensor([x.shape[0] for x in log_probs], device=device)\n",
    "            final_probs = log_probs.transpose(0, 1)\n",
    "\n",
    "            losses.append(F.ctc_loss(\n",
    "                final_probs,\n",
    "                target_batch,\n",
    "                input_lengths=input_lengths,\n",
    "                target_lengths=target_lengths\n",
    "            ))\n",
    "\n",
    "        if len(coders) == 1:\n",
    "            loss = losses[0]\n",
    "        else:\n",
    "            loss = (losses[0] + losses[1]) / 2\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        if len(coders) == 1:\n",
    "            for logs, target_phons1 in zip(linear_mapper.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"]):\n",
    "                write_to_csv(\n",
    "                    [\n",
    "                        increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1)\n",
    "                    ]\n",
    "                )\n",
    "        else:\n",
    "            for logs, target_phons1, target_phons2 in zip(linear_mapper.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"], batch_data[\"target_phonemes2\"]):\n",
    "                write_to_csv(\n",
    "                    [\n",
    "                        increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1), \"\".join(target_phons2)\n",
    "                    ]\n",
    "                )\n",
    "        progress_bar.set_postfix({\"Epoch\": epoch, \"Loss\": loss.item()})\n",
    "        increment += 1\n",
    "    torch.save(\n",
    "        linear_mapper.state_dict(),\n",
    "        f\"{MODEL_DIR}/{base_name}_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e2537",
   "metadata": {},
   "source": [
    "## View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcce0445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "\n",
    "with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        losses.append(float(row[3]))\n",
    "\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "fig.suptitle(\"Model loss during training\")\n",
    "axes[0].plot(losses, label=\"Loss over training step\", linestyle='dotted')\n",
    "axes[0].grid()\n",
    "axes[0].set_xlabel(\"Training step\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "axes[1].plot(losses, label=\"Loss over training step\", linestyle='dotted')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid()\n",
    "axes[1].set_xlabel(\"Training step\")\n",
    "axes[1].set_ylabel(\"Loss (log scale)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32e8f1",
   "metadata": {},
   "source": [
    "## Performances analysis\n",
    "\n",
    "Check how good is the linear classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17872d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 8192/8192 [00:28<00:00, 285.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme error rate: 44.02635551560558%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "linear_mapper.eval()\n",
    "\n",
    "features_dataset = features_dataset.map(\n",
    "    lambda data_row: {\n",
    "        \"reference\": \"\".join(data_row[\"target_phonemes1\"]),\n",
    "        \"prediction\": \"\".join(linear_mapper.classify_to_phonemes(linear_mapper(\n",
    "            data_row[\"features\"].unsqueeze(0).to(device), \n",
    "            LANGUAGE\n",
    "        ))[0])\n",
    "    },\n",
    "    desc=\"Generating predictions\"\n",
    ")\n",
    "\n",
    "cer = cer_metric.compute(\n",
    "    references=features_dataset[\"reference\"], predictions=features_dataset[\"prediction\"]\n",
    ")\n",
    "print(f\"Phoneme error rate: {cer * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df87423",
   "metadata": {},
   "source": [
    "## Model fine-tuning\n",
    "\n",
    "Now we want to increase performances.\n",
    "The It means fine-tuning the full model (WavLM + LinearMapper) that was already pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b488c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133125/2566585548.py:27: UserWarning: No .pth files found in the model directory! Starting from scratch!\n",
      "  warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
      "  0%|          | 0/2048 [00:00<?, ?it/s]/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2048 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.67 GiB of which 47.88 MiB is free. Process 2189 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 14.20 GiB memory in use. Of the allocated memory 13.87 GiB is allocated by PyTorch, and 28.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m inputs[\u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m] = LANGUAGE\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# log_probs = phoneme_recognizer(**inputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m log_probs = \u001b[43mphoneme_recognizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLANGUAGE\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m max_len = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, batch_data[\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    117\u001b[39m input_batch = torch.zeros((batch_size, max_len, wavlm_model.config.hidden_size), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mPhonemeRecognizer.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, language)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_values, attention_mask, language):\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Get WavLM embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwavlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Apply dropout\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:1184\u001b[39m, in \u001b[36mWavLMModel.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1179\u001b[39m hidden_states, extract_features = \u001b[38;5;28mself\u001b[39m.feature_projection(extract_features)\n\u001b[32m   1180\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m._mask_hidden_states(\n\u001b[32m   1181\u001b[39m     hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n\u001b[32m   1182\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1192\u001b[39m hidden_states = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:440\u001b[39m, in \u001b[36mWavLMEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    432\u001b[39m         layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    433\u001b[39m             layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    434\u001b[39m             hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    437\u001b[39m             output_attentions,\n\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m         layer_outputs = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     hidden_states, position_bias = layer_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:325\u001b[39m, in \u001b[36mWavLMEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, position_bias=\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m, index=\u001b[32m0\u001b[39m):\n\u001b[32m    324\u001b[39m     attn_residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     hidden_states, attn_weights, position_bias = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    333\u001b[39m     hidden_states = attn_residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:191\u001b[39m, in \u001b[36mWavLMAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[39m\n\u001b[32m    188\u001b[39m gated_position_bias = gate_output.view(bsz * \u001b[38;5;28mself\u001b[39m.num_heads, -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) * position_bias\n\u001b[32m    189\u001b[39m gated_position_bias = gated_position_bias.view((-\u001b[32m1\u001b[39m, tgt_len, tgt_len))\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m attn_output, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_multi_head_self_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgated_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights, position_bias\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:215\u001b[39m, in \u001b[36mWavLMAttention.torch_multi_head_self_attention\u001b[39m\u001b[34m(self, hidden_states, attention_mask, gated_position_bias, output_attentions)\u001b[39m\n\u001b[32m    211\u001b[39m add_zero_attn = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# PyTorch 1.3.0 has F.multi_head_attention_forward defined\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# so no problem with backwards compatibility\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m attn_output, attn_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgated_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_separate_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# [Seq_Len, Batch Size, ...] -> [Batch Size, Seq_Len, ...]\u001b[39;00m\n\u001b[32m    240\u001b[39m attn_output = attn_output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:6410\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6407\u001b[39m k = k.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m   6408\u001b[39m v = v.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m-> \u001b[39m\u001b[32m6410\u001b[39m attn_output = \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[32m   6412\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6413\u001b[39m attn_output = (\n\u001b[32m   6414\u001b[39m     attn_output.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).contiguous().view(bsz * tgt_len, embed_dim)\n\u001b[32m   6415\u001b[39m )\n\u001b[32m   6417\u001b[39m attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.67 GiB of which 47.88 MiB is free. Process 2189 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 14.20 GiB memory in use. Of the allocated memory 13.87 GiB is allocated by PyTorch, and 28.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "class PhonemeRecognizer(nn.Module):\n",
    "    def __init__(self, wavlm_model, linear_mapper_model):\n",
    "        \"\"\"\n",
    "        Create the new model out of a combination of both models.\n",
    "        \n",
    "        :param wavlm_model: Features extraction performing speech-to-features.\n",
    "        :param linear_mapper_model: Second model to perform features-to-phonemes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.wavlm = wavlm_model\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes\n",
    "        self.phoneme_classifier = linear_mapper_model\n",
    "\n",
    "    def forward(self, input_values, attention_mask, language):\n",
    "        # Get WavLM embeddings\n",
    "        outputs = self.wavlm(input_values=input_values, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        log_probs = self.phoneme_classifier(hidden_states, language)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        return self.phoneme_classifier.classify_to_phonemes(log_probs)\n",
    "\n",
    "\n",
    "    def recognize(self, inputs):\n",
    "        \"\"\"Perform phoneme recognition.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get log probabilities\n",
    "            log_probs = self(inputs)\n",
    "\n",
    "            return self.classify_to_phonemes(log_probs)\n",
    "\n",
    "    def tokenize(self, char_list, lenient=False):\n",
    "        \"\"\"\n",
    "        Go from a list of characters to a list of indices.\n",
    "        \n",
    "        :param list[str] char_list: Characters top be mapped.\n",
    "        :param bool lenient: If True, characters not in vocab are mapped to [UNK] \n",
    "        \"\"\"\n",
    "        return self.phoneme_classifier.tokenize(char_list, lenient)\n",
    "    \n",
    "    def get_embedding(self, char_list):\n",
    "        tokens = self.tokenize(char_list)\n",
    "        out_tensor = torch.zeros((len(tokens), len(VOCAB)))\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            out_tensor[i, token_id] = 1\n",
    "        return out_tensor\n",
    "\n",
    "phoneme_recognizer = PhonemeRecognizer(wavlm_model, linear_mapper)\n",
    "phoneme_recognizer.to(device).train()\n",
    "\n",
    "recognizer_optimizer = torch.optim.Adam(\n",
    "    phoneme_recognizer.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "\n",
    "prepare_folders()\n",
    "base_name = \"phoneme_recognizer\"\n",
    "increment = load_last_checkpoint(MODEL_DIR, base_name)\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training_complete.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "batch_size = 4 if LANGUAGE == \"fr\" else 16\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "coders = ((\"1\", \"2\") if USE_IN_HOUSE else (\"1\", ))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(600):\n",
    "    dataset = dataset.shuffle()\n",
    "    progress_bar = tqdm.trange(0, dataset.num_rows, batch_size)\n",
    "    for i in progress_bar:\n",
    "        # Skip the incomplete batches as there would change all sizes\n",
    "        if i + batch_size > dataset.num_rows:\n",
    "            continue\n",
    "        batch_data = dataset[i:i + batch_size]\n",
    "\n",
    "        input_lengths = torch.empty(batch_size, dtype=torch.uint32, device=device)\n",
    "\n",
    "        inputs = preprocess_audios(batch_data[\"audio\"])\n",
    "        inputs[\"language\"] = LANGUAGE\n",
    "\n",
    "        log_probs = phoneme_recognizer(\n",
    "            inputs[\"input_values\"].to(device),\n",
    "            inputs[\"attention_mask\"].to(device),\n",
    "            LANGUAGE\n",
    "        )\n",
    "\n",
    "        max_len = max(map(len, batch_data[\"features\"]))\n",
    "\n",
    "        input_batch = torch.zeros((batch_size, max_len, wavlm_model.config.hidden_size), device=device)\n",
    "        \n",
    "        for j, feat in enumerate(batch_data[\"features\"]):\n",
    "            input_batch[j, :feat.shape[0]] = feat\n",
    "            input_lengths[j] = feat.shape[0]\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for coder in coders:\n",
    "            targets = [\n",
    "                phoneme_recognizer.tokenize(string, lenient=True)\n",
    "                for string in batch_data[f\"target_phonemes{coder}\"]\n",
    "            ]\n",
    "            target_lengths = torch.empty(batch_size, dtype=torch.uint8, device=device)\n",
    "            max_len = max(map(lambda x: x.shape[0], targets))\n",
    "\n",
    "            target_batch = torch.zeros((batch_size, max_len), device=device)\n",
    "            for j, target in enumerate(targets):\n",
    "                target_batch[j, :target.shape[0]] = target\n",
    "                target_lengths[j] = target.shape[0]\n",
    "\n",
    "            final_probs = log_probs.transpose(0, 1)\n",
    "\n",
    "            losses.append(F.ctc_loss(\n",
    "                final_probs,\n",
    "                target_batch,\n",
    "                input_lengths=input_lengths,\n",
    "                target_lengths=target_lengths\n",
    "            ))\n",
    "            if losses[-1].item() > 100:\n",
    "                print(\"Loss explosion!\")\n",
    "\n",
    "        if len(coders) == 1:\n",
    "            loss = losses[0]\n",
    "        else:\n",
    "            loss = (losses[0] + losses[1]) / 2\n",
    "        recognizer_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        recognizer_optimizer.step()\n",
    "        if len(coders) == 1:\n",
    "            for logs, target_phons1 in zip(phoneme_recognizer.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"]):\n",
    "                write_to_csv(\n",
    "                    [\n",
    "                        increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1)\n",
    "                    ]\n",
    "                )\n",
    "        else:\n",
    "            for logs, target_phons1, target_phons2 in zip(phoneme_recognizer.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"], batch_data[\"target_phonemes2\"]):\n",
    "                write_to_csv(\n",
    "                    [\n",
    "                        increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1), \"\".join(target_phons2)\n",
    "                    ]\n",
    "                )\n",
    "        progress_bar.set_postfix({\"Epoch\": epoch, \"Loss\": loss.item()})\n",
    "        increment += 1\n",
    "    torch.save(\n",
    "        linear_mapper.state_dict(),\n",
    "        f\"{MODEL_DIR}/{base_name}_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d7f10",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "We have a model roughly trained for phonemes.\n",
    "We want a binary classification though.\n",
    "\n",
    "Two strategies:\n",
    "\n",
    "1. We consider the model reliable on phonemes, the task is a string search. We don't need AI.\n",
    "2. We call for hidden complexity, ans we use some NLP for the classification.\n",
    "\n",
    "Once we have everything, we could build an end-to-end pipeline, but we won't do it for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
