{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b6c7c4",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Let's create a model first, with some vocab.\n",
    "The output is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f115131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.12/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.venv/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa numpy soundfile torch torchaudio datasets transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d917c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x787551babb00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  2.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities shape: (1, 292, 50)\n",
      "Recognized phoneme sequence: bɾɪɾɡlɾl)kɪ̃ɡ)̪bob<blank>ɾ)ɡbʊvœl̪ʁlyɾklkõdɾ̃vyɹ)ɡʁɔnzpɾbodɡvɑkʁɪ̃ɡvwʁoɡorb<blank>ː)̪ɪlpnʊɑɾoktʊi)k<blank>̃)̪k̃kəɒvəɲwɑpʌbpɡʒɑʒœl)wɜ̃kɑvœ<blank>ʒɑəkɪ̃kɜɡnonɾ̃)vːpɔ)ɡvbopɾʒɾɡl)ɪuɾ\n",
      "Transcript for reference: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# PhonemeRecognizer: WavLM + CTC for phoneme speech recognition\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "LANGUAGE = (\"fr\", \"it\")[1]\n",
    "\n",
    "# IT + FR phonemes + blank\n",
    "VOCAB = (\n",
    "    \"[PAD]\", \"[UNK]\",\n",
    "    \"ʒ\",\"ɹ\",\"j\",\"d\",\"ɲ\",\"ʌ\",\"ɒ\",\"ɐ\",\"ʃ\",\"ɔ\",\"f\",\"ø\",\"z\",\"ŋ\",\"i\",\"u\",\"̃\",\"o\",\"œ\",\"a\",\"(\",\"ə\",\"ɜ\",\n",
    "    \"ɾ\",\"ː\",\"̪\",\"e\",\"b\",\"ʁ\",\"w\",\"n\",\"p\",\"y\",\"ɡ\",\"ɪ\",\"r\",\"v\",\"t\",\")\",\"m\",\"k\",\"ʊ\",\"ʎ\",\"ɑ\",\"s\",\"l\",\"ɛ\",\n",
    "    '<blank>'\n",
    ")\n",
    "PHONEME_DICT = {v: i for i, v in enumerate(VOCAB)}\n",
    "\n",
    "NUM_PHONEMES = len(VOCAB)\n",
    "\n",
    "class PhonemeRecognizer(nn.Module):\n",
    "    def __init__(self, wavlm_model, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "        self.wavlm = wavlm_model\n",
    "\n",
    "        # Get the hidden size from the WavLM model\n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank), adds a new input for language\n",
    "        self.phoneme_classifier = nn.Linear(1 + hidden_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get WavLM embeddings\n",
    "        outputs = self.wavlm(input_values=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(torch.dstack([hidden_states, torch.ones(hidden_states.shape[1]) * inputs[\"language\"]]))\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    seq.append(VOCAB[p])\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "\n",
    "\n",
    "    def recognize(self, inputs):\n",
    "        \"\"\"Perform phoneme recognition.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get log probabilities\n",
    "            log_probs = self(inputs)\n",
    "\n",
    "            return self.classify_to_phonemes(log_probs)\n",
    "\n",
    "    def tokenize(self, char_list, lenient=False):\n",
    "        \"\"\"\n",
    "        Go from a list of characters to a list of indices.\n",
    "        \n",
    "        :param list[str] char_list: Characters top be mapped.\n",
    "        :param bool lenient: If True, characters not in vocab are mapped to [UNK] \n",
    "        \"\"\"\n",
    "        if not lenient:\n",
    "            return torch.tensor([PHONEME_DICT[x] for x in char_list])\n",
    "        \n",
    "        return torch.tensor([PHONEME_DICT[x] if x in PHONEME_DICT else PHONEME_DICT[\"[UNK]\"] for x in char_list])\n",
    "    \n",
    "    def get_embedding(self, char_list):\n",
    "        tokens = self.tokenize(char_list)\n",
    "        out_tensor = torch.zeros((len(tokens), len(VOCAB)))\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            out_tensor[i, token_id] = 1\n",
    "        return out_tensor\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# Method A: Using the PhonemeRecognizer for speech-to-phoneme ASR\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1. Load the feature extractor and model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "\n",
    "def preprocess_audios(batch):\n",
    "    \"\"\"Preprocess the audio files (pad/truncate + batch-dim).\"\"\"\n",
    "    for data_row in batch:\n",
    "        if data_row[\"sampling_rate\"] != 16000:\n",
    "            raise NotImplementedError(\n",
    "                f\"No sampling rate can be different from 16000, is {data_row[\"sampling_rate\"]}\"\n",
    "            )\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        [data_row[\"array\"] for data_row in batch],\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,       # pad to longest in batch\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def run_inference(batch, model, language=\"fr\"):\n",
    "    \"\"\"Return log probs and most likely phonemes.\"\"\"\n",
    "    inputs = preprocess_audios(batch[\"audio\"])\n",
    "    inputs[\"language\"] = {\"fr\": 0, \"it\": 1}[language]\n",
    "\n",
    "    # 4. Inference for phoneme recognition\n",
    "    with torch.no_grad():\n",
    "        # Get phoneme log probabilities\n",
    "        log_probs = model(inputs)\n",
    "\n",
    "        # Recognize phoneme sequence\n",
    "        phoneme_sequences = model.recognize(inputs)\n",
    "\n",
    "    return {\"log_probs\": log_probs, \"phonemes\": phoneme_sequences}\n",
    "\n",
    "\n",
    "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "# Create the phoneme recognizer with the WavLM model\n",
    "phoneme_recognizer = PhonemeRecognizer(wavlm_model)\n",
    "phoneme_recognizer.eval()  # disable dropout, etc.\n",
    "\n",
    "# 2. Load an example audio file (here using a small demo from `datasets`)\n",
    "# ds = datasets.load_dataset(\"csv\", data_files=\"Hackathon_ASR/2_Audiofiles/Phoneme_Deletion_FR_T1\", features=features, split=\"train\")\n",
    "ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "audio_sample = ds[0][\"audio\"][\"array\"]\n",
    "sr = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "\n",
    "predicted = ds.select(range(1)).map(\n",
    "    lambda data_row: run_inference(data_row, phoneme_recognizer, \"fr\"),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Print output\n",
    "print(\"Log probabilities shape:\", np.shape(predicted[\"log_probs\"]))  # (batch_size, seq_len, num_phonemes)\n",
    "print(\"Recognized phoneme sequence:\", \"\".join(predicted[\"phonemes\"][0]))\n",
    "print(\"Transcript for reference:\", ds[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5207f7",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load our data in a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65d3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'sentence', 'age', 'accent', 'target_phonemes1'],\n",
      "    num_rows: 30\n",
      "})\n",
      "{'path': '/home/hugo/.cache/huggingface/datasets/downloads/extracted/273dd5304af8ab44c299689a6a1fc753eebc0c6ecfc9fd648c8255c26573941f/it_train_0/common_voice_it_25135786.mp3', 'array': array([-6.03961325e-14,  1.45661261e-13,  1.88293825e-13, ...,\n",
      "       -2.80459790e-06, -5.09811434e-06, -2.36993947e-06], shape=(159552,)), 'sampling_rate': 16000}\n",
      "a p a r t i r e d a l l a d e f i n i t t͡s j a m e n t o p o s s j\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import soundfile\n",
    "\n",
    "import ipa_encoder\n",
    "from src.phonemizer import commonvoice\n",
    "\n",
    "# 1. Location of your CSV\n",
    "def get_in_house_dataset(language):\n",
    "    audio_files_path = \"Hackathon_ASR/2_Audiofiles/\" + {\n",
    "        \"fr\": \"Phoneme_Deletion_FR\",\n",
    "        \"it\": \"Decoding_IT\"\n",
    "    }[language] + \"_T1/\"\n",
    "\n",
    "    dataset_path = f\"datasets/phonemized_{language}.csv\"\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Regenerating IPA CSV file\")\n",
    "        ipa_encoder.regenerate_ipa_csv(language)\n",
    "\n",
    "\n",
    "    # 2. Define initial features: audio paths as plain strings, phonemes as plain strings\n",
    "    features = datasets.Features({\n",
    "        \"file_name\": datasets.Value(\"string\"),\n",
    "        \"phonemes_coder1\": datasets.Value(\"string\"),\n",
    "        \"phonemes_coder2\": datasets.Value(\"string\")\n",
    "    })\n",
    "\n",
    "    # 3. Load the CSV into a DatasetDict (default split is 'train')\n",
    "    dataset = datasets.load_dataset(\"csv\", data_files=dataset_path, features=features, split=\"train\")\n",
    "\n",
    "    dataset = dataset.map(lambda data_row: {\"audio\": audio_files_path + data_row[\"file_name\"]}, desc=\"Select audio files path\")\n",
    "\n",
    "\n",
    "    print(dataset.num_rows, \"rows before filtering\")\n",
    "    dataset = dataset.filter(check_audios_valid, desc=\"Filtering out unreadable files\")\n",
    "    print(dataset.num_rows, \"rows after filtering\")\n",
    "\n",
    "    dataset = dataset.map(split_phonemes, remove_columns=[\"phonemes_coder1\", \"phonemes_coder2\"], desc=\"Phonemize data\")\n",
    "\n",
    "    # 7. Cast the phoneme_sequence column to a Sequence of strings\n",
    "    dataset = dataset.cast_column(\n",
    "        \"target_phonemes1\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    ).cast_column(\n",
    "        \"target_phonemes2\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_common_voice_phonemized_dataset(language):\n",
    "    return commonvoice.get_phonemized_datasets([language])[language]\n",
    "\n",
    "\n",
    "# 6. Map + split phoneme strings into lists\n",
    "def split_in_bracket(string):\n",
    "    output = []\n",
    "    in_brackets = False\n",
    "    if string is None:\n",
    "        return output\n",
    "    for char in string:\n",
    "        if in_brackets:\n",
    "            output[-1] += char\n",
    "        else:\n",
    "            output.append(char)\n",
    "\n",
    "        if char == '[':\n",
    "            in_brackets = True\n",
    "        elif char == ']':\n",
    "            if output[-1] not in VOCAB:\n",
    "                print(f\"Removing {output.pop()}\")\n",
    "            in_brackets = False\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_phonemes(data_row):\n",
    "    \"\"\"Split each phoneme into a list.\"\"\"\n",
    "    data_row[\"target_phonemes1\"] = split_in_bracket(data_row[\"phonemes_coder1\"])\n",
    "    data_row[\"target_phonemes2\"] = split_in_bracket(data_row[\"phonemes_coder2\"])\n",
    "    return data_row\n",
    "\n",
    "def check_audios_valid(data_row):\n",
    "    \"\"\"Mark file invalid when it cannot be read.\"\"\"\n",
    "    if not os.path.exists(data_row[\"audio\"]):\n",
    "        return False\n",
    "    \n",
    "    with open(data_row[\"audio\"], 'rb') as file:\n",
    "        try:\n",
    "            soundfile.read(file)\n",
    "            return True\n",
    "        except soundfile.LibsndfileError:\n",
    "            return False\n",
    "\n",
    "USE_IN_HOUSE = False\n",
    "\n",
    "if USE_IN_HOUSE:\n",
    "    dataset = get_in_house_dataset(LANGUAGE)\n",
    "else:\n",
    "    dataset = get_common_voice_phonemized_dataset(LANGUAGE)\n",
    "\n",
    "# 5. Cast 'audio' to the Audio type (will load the file when you access it)\n",
    "dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "\n",
    "\n",
    "# Now 'dataset' has:\n",
    "#   - dataset[i][\"audio\"] → { \"array\": np.ndarray, \"sampling_rate\": 16000 }\n",
    "#   - dataset[i][\"target_phonemes1\"] → list of strings\n",
    "print(dataset)\n",
    "print(dataset[0][\"audio\"])\n",
    "print(dataset[0][\"target_phonemes1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf53be",
   "metadata": {},
   "source": [
    "## Putting stuff together\n",
    "\n",
    "Now we run the model on our in-house dataset.\n",
    "We will extract the features so that it is easier to work with latter on.\n",
    "For this version we don't train the model, only a fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb0ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting audio features:   0%|          | 0/932 [00:00<?, ? examples/s]/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Extracting audio features:   2%|▏         | 15/932 [00:01<01:43,  8.83 examples/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.84 GiB. GPU 0 has a total capacity of 15.67 GiB of which 566.31 MiB is free. Process 2088 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 13.43 GiB memory in use. Of the allocated memory 9.43 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     25\u001b[39m wavlm_model.to(device)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     28\u001b[39m     features_dataset = (\n\u001b[32m     29\u001b[39m         \u001b[43mdataset\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43maudio_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLANGUAGE\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExtracting audio features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m         .with_format(\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m     )\n\u001b[32m     41\u001b[39m features_dataset.save_to_disk(dataset_path)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3525\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3524\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3525\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36maudio_processor\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maudio_processor\u001b[39m(batch):\n\u001b[32m     14\u001b[39m     preprocessed = preprocess_audios(batch)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mwavlm_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.last_hidden_state}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:1184\u001b[39m, in \u001b[36mWavLMModel.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1179\u001b[39m hidden_states, extract_features = \u001b[38;5;28mself\u001b[39m.feature_projection(extract_features)\n\u001b[32m   1180\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m._mask_hidden_states(\n\u001b[32m   1181\u001b[39m     hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n\u001b[32m   1182\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1192\u001b[39m hidden_states = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:440\u001b[39m, in \u001b[36mWavLMEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    432\u001b[39m         layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    433\u001b[39m             layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    434\u001b[39m             hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    437\u001b[39m             output_attentions,\n\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m         layer_outputs = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     hidden_states, position_bias = layer_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:325\u001b[39m, in \u001b[36mWavLMEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, position_bias=\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m, index=\u001b[32m0\u001b[39m):\n\u001b[32m    324\u001b[39m     attn_residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     hidden_states, attn_weights, position_bias = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    333\u001b[39m     hidden_states = attn_residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:191\u001b[39m, in \u001b[36mWavLMAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[39m\n\u001b[32m    188\u001b[39m gated_position_bias = gate_output.view(bsz * \u001b[38;5;28mself\u001b[39m.num_heads, -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) * position_bias\n\u001b[32m    189\u001b[39m gated_position_bias = gated_position_bias.view((-\u001b[32m1\u001b[39m, tgt_len, tgt_len))\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m attn_output, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_multi_head_self_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgated_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights, position_bias\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:215\u001b[39m, in \u001b[36mWavLMAttention.torch_multi_head_self_attention\u001b[39m\u001b[34m(self, hidden_states, attention_mask, gated_position_bias, output_attentions)\u001b[39m\n\u001b[32m    211\u001b[39m add_zero_attn = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# PyTorch 1.3.0 has F.multi_head_attention_forward defined\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# so no problem with backwards compatibility\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m attn_output, attn_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgated_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_separate_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# [Seq_Len, Batch Size, ...] -> [Batch Size, Seq_Len, ...]\u001b[39;00m\n\u001b[32m    240\u001b[39m attn_output = attn_output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:6350\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6348\u001b[39m         attn_mask = key_padding_mask\n\u001b[32m   6349\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6350\u001b[39m         attn_mask = \u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\n\u001b[32m   6352\u001b[39m \u001b[38;5;66;03m# adjust dropout probability\u001b[39;00m\n\u001b[32m   6353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.84 GiB. GPU 0 has a total capacity of 15.67 GiB of which 566.31 MiB is free. Process 2088 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 13.43 GiB memory in use. Of the allocated memory 9.43 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "DATASETS_DIR = \"datasets\"\n",
    "dataset_path = f\"{DATASETS_DIR}/features_{LANGUAGE}\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def audio_processor(batch):\n",
    "    preprocessed = preprocess_audios(batch)\n",
    "\n",
    "    return {\"features\": wavlm_model(\n",
    "        input_values=preprocessed[\"input_values\"].to(device), attention_mask=preprocessed[\"attention_mask\"].to(device)\n",
    "    ).last_hidden_state}\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    features_dataset = datasets.load_from_disk(dataset_path)\n",
    "    print(f\"Using existing dataset {dataset_path}\")\n",
    "else:\n",
    "    wavlm_model.eval()\n",
    "    wavlm_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_dataset = (\n",
    "            dataset\n",
    "            .map(\n",
    "                audio_processor,\n",
    "                batched=True,\n",
    "                input_columns=[\"audio\"],\n",
    "                remove_columns=[\"audio\"],\n",
    "                batch_size=30 if LANGUAGE == \"fr\" else 15,\n",
    "                desc=\"Extracting audio features\"\n",
    "            )\n",
    "            .with_format(\"torch\")\n",
    "        )\n",
    "\n",
    "    features_dataset.save_to_disk(dataset_path)\n",
    "    print(f\"Saved to {dataset_path}\")\n",
    "features_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690dea4",
   "metadata": {},
   "source": [
    "## Defining a new linear layer\n",
    "\n",
    "As a speed-up, we simply create a linear layer to map from the extracted features to the phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfda0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PhonemeMapper(nn.Module):\n",
    "    def __init__(self, features_size, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(features_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(inputs)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    phoneme = list(PHONEME_DICT.keys())[list(PHONEME_DICT.values()).index(p)]\n",
    "                    seq.append(phoneme)\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "    \n",
    "linear_mapper = PhonemeMapper(wavlm_model.config.hidden_size + 1, NUM_PHONEMES)\n",
    "\n",
    "wavlm_model.config.hidden_size + 1, NUM_PHONEMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc4765",
   "metadata": {},
   "source": [
    "## Model fine-tuning\n",
    "\n",
    "Now that the linear layer is ready, we can simply train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bfb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9678480625152588\n",
      "Epoch 0, Loss: 1.8199660778045654\n",
      "Epoch 0, Loss: 1.8158434629440308\n",
      "Epoch 0, Loss: 1.906231164932251\n",
      "Epoch 0, Loss: 1.6913785934448242\n",
      "Epoch 0, Loss: 1.941213607788086\n",
      "Epoch 0, Loss: 1.7701427936553955\n",
      "Epoch 0, Loss: 1.7291605472564697\n",
      "Epoch 0, Loss: 1.7339723110198975\n",
      "Epoch 0, Loss: 1.777764081954956\n",
      "Epoch 0, Loss: 1.6636220216751099\n",
      "Epoch 0, Loss: 1.951379656791687\n",
      "Epoch 0, Loss: 1.8753812313079834\n",
      "Epoch 0, Loss: 1.6734904050827026\n",
      "Epoch 0, Loss: 1.8911526203155518\n",
      "Epoch 0, Loss: 1.7707959413528442\n",
      "Epoch 0, Loss: 1.7494168281555176\n",
      "Epoch 0, Loss: 1.9437024593353271\n",
      "Epoch 0, Loss: 1.9688752889633179\n",
      "Epoch 0, Loss: 1.762195348739624\n",
      "Epoch 0, Loss: 2.069976806640625\n",
      "Epoch 0, Loss: 2.067138671875\n",
      "Epoch 0, Loss: 2.0046169757843018\n",
      "Epoch 0, Loss: 2.0450799465179443\n",
      "Epoch 0, Loss: 1.9044194221496582\n",
      "Epoch 0, Loss: 1.7887951135635376\n",
      "Epoch 0, Loss: 1.8656823635101318\n",
      "Epoch 0, Loss: 2.0004332065582275\n",
      "Epoch 0, Loss: 1.7422441244125366\n",
      "Epoch 0, Loss: 1.849332332611084\n",
      "Epoch 0, Loss: 1.9428507089614868\n",
      "Epoch 0, Loss: 1.6903798580169678\n",
      "Epoch 0, Loss: 1.9764323234558105\n",
      "Epoch 0, Loss: 1.687422752380371\n",
      "Epoch 0, Loss: 1.9885485172271729\n",
      "Epoch 0, Loss: 1.721339225769043\n",
      "Epoch 0, Loss: 2.0278961658477783\n",
      "Epoch 0, Loss: 1.7503666877746582\n",
      "Epoch 0, Loss: 1.9315916299819946\n",
      "Epoch 0, Loss: 1.8344013690948486\n",
      "Epoch 0, Loss: 1.7609714269638062\n",
      "Epoch 0, Loss: 1.6355355978012085\n",
      "Epoch 0, Loss: 2.0387587547302246\n",
      "Epoch 0, Loss: 1.7816712856292725\n",
      "Epoch 0, Loss: 1.84043550491333\n",
      "Epoch 0, Loss: 2.1159868240356445\n",
      "Epoch 0, Loss: 1.6711673736572266\n",
      "Epoch 0, Loss: 1.9822807312011719\n",
      "Epoch 0, Loss: 2.3949880599975586\n",
      "Epoch 0, Loss: 1.7873692512512207\n",
      "Epoch 0, Loss: 1.9065027236938477\n",
      "Epoch 0, Loss: 1.82518470287323\n",
      "Epoch 0, Loss: 2.0321390628814697\n",
      "Epoch 0, Loss: 1.7763348817825317\n",
      "Epoch 0, Loss: 2.0135562419891357\n",
      "Epoch 0, Loss: 1.6334084272384644\n",
      "Epoch 0, Loss: 2.014699697494507\n",
      "Epoch 0, Loss: 1.6233608722686768\n",
      "Epoch 0, Loss: 1.8404076099395752\n",
      "Epoch 0, Loss: 1.6736576557159424\n",
      "Epoch 0, Loss: 1.6779221296310425\n",
      "Epoch 0, Loss: 2.0519561767578125\n",
      "Epoch 0, Loss: 1.8414711952209473\n",
      "Epoch 0, Loss: 1.6362383365631104\n",
      "Epoch 0, Loss: 1.906299114227295\n",
      "Epoch 0, Loss: 1.8359256982803345\n",
      "Epoch 0, Loss: 1.9685754776000977\n",
      "Epoch 0, Loss: 2.002547264099121\n",
      "Epoch 0, Loss: 1.7553154230117798\n",
      "Epoch 0, Loss: 1.8094086647033691\n",
      "Epoch 0, Loss: 2.0383496284484863\n",
      "Epoch 0, Loss: 1.740733027458191\n",
      "Epoch 0, Loss: 1.866498351097107\n",
      "Epoch 0, Loss: 1.8959119319915771\n",
      "Epoch 0, Loss: 1.9965381622314453\n",
      "Epoch 1, Loss: 1.7078797817230225\n",
      "Epoch 1, Loss: 1.7337805032730103\n",
      "Epoch 1, Loss: 1.8684024810791016\n",
      "Epoch 1, Loss: 1.59665846824646\n",
      "Epoch 1, Loss: 1.9612641334533691\n",
      "Epoch 1, Loss: 1.9008538722991943\n",
      "Epoch 1, Loss: 1.671586036682129\n",
      "Epoch 1, Loss: 1.7996838092803955\n",
      "Epoch 1, Loss: 1.9627341032028198\n",
      "Epoch 1, Loss: 1.6952462196350098\n",
      "Epoch 1, Loss: 1.8185913562774658\n",
      "Epoch 1, Loss: 1.6507899761199951\n",
      "Epoch 1, Loss: 1.7997455596923828\n",
      "Epoch 1, Loss: 2.0635437965393066\n",
      "Epoch 1, Loss: 1.9563586711883545\n",
      "Epoch 1, Loss: 1.81705904006958\n",
      "Epoch 1, Loss: 1.7327755689620972\n",
      "Epoch 1, Loss: 2.003537654876709\n",
      "Epoch 1, Loss: 1.6793729066848755\n",
      "Epoch 1, Loss: 1.7957900762557983\n",
      "Epoch 1, Loss: 2.032703399658203\n",
      "Epoch 1, Loss: 1.8912758827209473\n",
      "Epoch 1, Loss: 1.833794116973877\n",
      "Epoch 1, Loss: 1.9205615520477295\n",
      "Epoch 1, Loss: 1.9595881700515747\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m features_dataset = features_dataset.shuffle()\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, features_dataset.num_rows, batch_size):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     batch_data = \u001b[43mfeatures_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     63\u001b[39m     input_lengths = torch.zeros(batch_size, dtype=torch.uint32)\n\u001b[32m     64\u001b[39m     max_len = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, batch_data[\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:410\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:119\u001b[39m, in \u001b[36mTorchFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    117\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.numpy_arrow_extractor().extract_batch(pa_table)\n\u001b[32m    118\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    121\u001b[39m     batch[column_name] = \u001b[38;5;28mself\u001b[39m._consolidate(batch[column_name])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:102\u001b[39m, in \u001b[36mTorchFormatter.recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:522\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    519\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    520\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m    521\u001b[39m mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\n\u001b[32m    524\u001b[39m ]\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    526\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:383\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    381\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function([data_struct])[\u001b[32m0\u001b[39m]\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    385\u001b[39m     batched\n\u001b[32m    386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    387\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    388\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    389\u001b[39m ):\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:96\u001b[39m, in \u001b[36mTorchFormatter._recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np.ndarray):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_struct.dtype == \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:102\u001b[39m, in \u001b[36mTorchFormatter.recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:494\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    493\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    496\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:96\u001b[39m, in \u001b[36mTorchFormatter._recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np.ndarray):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_struct.dtype == \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:102\u001b[39m, in \u001b[36mTorchFormatter.recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:494\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    493\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    496\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:99\u001b[39m, in \u001b[36mTorchFormatter._recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85\u001b[39m, in \u001b[36mTorchFormatter._tensorize\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, VideoReader):\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m value  \u001b[38;5;66;03m# TODO(QL): set output to torch tensors ?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_tensor_kwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "linear_mapper.to(device).train()\n",
    "linear_optimizer = torch.optim.Adam(linear_mapper.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "batch_size = 64 if LANGUAGE == \"fr\" else 16\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    name_format = r\"linear_mapper_.*(\\d+)\\.pth$\"\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if re.search(name_format, f)]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(name_format, x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(name_format, checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            linear_mapper.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "prepare_folders()\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    features_dataset = features_dataset.shuffle()\n",
    "    for i in range(0, features_dataset.num_rows, batch_size):\n",
    "        batch_data = features_dataset[i:i + batch_size]\n",
    "\n",
    "\n",
    "        input_lengths = torch.zeros(batch_size, dtype=torch.uint32)\n",
    "        max_len = max(map(len, batch_data[\"features\"]))\n",
    "\n",
    "        input_batch = torch.zeros((batch_size, max_len, wavlm_model.config.hidden_size + 1)).to(device)\n",
    "        input_batch[:, :, :-1] = int(LANGUAGE == \"it\")\n",
    "        for i, feat in enumerate(batch_data[\"features\"]):\n",
    "            input_batch[i, :feat.shape[0]] = feat\n",
    "            input_lengths[i] = feat.shape[0]\n",
    "\n",
    "        log_probs = linear_mapper(\n",
    "            input_batch.reshape(\n",
    "                (batch_size, -1, wavlm_model.config.hidden_size)\n",
    "            )\n",
    "        )\n",
    "        losses = []\n",
    "\n",
    "        for coder in (\"1\", \"2\"):\n",
    "            targets = [phoneme_recognizer.tokenize(string, lenient=True) for string in batch_data[f\"target_phonemes{coder}\"]]\n",
    "            target_lengths = torch.zeros(batch_size, dtype=torch.uint8)\n",
    "            max_len = max(map(lambda x: x.shape[0], targets))\n",
    "\n",
    "            target_batch = torch.zeros((batch_size, max_len))\n",
    "            for i, target in enumerate(targets):\n",
    "                target_batch[i, :target.shape[0]] = target\n",
    "                target_lengths[i] = target.shape[0]\n",
    "\n",
    "            losses.append(F.ctc_loss(\n",
    "                log_probs.transpose(0, 1),\n",
    "                target_batch,\n",
    "                input_lengths=torch.tensor([x.shape[0] for x in log_probs]),\n",
    "                target_lengths=target_lengths\n",
    "            ))\n",
    "\n",
    "        loss = (losses[0] + losses[1]) / 2\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        for logs, target_phons1, target_phons2 in zip(phoneme_recognizer.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"], batch_data[\"target_phonemes2\"]):\n",
    "            write_to_csv(\n",
    "                [\n",
    "                    increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1), \"\".join(target_phons2)\n",
    "                ]\n",
    "            )\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        increment += 1\n",
    "    torch.save(\n",
    "        linear_mapper.state_dict(),\n",
    "        f\"{MODEL_DIR}/linear_mapper_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e2537",
   "metadata": {},
   "source": [
    "## View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce0445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "\n",
    "with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        losses.append(float(row[3]))\n",
    "\n",
    "plt.plot(losses, label=\"Loss over training step\", linestyle='dotted')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df87423",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we want a phoneme recognition.\n",
    "It means to train the last layer of the model to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b488c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4194483757019043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.888230800628662\n",
      "Epoch 0, Loss: 1.146170735359192\n",
      "Epoch 0, Loss: 6.249912738800049\n",
      "Epoch 0, Loss: 1.2091354131698608\n",
      "Epoch 0, Loss: 3.8059654235839844\n",
      "Epoch 0, Loss: 2.063833713531494\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "phoneme_recognizer.train()\n",
    "linear_optimizer = torch.optim.Adam(\n",
    "    phoneme_recognizer.phoneme_classifier.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_ctc_loss(log_probs, target_sequence):\n",
    "    \"\"\"Calculates CTC loss.\"\"\"\n",
    "    # Create input_lengths and target_lengths tensors\n",
    "    input_lengths = torch.tensor([1])  # Batch size of 1\n",
    "    target_lengths = torch.tensor([1])  # Batch size of 1\n",
    "\n",
    "    # Calculate CTC loss\n",
    "    loss = F.ctc_loss(\n",
    "        log_probs,\n",
    "        target_sequence,\n",
    "        input_lengths=input_lengths,\n",
    "        target_lengths=target_lengths\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if f.endswith(\".pth\")]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(r\"(\\d+)\\.pth$\", x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(r\"(\\d+)\\.pth$\", checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            phoneme_recognizer.phoneme_classifier.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "prepare_folders()\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "\n",
    "# Freeze the wavlm model\n",
    "for param in phoneme_recognizer.wavlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(dataset.shuffle().select(range(50))):\n",
    "        inputs = preprocess_audios(data)\n",
    "        log_probs = phoneme_recognizer(inputs)\n",
    "        split_phonemes = smart_split_coder(data[\"phoneme_sequence\"][0])\n",
    "        target = phoneme_recognizer.tokenize(split_phonemes)\n",
    "        loss = calculate_ctc_loss(log_probs[0], target.reshape([1, -1]))\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        write_to_csv(\n",
    "            [\n",
    "                increment, epoch, i, loss.item(),\n",
    "                \"\".join(phoneme_recognizer.classify_to_phonemes(log_probs)[0]),\n",
    "                \"\".join(split_phonemes)\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        increment += 1\n",
    "        torch.save(\n",
    "            phoneme_recognizer.phoneme_classifier.state_dict(),\n",
    "            f\"{MODEL_DIR}/phoneme_classifier_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d7f10",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "We have a model roughly trained for phonemes.\n",
    "We want a binary classification though.\n",
    "We won't do that for now as it would be an end-to-end pipeline, defeating the purpose of the created pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
