{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b6c7c4",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Let's create a model first, with some vocab.\n",
    "The output is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f115131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287.46s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.12/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in ./.venv/lib/python3.12/site-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.venv/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa numpy soundfile torch torchaudio datasets transformers matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a560f",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First, let's load our data in a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded48737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phonemizing fr: 100%|██████████| 8192/8192 [00:49<00:00, 165.26 examples/s]\n",
      "Map: 100%|██████████| 8192/8192 [00:00<00:00, 22947.60 examples/s]\n",
      "Casting the dataset: 100%|██████████| 8192/8192 [00:00<00:00, 2231876.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8192/8192 [00:00<00:00, 10986.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to datasets/phonemized_fr_common\n",
      "Updating vocabulary\n",
      "Vocabulary saved as custom_tokenizer/vocab.json\n",
      "Dataset({\n",
      "    features: ['audio', 'sentence', 'age', 'accent', 'target_phonemes1'],\n",
      "    num_rows: 8192\n",
      "})\n",
      "['ʒ', 'ə', 'n', 'ə', 's', 'ɛ', 'd', 'u', 's', 'a', 'm', 'a', 'ʒ', 'ɛ', 's', 't', 'e', 'v', 'ə', 'n', 'ɛ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import soundfile\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "import ipa_encoder\n",
    "from src.phonemizer import commonvoice, text_phonemizer\n",
    "\n",
    "LANGUAGE = (\"fr\", \"it\")[0]\n",
    "USE_IN_HOUSE = False\n",
    "DATASETS_DIR = \"datasets\"\n",
    "\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "# 1. Location of your CSV\n",
    "def get_in_house_dataset(language):\n",
    "    audio_files_path = \"Hackathon_ASR/2_Audiofiles/\" + {\n",
    "        \"fr\": \"Phoneme_Deletion_FR\",\n",
    "        \"it\": \"Decoding_IT\"\n",
    "    }[language] + \"_T1/\"\n",
    "\n",
    "    dataset_path = f\"datasets/phonemized_{language}.csv\"\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Regenerating IPA CSV file\")\n",
    "        ipa_encoder.regenerate_ipa_csv(language)\n",
    "\n",
    "\n",
    "    # 2. Define initial features: audio paths as plain strings, phonemes as plain strings\n",
    "    features = datasets.Features({\n",
    "        \"file_name\": datasets.Value(\"string\"),\n",
    "        \"phonemes_coder1\": datasets.Value(\"string\"),\n",
    "        \"phonemes_coder2\": datasets.Value(\"string\")\n",
    "    })\n",
    "\n",
    "    # 3. Load the CSV into a DatasetDict (default split is 'train')\n",
    "    dataset = datasets.load_dataset(\"csv\", data_files=dataset_path, features=features, split=\"train\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda data_row: {\"audio\": audio_files_path + data_row[\"file_name\"]},\n",
    "        desc=\"Select audio files path\"\n",
    "    )\n",
    "\n",
    "\n",
    "    print(dataset.num_rows, \"rows before filtering\")\n",
    "    dataset = dataset.filter(check_audios_valid, desc=\"Filtering out unreadable files\")\n",
    "    print(dataset.num_rows, \"rows after filtering\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        split_phonemes, \n",
    "        remove_columns=[\"phonemes_coder1\", \"phonemes_coder2\"],\n",
    "        desc=\"Phonemize data\"\n",
    "    )\n",
    "\n",
    "    # 7. Cast the phoneme_sequence column to a Sequence of strings\n",
    "    dataset = dataset.cast_column(\n",
    "        \"target_phonemes1\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    ).cast_column(\n",
    "        \"target_phonemes2\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_common_voice_phonemized_dataset(language, limit=-1):\n",
    "    \"\"\"Phonemized dataset from common voice.\"\"\"\n",
    "    dataset = commonvoice.get_phonemized_datasets([language], limit_items=limit)[language]\n",
    "    return dataset.map(\n",
    "        lambda x: {\"target_phonemes1\": x.split(\" \")},\n",
    "        input_columns=[\"target_phonemes1\"]\n",
    "    ).cast_column(\n",
    "        \"target_phonemes1\",\n",
    "        datasets.Sequence(feature=datasets.Value(\"string\"))\n",
    "    )\n",
    "\n",
    "\n",
    "# 6. Map + split phoneme strings into lists\n",
    "def split_in_bracket(string):\n",
    "    output = []\n",
    "    in_brackets = False\n",
    "    if string is None:\n",
    "        return output\n",
    "    for char in string:\n",
    "        if in_brackets:\n",
    "            output[-1] += char\n",
    "        else:\n",
    "            output.append(char)\n",
    "\n",
    "        if char == '[':\n",
    "            in_brackets = True\n",
    "        elif char == ']':\n",
    "            if output[-1] not in VOCAB:\n",
    "                print(f\"Removing {output.pop()}\")\n",
    "            in_brackets = False\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_phonemes(data_row):\n",
    "    \"\"\"Split each phoneme into a list.\"\"\"\n",
    "    data_row[\"target_phonemes1\"] = split_in_bracket(data_row[\"phonemes_coder1\"])\n",
    "    data_row[\"target_phonemes2\"] = split_in_bracket(data_row[\"phonemes_coder2\"])\n",
    "    return data_row\n",
    "\n",
    "def check_audios_valid(data_row):\n",
    "    \"\"\"Mark file invalid when it cannot be read.\"\"\"\n",
    "    if not os.path.exists(data_row[\"audio\"]):\n",
    "        return False\n",
    "    \n",
    "    with open(data_row[\"audio\"], 'rb') as file:\n",
    "        try:\n",
    "            soundfile.read(file)\n",
    "            return True\n",
    "        except soundfile.LibsndfileError:\n",
    "            return False\n",
    "    \n",
    "\n",
    "def create_phonemized_dataset(max_rows=-1):\n",
    "    clear_cache()\n",
    "    if USE_IN_HOUSE:\n",
    "        dataset = get_in_house_dataset(LANGUAGE)\n",
    "    else:\n",
    "        dataset = get_common_voice_phonemized_dataset(LANGUAGE, max_rows)\n",
    "\n",
    "    # 5. Cast 'audio' to the Audio type (will load the file when you access it)\n",
    "    dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_path = f\"{DATASETS_DIR}/phonemized_{LANGUAGE}{'' if USE_IN_HOUSE else '_common'}\"\n",
    "\n",
    "num_audios = 8192\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = datasets.load_from_disk(dataset_path)\n",
    "    if dataset.num_rows < num_audios:\n",
    "        dataset = create_phonemized_dataset(num_audios)\n",
    "        dataset.save_to_disk(dataset_path)\n",
    "        print(f\"Saved to {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"Using existing dataset {dataset_path}\")\n",
    "else:\n",
    "    dataset = create_phonemized_dataset(num_audios)\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "    print(f\"Saved to {dataset_path}\")\n",
    "\n",
    "# Regenerate the vocabulary if new phonemes\n",
    "coders = ((\"1\", \"2\") if USE_IN_HOUSE else (\"1\", ))\n",
    "phonemes = set()\n",
    "for coder in coders:\n",
    "    for phonemes_list in dataset[f\"target_phonemes{coder}\"]:\n",
    "        phonemes.update(phonemes_list)\n",
    "\n",
    "# IT + FR phonemes\n",
    "PHONEMES_DICT = text_phonemizer.check_regenerate_vocabulary(phonemes)\n",
    "VOCAB = tuple(PHONEMES_DICT.keys())\n",
    "\n",
    "# Now 'dataset' has:\n",
    "#   - dataset[i][\"audio\"] → { \"array\": np.ndarray, \"sampling_rate\": 16000 }\n",
    "#   - dataset[i][\"target_phonemes1\"] → list of strings\n",
    "print(dataset)\n",
    "print(dataset[0][\"target_phonemes1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe00d86",
   "metadata": {},
   "source": [
    "## Define a new linear layer\n",
    "\n",
    "As a speed-up, we simply create a linear layer to map from the extracted features to the phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d917c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size 769 Num phonemes 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities shape: (1, 154, 768)\n",
      "Recognized phoneme sequence: ĩ̯ɛɔ̯œ̃ʼɪhɪvʲOɦŋɦʏ̃ɦɛɦwɦʼykʼt͡ʂœ̃ĩʼyʁʼɪĩOʃɛzt͡ʂmiʼŋwaʼɥmʲbœ̃dd͡zljẽɔ̯t͡ʂʼɛ̃ẽɔ̯zwmiʼt͡ɕjɛdt͡ɕOɥɔ̯ʁʼɪœ̃aʼɔ̯ʒwpIt͡ʂʁʼʼjjɨfɟkʼtɪʃʒɛpwɪʃɔ̯ŋdɯdyɪrɔ̃oʼɦɔ̃Iɔ̃Oɔ̃Oɔ̃ɛOsʏ̃tlĩ̯\n",
      "Transcript for reference: Je ne sais d'où Sa Majesté venait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# PhonemeRecognizer: WavLM + CTC for phoneme speech recognition\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "NUM_PHONEMES = len(VOCAB)\n",
    "\n",
    "class PhonemeMapper(nn.Module):\n",
    "    def __init__(self, features_size, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank)\n",
    "        self.phoneme_classifier = nn.Linear(features_size, num_phonemes)\n",
    "\n",
    "    def forward(self, input_values, language):\n",
    "        input_batch = torch.empty(\n",
    "            (input_values.shape[0], input_values.shape[1], input_values.shape[2] + 1),\n",
    "            dtype=input_values.dtype, device=input_values.device\n",
    "        )\n",
    "        input_batch[:, :, :-1] = input_values\n",
    "        if language == \"it\":\n",
    "            lang_val = 1\n",
    "        elif language == \"fr\":\n",
    "            lang_val = 0\n",
    "        else:\n",
    "            lang_val = 0.5\n",
    "        input_batch[:, :, -1] = lang_val\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(input_batch)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(hidden_states)\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def tokenize(self, char_list, lenient=False):\n",
    "        \"\"\"\n",
    "        Go from a list of characters to a list of indices.\n",
    "        \n",
    "        :param list[str] char_list: Characters top be mapped.\n",
    "        :param bool lenient: If True, characters not in vocab are mapped to [UNK] \n",
    "        \"\"\"\n",
    "        if not lenient:\n",
    "            return torch.tensor([PHONEMES_DICT[x] for x in char_list])\n",
    "        \n",
    "        return torch.tensor([PHONEMES_DICT[x] if x in PHONEMES_DICT else PHONEMES_DICT[\"[UNK]\"] for x in char_list])\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    seq.append(VOCAB[p])\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_audios(batch):\n",
    "    \"\"\"Preprocess the audio files (pad/truncate + batch-dim).\"\"\"\n",
    "    for data_row in batch:\n",
    "        if data_row[\"sampling_rate\"] != 16000:\n",
    "            raise NotImplementedError(\n",
    "                f\"No sampling rate can be different from 16000, is {data_row[\"sampling_rate\"]}\"\n",
    "            )\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        [data_row[\"array\"] for data_row in batch],\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,       # pad to longest in batch\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def run_inference(batch, model, language=\"fr\"):\n",
    "    \"\"\"Return log probs and most likely phonemes.\"\"\"\n",
    "    inputs = preprocess_audios(batch[\"audio\"])\n",
    "\n",
    "    # 4. Inference for phoneme recognition\n",
    "    with torch.no_grad():\n",
    "        # Get phoneme log probabilities\n",
    "        log_probs = model(**inputs).last_hidden_state\n",
    "\n",
    "        # Recognize phoneme sequence\n",
    "        phonemes_sequences = linear_mapper.classify_to_phonemes(linear_mapper(log_probs, language))\n",
    "\n",
    "    return {\"log_probs\": log_probs, \"phonemes\": phonemes_sequences}\n",
    "\n",
    "\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "# Method A: Using the PhonemeRecognizer for speech-to-phoneme ASR\n",
    "# ————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1. Load the feature extractor and model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "\n",
    "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "linear_mapper = PhonemeMapper(wavlm_model.config.hidden_size + 1, NUM_PHONEMES)\n",
    "\n",
    "print(\"Input size\", wavlm_model.config.hidden_size + 1, \"Num phonemes\", NUM_PHONEMES)\n",
    "\n",
    "# Create the phoneme recognizer with the WavLM model\n",
    "\n",
    "# 2. Load an example audio file\n",
    "audio_sample = dataset[0][\"audio\"][\"array\"]\n",
    "sr = dataset[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "\n",
    "predicted = dataset.select(range(1)).map(\n",
    "    lambda data_row: run_inference(data_row, wavlm_model, \"fr\"),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Print output\n",
    "print(\"Log probabilities shape:\", np.shape(predicted[\"log_probs\"]))  # (batch_size, seq_len, num_phonemes)\n",
    "print(\"Recognized phoneme sequence:\", \"\".join(predicted[\"phonemes\"][0]))\n",
    "print(\"Transcript for reference:\", dataset[0][\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf53be",
   "metadata": {},
   "source": [
    "## Putting stuff together\n",
    "\n",
    "Now we run the model on our in-house dataset.\n",
    "We will extract the features so that it is easier to work with latter on.\n",
    "For this version we don't train the model, only a fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb0ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset datasets/features_it_common\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'age', 'accent', 'target_phonemes1', 'features'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def audio_processor(batch):\n",
    "    preprocessed = preprocess_audios(batch)\n",
    "\n",
    "    return {\n",
    "        \"features\": wavlm_model(\n",
    "            input_values=preprocessed[\"input_values\"].to(device),\n",
    "            attention_mask=preprocessed[\"attention_mask\"].to(device)\n",
    "        ).last_hidden_state\n",
    "    }\n",
    "\n",
    "\n",
    "def regenerate_audio_features(dataset):\n",
    "    wavlm_model.eval()\n",
    "    wavlm_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_dataset = (\n",
    "            dataset\n",
    "            .map(\n",
    "                audio_processor,\n",
    "                batched=True,\n",
    "                input_columns=[\"audio\"],\n",
    "                remove_columns=[\"audio\"],\n",
    "                batch_size=30 if LANGUAGE == \"fr\" else 15,\n",
    "                desc=\"Extracting audio features\"\n",
    "            )\n",
    "            .with_format(\"torch\")\n",
    "        )\n",
    "    return features_dataset\n",
    "\n",
    "\n",
    "dataset_path = f\"{DATASETS_DIR}/features_{LANGUAGE}{'' if USE_IN_HOUSE else '_common'}\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    features_dataset = datasets.load_from_disk(dataset_path)\n",
    "    if features_dataset.num_rows < num_audios:\n",
    "        features_dataset = regenerate_audio_features(dataset)\n",
    "        features_dataset.save_to_disk(dataset_path)\n",
    "        print(f\"Saved to {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"Using existing dataset {dataset_path}\")\n",
    "else:\n",
    "    features_dataset = regenerate_audio_features(dataset)\n",
    "    features_dataset.save_to_disk(dataset_path)\n",
    "    print(f\"Saved to {dataset_path}\")\n",
    "\n",
    "features_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc4765",
   "metadata": {},
   "source": [
    "## Transfer learning (Linear model training)\n",
    "\n",
    "Now that the linear layer is ready, we can simply train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79bfb06",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PhonemeMapper:\n\tsize mismatch for phoneme_classifier.weight: copying a param with shape torch.Size([50, 769]) from checkpoint, the shape in current model is torch.Size([65, 769]).\n\tsize mismatch for phoneme_classifier.bias: copying a param with shape torch.Size([50]) from checkpoint, the shape in current model is torch.Size([65]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m linear_optimizer = torch.optim.Adam(linear_mapper.parameters(), lr=\u001b[32m1e-3\u001b[39m, weight_decay=\u001b[32m0\u001b[39m)\n\u001b[32m     58\u001b[39m batch_size = \u001b[32m64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LANGUAGE == \u001b[33m\"\u001b[39m\u001b[33mfr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m16\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m increment = \u001b[43mload_last_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m coders = ((\u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m USE_IN_HOUSE \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m, ))\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mload_last_checkpoint\u001b[39m\u001b[34m(model_dir)\u001b[39m\n\u001b[32m     36\u001b[39m     increment = \u001b[38;5;28mint\u001b[39m(match[\u001b[32m1\u001b[39m])\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Load the linear layer's parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43mlinear_mapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     42\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a model! Starting from scratch!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/asr/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for PhonemeMapper:\n\tsize mismatch for phoneme_classifier.weight: copying a param with shape torch.Size([50, 769]) from checkpoint, the shape in current model is torch.Size([65, 769]).\n\tsize mismatch for phoneme_classifier.bias: copying a param with shape torch.Size([50]) from checkpoint, the shape in current model is torch.Size([65])."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    name_format = r\"linear_mapper_.*(\\d+)\\.pth$\"\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if re.search(name_format, f)]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(name_format, x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(name_format, checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            linear_mapper.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "prepare_folders()\n",
    "clear_cache()\n",
    "\n",
    "linear_mapper.to(device).train()\n",
    "linear_optimizer = torch.optim.Adam(linear_mapper.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "batch_size = 64 if LANGUAGE == \"fr\" else 16\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "coders = ((\"1\", \"2\") if USE_IN_HOUSE else (\"1\", ))\n",
    "\n",
    "# Training loop\n",
    "features_dataset = features_dataset.select(range(1000))\n",
    "for epoch in range(600):\n",
    "    features_dataset = features_dataset.shuffle()\n",
    "    progress_bar = tqdm.trange(0, features_dataset.num_rows, batch_size)\n",
    "    for i in progress_bar:\n",
    "        batch_data = features_dataset[i:i + batch_size]\n",
    "\n",
    "        input_lengths = torch.zeros(batch_size, dtype=torch.uint32, device=device)\n",
    "        max_len = max(map(len, batch_data[\"features\"]))\n",
    "\n",
    "        input_batch = torch.zeros((batch_size, max_len, wavlm_model.config.hidden_size), device=device)\n",
    "        \n",
    "        for i, feat in enumerate(batch_data[\"features\"]):\n",
    "            input_batch[i, :feat.shape[0]] = feat\n",
    "            input_lengths[i] = feat.shape[0]\n",
    "\n",
    "        log_probs = linear_mapper(input_batch, LANGUAGE)\n",
    "        losses = []\n",
    "\n",
    "        for coder in coders:\n",
    "            targets = [linear_mapper.tokenize(string, lenient=True) for string in batch_data[f\"target_phonemes{coder}\"]]\n",
    "            target_lengths = torch.zeros(batch_size, dtype=torch.uint8, device=device)\n",
    "            max_len = max(map(lambda x: x.shape[0], targets))\n",
    "\n",
    "            target_batch = torch.zeros((batch_size, max_len), device=device)\n",
    "            for i, target in enumerate(targets):\n",
    "                target_batch[i, :target.shape[0]] = target\n",
    "                target_lengths[i] = target.shape[0]\n",
    "\n",
    "            input_lengths = torch.tensor([x.shape[0] for x in log_probs], device=device)\n",
    "            final_probs = log_probs.transpose(0, 1)\n",
    "\n",
    "            losses.append(F.ctc_loss(\n",
    "                final_probs,\n",
    "                target_batch,\n",
    "                input_lengths=input_lengths,\n",
    "                target_lengths=target_lengths\n",
    "            ))\n",
    "\n",
    "        if len(coders) == 1:\n",
    "            loss = losses[0]\n",
    "        else:\n",
    "            loss = (losses[0] + losses[1]) / 2\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        if len(coders) == 1:\n",
    "            for logs, target_phons1 in zip(linear_mapper.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"]):\n",
    "                write_to_csv(\n",
    "                    [\n",
    "                        increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1)\n",
    "                    ]\n",
    "                )\n",
    "        else:\n",
    "            for logs, target_phons1, target_phons2 in zip(linear_mapper.classify_to_phonemes(log_probs), batch_data[\"target_phonemes1\"], batch_data[\"target_phonemes2\"]):\n",
    "                write_to_csv(\n",
    "                    [\n",
    "                        increment, epoch, i, loss.item(), \"\".join(logs), \"\".join(target_phons1), \"\".join(target_phons2)\n",
    "                    ]\n",
    "                )\n",
    "        progress_bar.set_postfix({\"Epoch\": epoch, \"Loss\": loss.item()})\n",
    "        increment += 1\n",
    "    torch.save(\n",
    "        linear_mapper.state_dict(),\n",
    "        f\"{MODEL_DIR}/linear_mapper_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e2537",
   "metadata": {},
   "source": [
    "## View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "\n",
    "with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        losses.append(float(row[3]))\n",
    "\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "fig.suptitle(\"Model loss during training\")\n",
    "axes[0].plot(losses, label=\"Loss over training step\", linestyle='dotted')\n",
    "axes[0].grid()\n",
    "axes[0].set_xlabel(\"Training step\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "axes[1].plot(losses, label=\"Loss over training step\", linestyle='dotted')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid()\n",
    "axes[1].set_xlabel(\"Training step\")\n",
    "axes[1].set_ylabel(\"Loss (log scale)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df87423",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we want a phoneme recognition.\n",
    "It means to train the last layer of the model to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b488c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "class PhonemeRecognizer(nn.Module):\n",
    "    def __init__(self, wavlm_model, num_phonemes=NUM_PHONEMES):\n",
    "        super().__init__()\n",
    "        self.wavlm = wavlm_model\n",
    "\n",
    "        # Get the hidden size from the WavLM model\n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Linear layer to map from WavLM hidden states to phoneme classes (including blank), adds a new input for language\n",
    "        self.phoneme_classifier = nn.Linear(1 + hidden_size, num_phonemes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get WavLM embeddings\n",
    "        outputs = self.wavlm(input_values=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply the linear layer to get logits for each time step\n",
    "        logits = self.phoneme_classifier(torch.dstack([hidden_states, torch.ones(hidden_states.shape[1]) * inputs[\"language\"]]))\n",
    "\n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def classify_to_phonemes(self, log_probs):\n",
    "        # Simple greedy decoding (for demonstration)\n",
    "        # In a real system, you would use beam search with ctcdecode\n",
    "        predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert to phoneme sequences with CTC decoding rules (merge repeats, remove blanks)\n",
    "        phoneme_sequences = []\n",
    "        for pred_seq in predictions:\n",
    "            seq = []\n",
    "            prev = -1\n",
    "            for p in pred_seq:\n",
    "                # Skip blanks (index 0) and repeated phonemes (CTC rules)\n",
    "                if p != 0 and p != prev:\n",
    "                    # Convert index back to phoneme\n",
    "                    seq.append(VOCAB[p])\n",
    "                prev = p\n",
    "            phoneme_sequences.append(seq)\n",
    "\n",
    "        return phoneme_sequences\n",
    "\n",
    "\n",
    "    def recognize(self, inputs):\n",
    "        \"\"\"Perform phoneme recognition.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get log probabilities\n",
    "            log_probs = self(inputs)\n",
    "\n",
    "            return self.classify_to_phonemes(log_probs)\n",
    "\n",
    "    def tokenize(self, char_list, lenient=False):\n",
    "        \"\"\"\n",
    "        Go from a list of characters to a list of indices.\n",
    "        \n",
    "        :param list[str] char_list: Characters top be mapped.\n",
    "        :param bool lenient: If True, characters not in vocab are mapped to [UNK] \n",
    "        \"\"\"\n",
    "        if not lenient:\n",
    "            return torch.tensor([PHONEMES_DICT[x] for x in char_list])\n",
    "        \n",
    "        return torch.tensor([PHONEMES_DICT[x] if x in PHONEMES_DICT else PHONEMES_DICT[\"[UNK]\"] for x in char_list])\n",
    "    \n",
    "    def get_embedding(self, char_list):\n",
    "        tokens = self.tokenize(char_list)\n",
    "        out_tensor = torch.zeros((len(tokens), len(VOCAB)))\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            out_tensor[i, token_id] = 1\n",
    "        return out_tensor\n",
    "\n",
    "phoneme_recognizer = PhonemeRecognizer(wavlm_model)\n",
    "phoneme_recognizer.train()\n",
    "linear_optimizer = torch.optim.Adam(\n",
    "    phoneme_recognizer.phoneme_classifier.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_ctc_loss(log_probs, target_sequence):\n",
    "    \"\"\"Calculates CTC loss.\"\"\"\n",
    "    # Create input_lengths and target_lengths tensors\n",
    "    input_lengths = torch.tensor([1])  # Batch size of 1\n",
    "    target_lengths = torch.tensor([1])  # Batch size of 1\n",
    "\n",
    "    # Calculate CTC loss\n",
    "    loss = F.ctc_loss(\n",
    "        log_probs,\n",
    "        target_sequence,\n",
    "        input_lengths=input_lengths,\n",
    "        target_lengths=target_lengths\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "\n",
    "\n",
    "def prepare_folders():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "    \n",
    "\n",
    "def load_last_checkpoint(model_dir):\n",
    "    increment = -1\n",
    "    # Load the latest version\n",
    "    pth_files = [f for f in os.listdir(model_dir) if f.endswith(\".pth\")]\n",
    "    increment = len(pth_files)\n",
    "\n",
    "    if not pth_files:\n",
    "        warnings.warn(\"No .pth files found in the model directory! Starting from scratch!\")\n",
    "    else:\n",
    "        # Sort the files by their index (last number)\n",
    "        pth_files.sort(key=lambda x: int(re.search(r\"(\\d+)\\.pth$\", x)[1]))\n",
    "\n",
    "        # Load the latest version\n",
    "        checkpoint = pth_files[-1]  # Load the last element (highest index)\n",
    "        match = re.search(r\"(\\d+)\\.pth$\", checkpoint)\n",
    "        if match:\n",
    "            increment = int(match[1])\n",
    "            # Load the linear layer's parameters\n",
    "            phoneme_recognizer.phoneme_classifier.load_state_dict(\n",
    "                torch.load(f\"{model_dir}/{checkpoint}\")\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Couldn't find a model! Starting from scratch!\")\n",
    "    return increment\n",
    "\n",
    "prepare_folders()\n",
    "increment = load_last_checkpoint(MODEL_DIR)\n",
    "\n",
    "# Freeze the wavlm model\n",
    "for param in phoneme_recognizer.wavlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "def write_to_csv(row):\n",
    "    with open(f'{OUTPUTS_DIR}/phonemes_training.csv', 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(dataset.shuffle().select(range(50))):\n",
    "        input_values = preprocess_audios(data)\n",
    "        log_probs = phoneme_recognizer(input_values)\n",
    "        split_phonemes = smart_split_coder(data[\"phoneme_sequence\"][0])\n",
    "        target = phoneme_recognizer.tokenize(split_phonemes)\n",
    "        loss = calculate_ctc_loss(log_probs[0], target.reshape([1, -1]))\n",
    "        linear_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        linear_optimizer.step()\n",
    "        write_to_csv(\n",
    "            [\n",
    "                increment, epoch, i, loss.item(),\n",
    "                \"\".join(phoneme_recognizer.classify_to_phonemes(log_probs)[0]),\n",
    "                \"\".join(split_phonemes)\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        increment += 1\n",
    "        torch.save(\n",
    "            phoneme_recognizer.phoneme_classifier.state_dict(),\n",
    "            f\"{MODEL_DIR}/phoneme_classifier_epoch_{epoch}_step_{i}_{increment}.pth\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d7f10",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "We have a model roughly trained for phonemes.\n",
    "We want a binary classification though.\n",
    "We won't do that for now as it would be an end-to-end pipeline, defeating the purpose of the created pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
