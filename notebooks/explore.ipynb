{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c616efe5eef4ac5950da05d2d4f5536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1dfa7f223344328e2456ea5788f0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from text2phonemesequence import Text2PhonemeSequence\n",
    "\n",
    "text2phone_model = Text2PhonemeSequence(language='jpn', is_cuda=True)\n",
    "sentence = \"これ は 、 テスト テキスト です .\"\n",
    "input_phonemes = text2phone_model.infer_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"your_audio.wav\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x714fc7681d87 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x714fc763275f in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x714fc8f64904 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x714fc8f67304 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #4: <unknown function> + 0x3ab5e (0x714f2df27b5e in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg6.so)\nframe #5: <unknown function> + 0x32737 (0x714f2df1f737 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg6.so)\n<omitting python frames>\nframe #11: <unknown function> + 0xf874 (0x714fc9eb7874 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torchaudio/lib/_torchaudio.so)\nframe #60: <unknown function> + 0x2a1ca (0x714fcac2a1ca in /lib/x86_64-linux-gnu/libc.so.6)\nframe #61: __libc_start_main + 0x8b (0x714fcac2a28b in /lib/x86_64-linux-gnu/libc.so.6)\nframe #62: <unknown function> + 0x108e (0x61bae60a708e in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/bin/python)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Your test audio\u001b[39;00m\n\u001b[32m     42\u001b[39m audio_path = \u001b[33m\"\u001b[39m\u001b[33myour_audio.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m waveform, sr = \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m emissions = get_emissions(model, waveform)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Provide a phoneme transcript manually (or from another model)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_audio\u001b[39m\u001b[34m(path, target_sr)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_audio\u001b[39m(path, target_sr=\u001b[32m16000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     waveform, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sr != target_sr:\n\u001b[32m     13\u001b[39m         waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torchaudio/_backend/utils.py:205\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m backend = dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:297\u001b[39m, in \u001b[36mFFmpegBackend.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    289\u001b[39m     uri: InputType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     buffer_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m4096\u001b[39m,\n\u001b[32m    296\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:88\u001b[39m, in \u001b[36mload_audio\u001b[39m\u001b[34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(src, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mvorbis\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mformat\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mogg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m s = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m sample_rate = \u001b[38;5;28mint\u001b[39m(s.get_src_stream_info(s.default_audio_stream).sample_rate)\n\u001b[32m     90\u001b[39m \u001b[38;5;28mfilter\u001b[39m = _get_load_filter(frame_offset, num_frames, convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/io/_streaming_media_decoder.py:526\u001b[39m, in \u001b[36mStreamingMediaDecoder.__init__\u001b[39m\u001b[34m(self, src, format, option, buffer_size)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28mself\u001b[39m._be = ffmpeg_ext.StreamingMediaDecoderFileObj(src, \u001b[38;5;28mformat\u001b[39m, option, buffer_size)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28mself\u001b[39m._be = \u001b[43mffmpeg_ext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStreamingMediaDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m i = \u001b[38;5;28mself\u001b[39m._be.find_best_audio_stream()\n\u001b[32m    529\u001b[39m \u001b[38;5;28mself\u001b[39m._default_audio_stream = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m i\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to open the input \"your_audio.wav\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x714fc7681d87 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x714fc763275f in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x714fc8f64904 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x714fc8f67304 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #4: <unknown function> + 0x3ab5e (0x714f2df27b5e in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg6.so)\nframe #5: <unknown function> + 0x32737 (0x714f2df1f737 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg6.so)\n<omitting python frames>\nframe #11: <unknown function> + 0xf874 (0x714fc9eb7874 in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/lib/python3.11/site-packages/torchaudio/lib/_torchaudio.so)\nframe #60: <unknown function> + 0x2a1ca (0x714fcac2a1ca in /lib/x86_64-linux-gnu/libc.so.6)\nframe #61: __libc_start_main + 0x8b (0x714fcac2a28b in /lib/x86_64-linux-gnu/libc.so.6)\nframe #62: <unknown function> + 0x108e (0x61bae60a708e in /home/jdecim/Documents/EPFL/Master/MA2/Hackathon/Lemanic-Life-Sciences-Hackathon-2025/.venv/bin/python)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import WAV2VEC2_ASR_BASE_960H\n",
    "\n",
    "# Load Wav2Vec2 model and bundle\n",
    "bundle = WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model()\n",
    "\n",
    "# Load and preprocess audio\n",
    "def load_audio(path, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
    "    return waveform.squeeze(0), target_sr\n",
    "\n",
    "# Get emissions (acoustic feature probabilities from Wav2Vec2)\n",
    "def get_emissions(model, waveform):\n",
    "    with torch.inference_mode():\n",
    "        emissions, _ = model(waveform.unsqueeze(0))  # shape: (1, time, classes)\n",
    "    return torch.log_softmax(emissions[0], dim=-1)\n",
    "\n",
    "# Force alignment using torchaudio's CTC decoder and alignment\n",
    "def forced_align(emissions, transcript, tokens, sample_rate, blank=0):\n",
    "    from torchaudio.functional import forced_align\n",
    "    # Convert transcript to indices\n",
    "    targets = torch.tensor([tokens[c] for c in transcript if c in tokens], dtype=torch.int32)\n",
    "    alignment = forced_align(emissions, targets, blank=blank)\n",
    "    time_per_frame = emissions.size(0) / sample_rate\n",
    "    results = []\n",
    "    for i, (t, duration) in enumerate(alignment):\n",
    "        phoneme = list(tokens.keys())[list(tokens.values()).index(t)]\n",
    "        start = i * time_per_frame\n",
    "        end = start + duration * time_per_frame\n",
    "        results.append((phoneme, start, end))\n",
    "    return results\n",
    "\n",
    "# Define phoneme tokens manually (simple mock)\n",
    "phoneme_tokens = {c: i + 1 for i, c in enumerate(\"aeiouʃkstnmrl|\")}  # Example set\n",
    "phoneme_tokens[\"_\"] = 0  # CTC blank\n",
    "\n",
    "# Your test audio\n",
    "audio_path = \"your_audio.wav\"\n",
    "waveform, sr = load_audio(audio_path)\n",
    "emissions = get_emissions(model, waveform)\n",
    "\n",
    "# Provide a phoneme transcript manually (or from another model)\n",
    "phoneme_transcript = \"ʃi|pɛr|sɔna\"  # Replace this with actual transcription if available\n",
    "\n",
    "# Get timestamps\n",
    "phoneme_timestamps = forced_align(emissions, phoneme_transcript, phoneme_tokens, sample_rate=sr)\n",
    "\n",
    "# Output\n",
    "for ph, start, end in phoneme_timestamps:\n",
    "    print(f\"{ph}: {start:.2f}s - {end:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<unk>NETRNLUENLEXVSH<unk>IS'KESUZ<unk>S'NE</s>ELDOBTRESB<s>UATHTDWHWX'HWHWCNAWAH'WXWEB</s>E'UCBCEZBC'E</s>CA'</s>E'U'</s>UHY'NSES'YNYISESESINVESEIEUNS</s>SUSUS</s>\",\n",
       " \"U<s>U'R</s>U</s>UY'<unk></s>'SESKN</s>N</s>ZNEB<unk>KHYHUWYW'YS'YKXD</s>S</s>XKX'SXS</s>STXPXU</s>'UYKES'P'X'S'NEIQ</s>'X'S'YHYA'<unk>'</s>KE</s>V</s>UXNEVNULS<unk>Q<unk>XEL'<unk>NEZT</s>LELES<unk>X<unk>AW</s>U</s>UC</s>CK</s>V</s>EL<unk>Z</s><s>HKE\",\n",
       " \"GLEDEKNXKE'IDHYH<unk>YUY<unk>Y</s>EUY'YHY'</s>ERTE</s>EH</s>H'ABT'K</s>XH</s>D</s>SD<s>HWE'EB'BUB</s>U'U'SESUS</s>T</s>EKE</s>EUQ'RECEUCQ</s>Q</s>ELC\",\n",
       " 'BO</s>KD',\n",
       " 'D',\n",
       " \"EUEUEK<unk>KESDWYH<unk>HYH'YAY<unk>YWZE</s>E</s>S'SX<unk>KENEUS<unk>E<unk>'<unk>'<unk>YSUT<unk>'SX<unk>ESXS'<unk>SXTYKSESESXTQKEIS</s>M<unk></s>EUT'</s>ES'<unk></s>X'S<unk>XESEK\",\n",
       " \"EZDOQOXEZE<unk><s>QTEUKESKSMHTDHD</s>HD</s>HDYDTDEDH'WHTE</s>KHKDKH'<unk></s>E'C</s>X'R'YUY'Y'YS'HY'</s>E</s>US</s>REUEUEZEDIDAKECE<unk><s>NXKSES<s>XASXRURESREU'<s>'RXR'YSRU'REURQ'Q'R'EQ<s></s>'SDPRBR'ES'ES'R'U'R'RURK'RU'</s>U'<unk>TUBXB'H<unk>Y'</s>'</s>Y'<unk></s>U'KXKBXORFDOV</s>E<unk>ERVU\",\n",
       " \"BN</s>S<unk>VNKEKSQCZXIMH<unk>WASEU'U'RUR<unk>'<unk>YUY<unk>Y'UASA'</s>'Y'H<unk>Y<unk>UKXBESU<s><unk>EKES<unk></s>U<unk>WS</s>Y</s>E</s>ETKEYEYEYEKEYDEDESY'</s>EZESQ'</s>US</s>UTZBXZE</s>EZAEK</s>E</s>TKQSQSQUKSEDXOXDLESUBX<s>KWLW</s>HVERES'<unk></s>NZCECEUBT</s>TWAWWE</s>E</s>UEYKESEMTSQTMX<s></s>M</s>SMS'<unk>'</s>SES'Y'Y</s>'AYUXUCBEUB<unk><s>KE<unk>EU</s>ZK</s>V</s><unk>EZES<unk>Y'SEUES'U'<unk>YS'Y'T<unk>KHWKWAHHWTWEHWYD</s>HDYDTHKXHSXSHXSMSESE</s>E</s>E</s>U</s>B</s>ES</s>'</s>S</s>'HSXHDSDSH'Y'ER'CEU\",\n",
       " \"Q<s>'RUPK\",\n",
       " \"EZIMIESY<unk>ESLH<unk>NXHKEKBMW<unk>H'A</s>SETC'D</s>EUB</s>'DYBD</s>'</s>B</s>D'D</s>BS</s>'</s>D</s>D'D</s>D'E</s>'D</s>E'EE</s>X</s>E'US</s>ESUS</s>ZERERER'UR'ARUR\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "-: 0.00s - 0.02s\n",
      "C: 0.02s - 0.04s\n",
      "-: 0.04s - 0.08s\n",
      "A: 0.08s - 0.10s\n",
      "-: 0.10s - 0.12s\n",
      "N: 0.12s - 0.14s\n",
      "-: 0.14s - 0.20s\n",
      "-: 0.22s - 0.38s\n",
      "K: 0.38s - 0.40s\n",
      "-: 0.40s - 0.46s\n",
      "I: 0.46s - 0.48s\n",
      "-: 0.48s - 0.54s\n",
      "M: 0.56s - 0.60s\n",
      "-: 0.60s - 0.64s\n",
      "A: 0.64s - 0.66s\n",
      "-: 0.66s - 0.74s\n",
      "P: 0.74s - 0.76s\n",
      "-: 0.76s - 0.86s\n",
      "A: 0.86s - 0.88s\n",
      "-: 0.88s - 1.00s\n",
      "-: 1.04s - 1.08s\n",
      "J: 1.08s - 1.10s\n",
      "-: 1.10s - 1.16s\n",
      "O: 1.16s - 1.18s\n",
      "U: 1.18s - 1.20s\n",
      "-: 1.22s - 1.26s\n",
      "N: 1.26s - 1.28s\n",
      "-: 1.28s - 1.32s\n",
      "O: 1.32s - 1.34s\n",
      "W: 1.34s - 1.36s\n",
      "-: 1.36s - 1.42s\n",
      "-: 1.46s - 5.44s\n",
      "O: 5.44s - 5.46s\n",
      "-: 5.46s - 5.56s\n",
      "S: 5.56s - 5.58s\n",
      "-: 5.58s - 5.78s\n",
      "O: 5.78s - 5.80s\n",
      "-: 5.80s - 5.90s\n",
      "-: 5.92s - 8.48s\n",
      "M: 8.48s - 8.50s\n",
      "-: 8.50s - 8.54s\n",
      "I: 8.54s - 8.56s\n",
      "-: 8.56s - 8.64s\n",
      "S: 8.64s - 8.70s\n",
      "-: 8.70s - 8.74s\n",
      "-: 8.76s - 8.80s\n",
      "M: 8.80s - 8.82s\n",
      "-: 8.82s - 8.94s\n",
      "A: 8.94s - 8.96s\n",
      "-: 8.96s - 8.98s\n",
      "R: 8.98s - 9.00s\n",
      "-: 9.00s - 9.12s\n",
      "-: 9.14s - 9.20s\n",
      "G: 9.20s - 9.22s\n",
      "-: 9.22s - 9.34s\n",
      "O: 9.34s - 9.36s\n",
      "-: 9.36s - 9.44s\n",
      "-: 9.48s - 9.90s\n",
      "S: 9.90s - 9.92s\n",
      "-: 9.92s - 10.28s\n",
      "-: 10.32s - 10.98s\n",
      "S: 10.98s - 11.00s\n",
      "-: 11.00s - 11.10s\n",
      "I: 11.10s - 11.12s\n",
      "-: 11.12s - 11.26s\n",
      "-: 11.28s - 11.32s\n",
      "B: 11.32s - 11.34s\n",
      "-: 11.34s - 11.40s\n",
      "E: 11.40s - 11.42s\n",
      "-: 11.42s - 11.44s\n",
      "L: 11.44s - 11.46s\n",
      "-: 11.46s - 11.52s\n",
      "L: 11.52s - 11.54s\n",
      "-: 11.54s - 11.66s\n",
      "O: 11.66s - 11.68s\n",
      "-: 11.68s - 11.78s\n",
      "-: 11.82s - 11.84s\n",
      "N: 11.84s - 11.86s\n",
      "-: 11.86s - 11.96s\n",
      "A: 11.96s - 11.98s\n",
      "-: 11.98s - 12.04s\n",
      "K: 12.04s - 12.06s\n",
      "-: 12.06s - 12.12s\n",
      "-: 12.16s - 14.92s\n",
      "K: 14.92s - 14.94s\n",
      "-: 14.94s - 15.00s\n",
      "A: 15.00s - 15.02s\n",
      "T: 15.02s - 15.04s\n",
      "-: 15.04s - 15.10s\n",
      "T: 15.10s - 15.12s\n",
      "-: 15.12s - 15.16s\n",
      "Y: 15.16s - 15.18s\n",
      "-: 15.18s - 15.20s\n",
      "N: 15.24s - 15.26s\n",
      "-: 15.26s - 15.38s\n",
      "O: 15.38s - 15.40s\n",
      "-: 15.40s - 15.50s\n",
      "T: 15.50s - 15.52s\n",
      "-: 15.52s - 15.66s\n",
      "E: 15.66s - 15.68s\n",
      "-: 15.68s - 15.72s\n",
      "T: 15.72s - 15.74s\n",
      "-: 15.74s - 15.82s\n",
      "-: 15.84s - 16.70s\n",
      "D: 16.70s - 16.72s\n",
      "-: 16.72s - 16.90s\n",
      "O: 16.90s - 16.92s\n",
      "-: 16.92s - 16.98s\n",
      "R: 16.98s - 17.00s\n",
      "-: 17.00s - 17.10s\n",
      "-: 17.12s - 17.20s\n",
      "C: 17.20s - 17.22s\n",
      "-: 17.22s - 17.40s\n",
      "A: 17.40s - 17.42s\n",
      "-: 17.42s - 17.48s\n",
      "P: 17.48s - 17.50s\n",
      "-: 17.50s - 17.56s\n",
      "-: 17.60s - 19.48s\n",
      "B: 19.48s - 19.50s\n",
      "-: 19.50s - 19.58s\n",
      "O: 19.58s - 19.60s\n",
      "-: 19.60s - 19.68s\n",
      "L: 19.68s - 19.70s\n",
      "-: 19.70s - 19.80s\n",
      "E: 19.80s - 19.82s\n",
      "-: 19.82s - 19.96s\n",
      "-: 19.98s - 20.02s\n",
      "J: 20.02s - 20.04s\n",
      "-: 20.04s - 20.14s\n",
      "O: 20.14s - 20.16s\n",
      "-: 20.16s - 20.34s\n",
      "K: 20.34s - 20.36s\n",
      "-: 20.36s - 20.68s\n",
      "A: 20.68s - 20.70s\n",
      "-: 20.70s - 20.84s\n",
      "-: 20.88s - 23.64s\n",
      "C: 23.64s - 23.68s\n",
      "-: 23.68s - 23.80s\n",
      "A: 23.80s - 23.82s\n",
      "R: 23.82s - 23.84s\n",
      "-: 23.84s - 23.92s\n",
      "-: 23.96s - 24.00s\n",
      "Y: 24.00s - 24.02s\n",
      "-: 24.02s - 24.10s\n",
      "E: 24.10s - 24.12s\n",
      "-: 24.12s - 24.18s\n",
      "-: 24.22s - 24.26s\n",
      "T: 24.26s - 24.28s\n",
      "-: 24.28s - 24.42s\n",
      "A: 24.42s - 24.44s\n",
      "-: 24.44s - 24.60s\n",
      "P: 24.60s - 24.62s\n",
      "-: 24.62s - 24.76s\n",
      "O: 24.76s - 24.78s\n",
      "-: 24.78s - 24.88s\n",
      "-: 24.94s - 28.64s\n",
      "N: 28.64s - 28.66s\n",
      "-: 28.66s - 28.76s\n",
      "A: 28.76s - 28.78s\n",
      "-: 28.78s - 28.89s\n",
      "K: 28.89s - 28.91s\n",
      "-: 28.91s - 28.99s\n",
      "E: 28.99s - 29.01s\n",
      "-: 29.01s - 29.07s\n",
      "P: 29.07s - 29.09s\n",
      "-: 29.09s - 29.25s\n",
      "A: 29.25s - 29.27s\n",
      "-: 29.27s - 29.33s\n",
      "N: 29.33s - 29.35s\n",
      "-: 29.35s - 29.51s\n",
      "T: 29.51s - 29.53s\n",
      "-: 29.53s - 29.67s\n",
      "E: 29.67s - 29.69s\n",
      "-: 29.69s - 29.83s\n",
      "-: 29.85s - 31.55s\n",
      "N: 31.55s - 31.59s\n",
      "-: 31.59s - 31.67s\n",
      "O: 31.67s - 31.69s\n",
      "-: 31.69s - 31.79s\n",
      "-: 31.81s - 31.93s\n",
      "T: 31.93s - 31.95s\n",
      "H: 31.95s - 31.97s\n",
      "-: 31.97s - 32.11s\n",
      "O: 32.11s - 32.13s\n",
      "-: 32.13s - 32.27s\n",
      "T: 32.27s - 32.29s\n",
      "-: 32.29s - 32.33s\n",
      "-: 32.35s - 34.61s\n",
      "A: 34.61s - 34.63s\n",
      "-: 34.63s - 34.65s\n",
      "N: 34.65s - 34.67s\n",
      "-: 34.67s - 34.77s\n",
      "-: 34.79s - 34.81s\n",
      "Y: 34.81s - 34.83s\n",
      "-: 34.83s - 34.93s\n",
      "O: 34.93s - 34.95s\n",
      "-: 34.95s - 35.07s\n",
      "-: 35.09s - 35.11s\n",
      "M: 35.11s - 35.13s\n",
      "-: 35.13s - 35.15s\n",
      "I: 35.15s - 35.17s\n",
      "-: 35.17s - 35.25s\n",
      "S: 35.25s - 35.27s\n",
      "-: 35.27s - 35.29s\n",
      "H: 35.29s - 35.31s\n",
      "-: 35.31s - 35.39s\n",
      "A: 35.39s - 35.41s\n",
      "-: 35.41s - 35.49s\n",
      "-: 35.53s - 38.05s\n",
      "N: 38.05s - 38.07s\n",
      "-: 38.07s - 38.11s\n",
      "O: 38.11s - 38.13s\n",
      "-: 38.13s - 38.15s\n",
      "N: 38.15s - 38.17s\n",
      "-: 38.17s - 38.25s\n",
      "-: 38.27s - 38.29s\n",
      "G: 38.29s - 38.31s\n",
      "-: 38.31s - 38.35s\n",
      "R: 38.35s - 38.37s\n",
      "-: 38.37s - 38.49s\n",
      "A: 38.49s - 38.51s\n",
      "-: 38.51s - 38.63s\n",
      "S: 38.63s - 38.65s\n",
      "-: 38.65s - 38.77s\n",
      "T: 38.77s - 38.79s\n",
      "-: 38.79s - 38.95s\n",
      "O: 38.95s - 38.97s\n",
      "-: 38.97s - 39.09s\n",
      "-: 39.11s - 42.31s\n",
      "L: 42.31s - 42.33s\n",
      "-: 42.33s - 42.37s\n",
      "I: 42.37s - 42.39s\n",
      "-: 42.39s - 42.45s\n",
      "T: 42.45s - 42.47s\n",
      "-: 42.47s - 42.53s\n",
      "-: 42.57s - 42.59s\n",
      "C: 42.59s - 42.61s\n",
      "H: 42.61s - 42.63s\n",
      "-: 42.63s - 42.73s\n",
      "A: 42.73s - 42.75s\n",
      "-: 42.75s - 42.77s\n",
      "R: 42.77s - 42.79s\n",
      "-: 42.79s - 42.91s\n",
      "C: 42.91s - 42.93s\n",
      "-: 42.93s - 43.01s\n",
      "O: 43.01s - 43.03s\n",
      "-: 43.03s - 43.11s\n",
      "P: 43.11s - 43.13s\n",
      "-: 43.13s - 43.21s\n",
      "E: 43.21s - 43.23s\n",
      "-: 43.23s - 43.31s\n",
      "-: 43.35s - 46.21s\n"
     ]
    }
   ],
   "source": [
    "# Load model with forced alignment support\n",
    "bundle = torchaudio.pipelines.WAVLM_BASE_PLUS\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = bundle.get_model().to(device)\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "# Load and preprocess audio\n",
    "def load_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=bundle.sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze(0)\n",
    "\n",
    "# Greedy decode with time info\n",
    "def forced_alignment(waveform):\n",
    "    with torch.inference_mode():\n",
    "        emissions, _ = model(waveform[None])\n",
    "    emissions = torch.log_softmax(emissions, dim=-1)\n",
    "\n",
    "    # Greedy decoding with timestamps\n",
    "    tokens = torch.argmax(emissions, dim=-1).squeeze(0)\n",
    "    timestamps = []\n",
    "\n",
    "    prev = None\n",
    "    start_frame = 0\n",
    "    time_per_frame = waveform.shape[0] / emissions.shape[1] / bundle.sample_rate\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token != prev:\n",
    "            if prev is not None and prev != bundle.get_labels().index(\"|\"):  # Skip blanks\n",
    "                label = labels[prev]\n",
    "                start = start_frame * time_per_frame\n",
    "                end = i * time_per_frame\n",
    "                timestamps.append((label, round(start, 2), round(end, 2)))\n",
    "            start_frame = i\n",
    "            prev = token\n",
    "\n",
    "    # Add last token\n",
    "    if prev is not None and prev != bundle.get_labels().index(\"|\"):\n",
    "        label = labels[prev]\n",
    "        start = start_frame * time_per_frame\n",
    "        end = len(tokens) * time_per_frame\n",
    "        timestamps.append((label, round(start, 2), round(end, 2)))\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "# Run\n",
    "audio_path_folder = \"data/2_Audiofiles/Decoding_IT_T1/\"\n",
    "audio_file = \"102_edugame2023_32c4a5e851c1431aba3aa409e3be8128_649d404f44214261b67b24f1845e1350.wav\"\n",
    "audio_path = audio_path_folder + audio_file\n",
    "waveform = load_audio(audio_path)\n",
    "phoneme_timestamps = forced_alignment(waveform)\n",
    "phonemes=[]\n",
    "timestamps = []\n",
    "for ph, start, end in phoneme_timestamps:\n",
    "    phonemes.append(ph)\n",
    "    timestamps.append([start, end])\n",
    "    \n",
    "csv_file = \"phoneme_timestamps.csv\"\n",
    "\n",
    "# Write header if creating a new file\n",
    "write_header = True  # Change to False if you're appending more files later\n",
    "\n",
    "with open(csv_file, mode='w' if write_header else 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if write_header:\n",
    "        writer.writerow([\"audio_file\", \"phonemes\", \"timestamps\"])\n",
    "    writer.writerow([audio_file, str(phonemes), str(timestamps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.02],\n",
       " [0.02, 0.04],\n",
       " [0.04, 0.08],\n",
       " [0.08, 0.1],\n",
       " [0.1, 0.12],\n",
       " [0.12, 0.14],\n",
       " [0.14, 0.2],\n",
       " [0.22, 0.38],\n",
       " [0.38, 0.4],\n",
       " [0.4, 0.46],\n",
       " [0.46, 0.48],\n",
       " [0.48, 0.54],\n",
       " [0.56, 0.6],\n",
       " [0.6, 0.64],\n",
       " [0.64, 0.66],\n",
       " [0.66, 0.74],\n",
       " [0.74, 0.76],\n",
       " [0.76, 0.86],\n",
       " [0.86, 0.88],\n",
       " [0.88, 1.0],\n",
       " [1.04, 1.08],\n",
       " [1.08, 1.1],\n",
       " [1.1, 1.16],\n",
       " [1.16, 1.18],\n",
       " [1.18, 1.2],\n",
       " [1.22, 1.26],\n",
       " [1.26, 1.28],\n",
       " [1.28, 1.32],\n",
       " [1.32, 1.34],\n",
       " [1.34, 1.36],\n",
       " [1.36, 1.42],\n",
       " [1.46, 5.44],\n",
       " [5.44, 5.46],\n",
       " [5.46, 5.56],\n",
       " [5.56, 5.58],\n",
       " [5.58, 5.78],\n",
       " [5.78, 5.8],\n",
       " [5.8, 5.9],\n",
       " [5.92, 8.48],\n",
       " [8.48, 8.5],\n",
       " [8.5, 8.54],\n",
       " [8.54, 8.56],\n",
       " [8.56, 8.64],\n",
       " [8.64, 8.7],\n",
       " [8.7, 8.74],\n",
       " [8.76, 8.8],\n",
       " [8.8, 8.82],\n",
       " [8.82, 8.94],\n",
       " [8.94, 8.96],\n",
       " [8.96, 8.98],\n",
       " [8.98, 9.0],\n",
       " [9.0, 9.12],\n",
       " [9.14, 9.2],\n",
       " [9.2, 9.22],\n",
       " [9.22, 9.34],\n",
       " [9.34, 9.36],\n",
       " [9.36, 9.44],\n",
       " [9.48, 9.9],\n",
       " [9.9, 9.92],\n",
       " [9.92, 10.28],\n",
       " [10.32, 10.98],\n",
       " [10.98, 11.0],\n",
       " [11.0, 11.1],\n",
       " [11.1, 11.12],\n",
       " [11.12, 11.26],\n",
       " [11.28, 11.32],\n",
       " [11.32, 11.34],\n",
       " [11.34, 11.4],\n",
       " [11.4, 11.42],\n",
       " [11.42, 11.44],\n",
       " [11.44, 11.46],\n",
       " [11.46, 11.52],\n",
       " [11.52, 11.54],\n",
       " [11.54, 11.66],\n",
       " [11.66, 11.68],\n",
       " [11.68, 11.78],\n",
       " [11.82, 11.84],\n",
       " [11.84, 11.86],\n",
       " [11.86, 11.96],\n",
       " [11.96, 11.98],\n",
       " [11.98, 12.04],\n",
       " [12.04, 12.06],\n",
       " [12.06, 12.12],\n",
       " [12.16, 14.92],\n",
       " [14.92, 14.94],\n",
       " [14.94, 15.0],\n",
       " [15.0, 15.02],\n",
       " [15.02, 15.04],\n",
       " [15.04, 15.1],\n",
       " [15.1, 15.12],\n",
       " [15.12, 15.16],\n",
       " [15.16, 15.18],\n",
       " [15.18, 15.2],\n",
       " [15.24, 15.26],\n",
       " [15.26, 15.38],\n",
       " [15.38, 15.4],\n",
       " [15.4, 15.5],\n",
       " [15.5, 15.52],\n",
       " [15.52, 15.66],\n",
       " [15.66, 15.68],\n",
       " [15.68, 15.72],\n",
       " [15.72, 15.74],\n",
       " [15.74, 15.82],\n",
       " [15.84, 16.7],\n",
       " [16.7, 16.72],\n",
       " [16.72, 16.9],\n",
       " [16.9, 16.92],\n",
       " [16.92, 16.98],\n",
       " [16.98, 17.0],\n",
       " [17.0, 17.1],\n",
       " [17.12, 17.2],\n",
       " [17.2, 17.22],\n",
       " [17.22, 17.4],\n",
       " [17.4, 17.42],\n",
       " [17.42, 17.48],\n",
       " [17.48, 17.5],\n",
       " [17.5, 17.56],\n",
       " [17.6, 19.48],\n",
       " [19.48, 19.5],\n",
       " [19.5, 19.58],\n",
       " [19.58, 19.6],\n",
       " [19.6, 19.68],\n",
       " [19.68, 19.7],\n",
       " [19.7, 19.8],\n",
       " [19.8, 19.82],\n",
       " [19.82, 19.96],\n",
       " [19.98, 20.02],\n",
       " [20.02, 20.04],\n",
       " [20.04, 20.14],\n",
       " [20.14, 20.16],\n",
       " [20.16, 20.34],\n",
       " [20.34, 20.36],\n",
       " [20.36, 20.68],\n",
       " [20.68, 20.7],\n",
       " [20.7, 20.84],\n",
       " [20.88, 23.64],\n",
       " [23.64, 23.68],\n",
       " [23.68, 23.8],\n",
       " [23.8, 23.82],\n",
       " [23.82, 23.84],\n",
       " [23.84, 23.92],\n",
       " [23.96, 24.0],\n",
       " [24.0, 24.02],\n",
       " [24.02, 24.1],\n",
       " [24.1, 24.12],\n",
       " [24.12, 24.18],\n",
       " [24.22, 24.26],\n",
       " [24.26, 24.28],\n",
       " [24.28, 24.42],\n",
       " [24.42, 24.44],\n",
       " [24.44, 24.6],\n",
       " [24.6, 24.62],\n",
       " [24.62, 24.76],\n",
       " [24.76, 24.78],\n",
       " [24.78, 24.88],\n",
       " [24.94, 28.64],\n",
       " [28.64, 28.66],\n",
       " [28.66, 28.76],\n",
       " [28.76, 28.78],\n",
       " [28.78, 28.89],\n",
       " [28.89, 28.91],\n",
       " [28.91, 28.99],\n",
       " [28.99, 29.01],\n",
       " [29.01, 29.07],\n",
       " [29.07, 29.09],\n",
       " [29.09, 29.25],\n",
       " [29.25, 29.27],\n",
       " [29.27, 29.33],\n",
       " [29.33, 29.35],\n",
       " [29.35, 29.51],\n",
       " [29.51, 29.53],\n",
       " [29.53, 29.67],\n",
       " [29.67, 29.69],\n",
       " [29.69, 29.83],\n",
       " [29.85, 31.55],\n",
       " [31.55, 31.59],\n",
       " [31.59, 31.67],\n",
       " [31.67, 31.69],\n",
       " [31.69, 31.79],\n",
       " [31.81, 31.93],\n",
       " [31.93, 31.95],\n",
       " [31.95, 31.97],\n",
       " [31.97, 32.11],\n",
       " [32.11, 32.13],\n",
       " [32.13, 32.27],\n",
       " [32.27, 32.29],\n",
       " [32.29, 32.33],\n",
       " [32.35, 34.61],\n",
       " [34.61, 34.63],\n",
       " [34.63, 34.65],\n",
       " [34.65, 34.67],\n",
       " [34.67, 34.77],\n",
       " [34.79, 34.81],\n",
       " [34.81, 34.83],\n",
       " [34.83, 34.93],\n",
       " [34.93, 34.95],\n",
       " [34.95, 35.07],\n",
       " [35.09, 35.11],\n",
       " [35.11, 35.13],\n",
       " [35.13, 35.15],\n",
       " [35.15, 35.17],\n",
       " [35.17, 35.25],\n",
       " [35.25, 35.27],\n",
       " [35.27, 35.29],\n",
       " [35.29, 35.31],\n",
       " [35.31, 35.39],\n",
       " [35.39, 35.41],\n",
       " [35.41, 35.49],\n",
       " [35.53, 38.05],\n",
       " [38.05, 38.07],\n",
       " [38.07, 38.11],\n",
       " [38.11, 38.13],\n",
       " [38.13, 38.15],\n",
       " [38.15, 38.17],\n",
       " [38.17, 38.25],\n",
       " [38.27, 38.29],\n",
       " [38.29, 38.31],\n",
       " [38.31, 38.35],\n",
       " [38.35, 38.37],\n",
       " [38.37, 38.49],\n",
       " [38.49, 38.51],\n",
       " [38.51, 38.63],\n",
       " [38.63, 38.65],\n",
       " [38.65, 38.77],\n",
       " [38.77, 38.79],\n",
       " [38.79, 38.95],\n",
       " [38.95, 38.97],\n",
       " [38.97, 39.09],\n",
       " [39.11, 42.31],\n",
       " [42.31, 42.33],\n",
       " [42.33, 42.37],\n",
       " [42.37, 42.39],\n",
       " [42.39, 42.45],\n",
       " [42.45, 42.47],\n",
       " [42.47, 42.53],\n",
       " [42.57, 42.59],\n",
       " [42.59, 42.61],\n",
       " [42.61, 42.63],\n",
       " [42.63, 42.73],\n",
       " [42.73, 42.75],\n",
       " [42.75, 42.77],\n",
       " [42.77, 42.79],\n",
       " [42.79, 42.91],\n",
       " [42.91, 42.93],\n",
       " [42.93, 43.01],\n",
       " [43.01, 43.03],\n",
       " [43.03, 43.11],\n",
       " [43.11, 43.13],\n",
       " [43.13, 43.21],\n",
       " [43.21, 43.23],\n",
       " [43.23, 43.31],\n",
       " [43.35, 46.21]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file = \"102_edugame2023_32c4a5e851c1431aba3aa409e3be8128_649d404f44214261b67b24f1845e1350.wav\"\n",
    "# Output CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace space with \n",
    "# import re\n",
    "# df_cod1 = pd.read_csv(\"data/processed/Decoding_ground_truth_IT.csv\",index_col=0)\n",
    "# def space_phonemes(text):\n",
    "#     # First, temporarily replace special tokens with placeholders\n",
    "#     text = text.replace('[PAD]', '<<PAD>>').replace('[UNK]', '<<UNK>>')\n",
    "#     # Now insert space between every character\n",
    "#     text = ' '.join(text)\n",
    "#     # Restore the special tokens\n",
    "#     text = text.replace('< < P A D > >', '[PAD]').replace('< < U N K > >', '[UNK]')\n",
    "#     # Clean up accidental spacing inside tokens (just in case)\n",
    "#     text = re.sub(r'\\[\\s*PAD\\s*\\]', '[PAD]', text)\n",
    "#     text = re.sub(r'\\[\\s*UNK\\s*\\]', '[UNK]', text)\n",
    "#     return text\n",
    "\n",
    "# # Replace space with | first\n",
    "# df_cod1['trial_answer_coder2_phoneme'] = df_cod1['trial_answer_coder2_phoneme'].str.replace(' ', '|')\n",
    "\n",
    "# # Then apply spacing logic\n",
    "# df_cod1['trial_answer_coder2_phoneme'] = df_cod1['trial_answer_coder2_phoneme'].apply(space_phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_934485/2641897723.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_cod1.iloc[-1][-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v a l o [PAD] | s a v e [PAD] | [UNK] [PAD] | k l o [PAD] | f e [PAD] | l o [PAD] | [UNK] [PAD] | s o n [PAD] | v i [PAD] | ʎ ɔ [PAD] | d e t e [PAD] | f a r d e [PAD] | p o r i t a [PAD] | p o r i t ː a [PAD] | p r o d i ʎ a [PAD] | a n [PAD] | t ʃ i [PAD] | b i ɲ a z o [PAD] | f l a n ɛ s t r o [PAD] | k o [PAD] | ɲ a [PAD] | r ɪ [PAD] | p o'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cod1.iloc[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vuzo[PAD]seɡa[PAD]klofɛno[PAD]raviʎo[PAD]da[PAD]pe[PAD]tarse[PAD]doridzːa[PAD]prateʎa[PAD]aː[PAD]ɛrɾe[PAD]lo[PAD]beɲole[PAD]fla[PAD]vɛstro[PAD]kʊɲaripːo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_cod1 = pd.read_csv(\"data/processed/2_Decoding_ground_truth_IT.csv\",index_col=0)\n",
    "df_cod1['trial_answer_coder2_phoneme'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "def extract_unique_phonemes(phoneme_str):\n",
    "    pattern = r\"\\[PAD\\]|\\[UNK\\]|.\"\n",
    "    phonemes = re.findall(pattern, phoneme_str)\n",
    "    return list(dict.fromkeys(phonemes))\n",
    "phonoemes = df_cod1['trial_answer_coder2_phoneme'].tolist()\n",
    "phonoemes = \"\".join(phonoemes)\n",
    "unique_phonemes_it = extract_unique_phonemes(phonoemes)\n",
    "unique_phonemes_dict_it = {i: ph for i, ph in enumerate(unique_phonemes_it)}\n",
    "\n",
    "\n",
    "with open(\"data/processed/unique_phonemes_IT.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique_phonemes_dict_it, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201409"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phonoemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_phonemes_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v',\n",
       " 'u',\n",
       " 'z',\n",
       " 'o',\n",
       " '[PAD]',\n",
       " 's',\n",
       " 'e',\n",
       " 'ɡ',\n",
       " 'a',\n",
       " 'k',\n",
       " 'l',\n",
       " 'f',\n",
       " 'ɛ',\n",
       " 'n',\n",
       " 'r',\n",
       " 'i',\n",
       " 'ʎ',\n",
       " 'd',\n",
       " 'p',\n",
       " 't',\n",
       " 'ː',\n",
       " 'ɾ',\n",
       " 'b',\n",
       " 'ɲ',\n",
       " 'ʊ',\n",
       " 'm',\n",
       " 'ɔ',\n",
       " 'ʒ',\n",
       " 'ɪ',\n",
       " 'ʃ',\n",
       " '[UNK]',\n",
       " 'w',\n",
       " 'ŋ',\n",
       " 'j',\n",
       " '̪']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_phonemes_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cod_fr=pd.read_csv(\"data/processed/2_Phoneme_Deleletion_ground_truth_FR.csv\",index_col=0)\n",
    "df_cod_fr.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_phonemes(phoneme_str):\n",
    "    pattern = r\"\\[PAD\\]|\\[UNK\\]|.\"\n",
    "    phonemes = re.findall(pattern, phoneme_str) \n",
    "    # remove ' ' this should not be in the list\n",
    "    phonemes = [ph for ph in phonemes if ph != ' ']\n",
    "    return list(dict.fromkeys(phonemes))\n",
    "phonoemes = df_cod_fr['trial_answer_coder1_phoneme'].dropna().tolist()\n",
    "phonoemes = \"\".join(phonoemes)\n",
    "unique_phonemes_fr = extract_unique_phonemes(phonoemes)\n",
    "unique_phonemes_dict_fr = {i: ph for i, ph in enumerate(unique_phonemes_fr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/unique_phonemes_FR.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique_phonemes_dict_fr, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique phonemes in the list between two languages\n",
    "unique_phonemes_fr = set(unique_phonemes_fr)\n",
    "unique_phonemes_it = set(unique_phonemes_it)\n",
    "unique_phonemes_fr_it = unique_phonemes_fr.union(unique_phonemes_it)\n",
    "unique_phonemes_fr_it = list(unique_phonemes_fr_it)\n",
    "unique_phonemes_fr_it = {i: ph for i, ph in enumerate(unique_phonemes_fr_it)}\n",
    "with open(\"data/processed/unique_phonemes_FR_IT.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique_phonemes_fr_it, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('output_IT.csv')\n",
    "df['probabilities'] = df['probabilities'].apply(eval)\n",
    "df['timestamps'] = df['timestamps'].apply(eval)\n",
    "len(df['probabilities'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
